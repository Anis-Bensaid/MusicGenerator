{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import sample\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM, Embedding\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras import backend as K\n",
    "from feature_funcs import *\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/desireewaugh/Desktop/MIT/Courses/6.883 - Modeling with ML/Projects/Final Project/MusicGenerator/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training3(char_nums, str_length):\n",
    "    '''Create training dataset with x and y values from your numeric list.\n",
    "    The x data is a list of all numeric sequences, and the y data is the next character.'''\n",
    "    \n",
    "    # The x_values begin at the starting indices and are str_length characters long\n",
    "    # The y_values are one character after the end of the x_values\n",
    "    x_data = np.array(char_nums[0:str_length])\n",
    "    y_data = [char_nums[str_length]]\n",
    "    for i in range(1, len(char_nums)-str_length):\n",
    "        x_data = np.vstack((x_data, np.array(char_nums[i:i+str_length])))\n",
    "        y_data.append(char_nums[i+str_length])\n",
    "    \n",
    "    #return x_data, y_data\n",
    "    return x_data, y_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4800, 200)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"Jigs.txt\") as my_file:\n",
    "    abc_text = my_file.read()\n",
    "    \n",
    "num_to_char, char_to_num = create_dictionaries(abc_text)\n",
    "vocab_length = len(num_to_char)\n",
    "\n",
    "text_nums_train = encoder(abc_text, char_to_num)\n",
    "\n",
    "x_vals, y_vals = create_training3(text_nums_train[0:5000], 200)\n",
    "x_vals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4800, 200, 91), (4800, 91))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://keras.io/examples/lstm_text_generation/\n",
    "# Vectorization\n",
    "x = np.zeros((x_vals.shape[0], x_vals.shape[1], vocab_length))\n",
    "y = np.zeros((x_vals.shape[0], vocab_length))\n",
    "for i, sequence in enumerate(x_vals):\n",
    "    for j, number in enumerate(sequence):\n",
    "        x[i, j, number] = 1\n",
    "        y[i, y_vals[i]] = 1\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources: <br />\n",
    "https://towardsdatascience.com/how-to-generate-music-using-a-lstm-neural-network-in-keras-68786834d4c5  <br />\n",
    "https://github.com/aamini/introtodeeplearning/blob/master/lab1/Part2_Music_Generation.ipynb  <br /> https://medium.com/datadriveninvestor/music-generation-using-deep-learning-85010fb982e2 \n",
    "<br /> https://keras.io/examples/lstm_text_generation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 200, 128)          112640    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 200, 256)          394240    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 512)               1574912   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 91)                46683     \n",
      "=================================================================\n",
      "Total params: 2,128,475\n",
      "Trainable params: 2,128,475\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "# model.add(layers.Embedding(vocab_length, 256, batch_input_shape=x_vals.shape))\n",
    "model.add(layers.LSTM(128, input_shape=(x.shape[1], x.shape[2]), return_sequences=True))\n",
    "model.add(layers.LSTM(256, return_sequences=True))\n",
    "model.add(layers.LSTM(512))\n",
    "model.add(layers.Dense(vocab_length, activation='softmax'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmsprop: Divide the learning rate for a weight by a running average of the\n",
    "# magnitues of the recent gradients for that weight\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4800 samples\n",
      "Epoch 1/20\n",
      "1600/4800 [=========>....................] - ETA: 4:34 - loss: 4.0829"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "model.fit(x, y, epochs=20, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_predictions = decoder(predictions, num_to_char)\n",
    "text_predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
