{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import h5py\n",
    "from random import sample\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Input\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import LSTM, Embedding\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras import backend as K\n",
    "from feature_funcs import *\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary\n",
    "with open(\"../data/Jigs.txt\") as my_file:\n",
    "    abc_text = my_file.read()\n",
    "\n",
    "# Cut out unnecessary backslashes\n",
    "abc_text = re.sub('\\\\\\\\+\\n', '\\n', abc_text)\n",
    "\n",
    "# Find starting index of the data we care about\n",
    "start_ind = abc_text.find(\"X:\")\n",
    "abc_text = abc_text[start_ind:]\n",
    "\n",
    "# Encode data\n",
    "num_to_char, char_to_num = create_dictionaries(abc_text)\n",
    "vocab_length = len(num_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate Songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = re.findall(r'(T:(?:.+\\n)+\\n+)X:', abc_text)\n",
    "\n",
    "song_texts = []\n",
    "\n",
    "for song in songs:\n",
    "    song_text = re.search(r'(P:.+|K:.+)', song, re.DOTALL)\n",
    "    song_texts.append(song_text[0])\n",
    "    \n",
    "# Get aggregate statistics about song texts\n",
    "print(\"Min text length: \", min([len(text) for text in song_texts]))\n",
    "print(\"Max text length: \", max([len(text) for text in song_texts]))\n",
    "print(\"Average text length: \", np.mean([len(text) for text in song_texts]))\n",
    "\n",
    "# Make sure all song texts are the same length\n",
    "song_texts = [text[:170] for text in song_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Metadata elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract titles\n",
    "titles = []\n",
    "\n",
    "for song in songs:\n",
    "    title = re.search(r'(T:.+\\n% Nottingham Music Database\\n)', song).group(1)\n",
    "    titles.append(title)\n",
    "    \n",
    "# Pad titles to same length\n",
    "max_length = max(len(title) for title in titles)\n",
    "titles = [title.ljust(max_length) for title in titles]\n",
    "\n",
    "\n",
    "# Extract authors\n",
    "authors = []\n",
    "\n",
    "for song in songs:\n",
    "    author = re.search(r'\\n(S:.+\\n)', song).group(1)\n",
    "    authors.append(author)\n",
    "    \n",
    "# Pad authors to same length\n",
    "max_length = max(len(author) for author in authors)\n",
    "authors = [author.ljust(max_length) for author in authors]\n",
    "\n",
    "\n",
    "# Extract meters\n",
    "meters = []\n",
    "\n",
    "for song in songs:\n",
    "    meter = re.search(r'(M:\\d/\\d\\n)', song).group(1)\n",
    "    meters.append(meter)\n",
    "\n",
    "    \n",
    "# Pad meters to same length\n",
    "max_length = max(len(meter) for meter in meters)\n",
    "meters = [meter.ljust(max_length) for meter in meters]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode data to numeric format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_nums_list = encoder2(song_texts, char_to_num)\n",
    "\n",
    "title_nums = encoder2(titles, char_to_num)\n",
    "author_nums = encoder2(authors, char_to_num)\n",
    "meter_nums = encoder2(meters, char_to_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meter_nums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Use Pickled data where songs are not separated (data already encoded to numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open pickled training data so you don't have to re-run create_training\n",
    "x_file_pickle = open('../data/x_train_pickle.obj', 'rb')\n",
    "y_file_pickle = open('../data/y_train_pickle.obj', 'rb')\n",
    "\n",
    "x_train = pickle.load(x_file_pickle)\n",
    "y_train = pickle.load(y_file_pickle)\n",
    "\n",
    "x_file_pickle.close()\n",
    "y_file_pickle.close()\n",
    "\n",
    "vocab_length = x_train.shape[2]\n",
    "vocab_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape, x_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "print(vocab_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build & Compile LSTM (Model 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources: <br />\n",
    "https://towardsdatascience.com/how-to-generate-music-using-a-lstm-neural-network-in-keras-68786834d4c5  <br />\n",
    "https://github.com/aamini/introtodeeplearning/blob/master/lab1/Part2_Music_Generation.ipynb  <br /> https://medium.com/datadriveninvestor/music-generation-using-deep-learning-85010fb982e2 \n",
    "<br /> https://keras.io/examples/lstm_text_generation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(x_train.shape[1], x_train.shape[2]), return_sequences=True))\n",
    "model.add(layers.LSTM(256, return_sequences=True))\n",
    "model.add(layers.LSTM(512))\n",
    "model.add(layers.Dense(vocab_length, activation='softmax'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmsprop: Divide the learning rate for a weight by a running average of the\n",
    "# magnitues of the recent gradients for that weight\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train RNN\n",
    "Epoch: When the Neural Network sees all of the training data <br />\n",
    "Batch: Subset of the data <br />\n",
    "i.e. If you have 1000 data points, your batch size is 500 and you want 1 epoch, then the NN will do 2 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use checkpoints to save training weights before the model finishes training\n",
    "# Using this file path, the model checkpoints will be saved with the epoch number \n",
    "# and the validation loss in the filename.\n",
    "weights_filepath = \"weights.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    weights_filepath, monitor=\"loss\", verbose=0,\n",
    "    save_best_only=True, mode=\"min\")\n",
    "\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# Fit model\n",
    "model.fit(x_train, y_train, epochs=1, batch_size=400, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training model\n",
    "model.load_weights(\"weights.hdf5\")\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# Fit model\n",
    "model.fit(x_train, y_train, epochs=1, batch_size=200, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open pickled test data so you don't have to re-run create_training\n",
    "x_file_pickle = open('../data/x_test_pickle.obj', 'rb')\n",
    "y_file_pickle = open('../data/y_test_pickle.obj', 'rb')\n",
    "\n",
    "x_test = pickle.load(x_file_pickle)\n",
    "y_test = pickle.load(y_file_pickle)\n",
    "\n",
    "x_file_pickle.close()\n",
    "y_file_pickle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_predictions = decoder(predictions, num_to_char)\n",
    "print(text_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build & Compile LSTM (Model 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:<br /> https://benjamintseng.com/portfolio/nlp-pubmed-data-using-tensorflow-and-keras/ <br />\n",
    "https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_Builder1(song_text_nums, metadata_nums, str_length, vocab_size):\n",
    "    # Prepare data\n",
    "    if str_length+1 > len(song_text_nums[0]):\n",
    "        return \"Your string length is too long for the data.\"\n",
    "\n",
    "    # The x_values begin at the start of each song text and are str_length characters long;\n",
    "    # Concatenate these with the metadata at the beginning\n",
    "    # The y_values are one character after the end of the x_values\n",
    "    x_data = np.concatenate((metadata_nums[0], song_text_nums[0][0:str_length]))\n",
    "    y_data = [song_text_nums[0][str_length]]\n",
    "    \n",
    "    for ind, song in enumerate(song_text_nums[1:], start=1):\n",
    "        x_concat = np.concatenate((metadata_nums[i], np.array(song[0:str_length])))\n",
    "        x_data = np.vstack((x_data, x_concat))\n",
    "        y_data.append(song[str_length])\n",
    "\n",
    "    # Convert x and y data to tensors\n",
    "    x_train = to_categorical(x_data, num_classes=vocab_size)\n",
    "    y_train = to_categorical(y_data, num_classes=vocab_size)\n",
    "    \n",
    "    # Build NN\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.LSTM(128, input_shape=(x_train.shape[1], x_train.shape[2]), return_sequences=True))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(vocab_length, activation='softmax'))\n",
    "    \n",
    "    return x_train, y_train, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, mod1 = LSTM_Builder1(text_nums_list[:train_ind], meter_nums, 160, vocab_length)\n",
    "mod1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmsprop: Divide the learning rate for a weight by a running average of the\n",
    "# magnitues of the recent gradients for that weight\n",
    "mod1.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use checkpoints to save training weights before the model finishes training\n",
    "# Using this file path, the model checkpoints will be saved with the epoch number \n",
    "# and the validation loss in the filename.\n",
    "weights_filepath = \"mod1_weights.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    weights_filepath, monitor=\"loss\", verbose=0,\n",
    "    save_best_only=True, mode=\"min\")\n",
    "\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "mod1.fit(x_train, y_train, epochs=1000, batch_size=100, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get testing data\n",
    "test_nums = text_nums_list[train_ind:]\n",
    "meter_test = meter_nums[train_ind:]\n",
    "str_length = 160\n",
    "\n",
    "x_test = np.concatenate((meter_test[0], test_nums[0][0:str_length]))\n",
    "y_test = [test_nums[0][str_length]]\n",
    "\n",
    "# The x_values begin at the start of each song text and are str_length characters long;\n",
    "# Concatenate these with the metadata at the beginning\n",
    "# The y_values are one character after the end of the x_values    \n",
    "for ind, song in enumerate(test_nums[1:], start=1):\n",
    "    x_concat = np.concatenate((meter_test[ind], song[0:str_length]))\n",
    "    x_test = np.vstack((x_test, x_concat))\n",
    "    y_test.append(song[str_length])\n",
    "\n",
    "# Convert x and y data to tensors\n",
    "x_test = to_categorical(x_test, num_classes=vocab_length)\n",
    "y_test = to_categorical(y_test, num_classes=vocab_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(binary_matrix, dictionary):\n",
    "    '''Convert numeric list a text string.'''\n",
    "    text_list = []\n",
    "    \n",
    "    for row in binary_matrix:\n",
    "        max_ind = np.argmax(row)\n",
    "        text_list.append(dictionary[max_ind])\n",
    "    \n",
    "    return \"\".join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder2(pred_matrix, dictionary):\n",
    "    '''Convert numeric list a text string.'''\n",
    "    my_session = tf.Session()\n",
    "    \n",
    "    text_list = []\n",
    "    \n",
    "    samples = tf.random.categorical(pred_matrix, num_samples=1)\n",
    "    samples = tf.squeeze(samples, axis=-1)\n",
    "    vals = sess.run(samples)\n",
    "    \n",
    "    text_pred = \"\".join([dictionary[i] for i in vals])\n",
    "        \n",
    "    return text_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder1(predictions, num_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starter = x_test[0]\n",
    "len_starter = len(x_test[0])\n",
    "\n",
    "x_vals = np.reshape(starter, (1, starter.shape[0], starter.shape[1]))\n",
    "result = decoder(starter, num_to_char)\n",
    "seq_len = 200\n",
    "\n",
    "for i in range(1,seq_len):\n",
    "    prediction = mod2.predict(x_vals)\n",
    "    starter = np.vstack((starter, prediction))\n",
    "    starter = starter[1:1+len_starter,]\n",
    "    \n",
    "    x_vals = np.reshape(starter, (1, starter.shape[0], starter.shape[1]))\n",
    "    \n",
    "    text_prediction = decoder2(prediction, num_to_char)\n",
    "    result += text_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(predictions, num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1)\n",
    "sess.run(sampled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = tfp.distributions.OneHotCategorical(probs=prediction)\n",
    "sample = tfp.distributions.Sample(dist)\n",
    "sample\n",
    "sess = tf.Session()\n",
    "vals = sess.run(sampled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build & Compile LSTM (Model 3; using music21 attributes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import *\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../../Anis/data/jiggs.txt\", \"r\")\n",
    "raw_input = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Repertoir():\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        f = open(path, \"r\")\n",
    "        self.string = f.read()\n",
    "        self.handler = abcFormat.ABCHandler()\n",
    "        self.handler.process(self.string)\n",
    "        self.songs_handlers = self.handler.splitByReferenceNumber()\n",
    "        self.songs = {}\n",
    "        self.__process()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.string\n",
    "    \n",
    "    \n",
    "    def __process(self):\n",
    "        for ref_number, handler in self.songs_handlers.items():\n",
    "            self.songs[ref_number] = Song(handler)\n",
    "            \n",
    "    def get_part_vocab(self):\n",
    "        tokens = []\n",
    "        for ref_number, song in self.songs.items():\n",
    "            tokens+= song.part\n",
    "        tokens = list(set(tokens))            \n",
    "        return tokens\n",
    "    \n",
    "    def get_metadata_vocab(self, key):\n",
    "        tokens = []\n",
    "        for ref_number, song in self.songs.items():\n",
    "            tokens+= [song.metadata[key]]\n",
    "        tokens = list(set(tokens))            \n",
    "        return tokens    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Song():\n",
    "    def __init__(self, handler):\n",
    "        self.handler = handler\n",
    "        self.metadata = {\n",
    "            'X':1,\n",
    "            'T':'Unknown',\n",
    "            'S':'Unknown',\n",
    "            'M':'none',\n",
    "            'L':'',\n",
    "            'Q':'',\n",
    "            'K':''\n",
    "        }\n",
    "        self.part = []\n",
    "        self.__process()\n",
    "        \n",
    "    def __process(self):\n",
    "        for token in self.handler.tokens:\n",
    "            meta_data_ended=False\n",
    "            if isinstance(token, abcFormat.ABCMetadata):\n",
    "                if token.tag in self.metadata.keys():\n",
    "                    if self.metadata[token.tag]=='' or not meta_data_ended:\n",
    "                        self.metadata[token.tag] = token.data\n",
    "                else:\n",
    "                    self.metadata[token.tag] = token.data\n",
    "            elif isinstance(token, abcFormat.ABCNote ) or isinstance(token, abcFormat.ABCBar):\n",
    "                meta_data_ended = True\n",
    "                self.part.append(token.src)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.to_abc()\n",
    "    \n",
    "    def to_abc(self):\n",
    "        output = ''\n",
    "        for key, value in self.metadata.items():\n",
    "            output+= key+':'+value+\"\\n\"\n",
    "        for note in self.part:\n",
    "            output+=note\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_char_idx_mappings(vocab):\n",
    "    char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "    idx2char = np.array(vocab)\n",
    "    return char2idx, idx2char\n",
    "\n",
    "def get_input_tensors(part, k, m, part_char2idx, k_char2idx, m_char2idx):\n",
    "    part_tensor = torch.tensor([part_char2idx[note] for note in part[0:-1]], dtype=torch.long)\n",
    "    k_tensor = torch.tensor([k_char2idx[k] for note in part[0:-1]], dtype=torch.long)\n",
    "    m_tensor = torch.tensor([m_char2idx[m] for note in part[0:-1]], dtype=torch.long)\n",
    "    return part_tensor, k_tensor, m_tensor,\n",
    "\n",
    "def get_target_tensor(part, part_char2idx):\n",
    "    target_tensor = torch.tensor([part_char2idx[note] for note in part[1:]], dtype=torch.long)\n",
    "    return target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:1\n",
      "T:A and D\n",
      "S:EF\n",
      "M:6/8\n",
      "L:\n",
      "Q:\n",
      "K:D\n",
      "P:B\n",
      "f|\"A\"eccc2f|\"A\"eccc2f|\"A\"eccc2f|\"Bm\"BcB\"E7\"B2f|\"A\"eccc2f|\"A\"eccc2c/2d/2|\"D\"efe\"E7\"dcB|[1\"A\"Acea2:|[2\"A\"Aceag=g||\"D\"f2fFdd|\"D\"AFAf2e/2f/2|\"G\"g2gecd|\"Em\"efd\"A7\"cBA|\"D\"f^efdcd|\"D\"AFAf=ef|\"G\"gfg\"A7\"ABc|[1\"D\"d3d2e:|[2\"D\"d3d2||\n"
     ]
    }
   ],
   "source": [
    "rep = Repertoir('../../Anis/data/jiggs.txt')\n",
    "print(str(rep.songs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_vocab = rep.get_part_vocab()\n",
    "m_vocab = rep.get_metadata_vocab('M')\n",
    "k_vocab = rep.get_metadata_vocab('K')\n",
    "\n",
    "part_vocab_size = len(part_vocab)\n",
    "k_vocab_size = len(k_vocab)\n",
    "m_vocab_size = len(m_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_char2idx, part_idx2char = generate_char_idx_mappings(part_vocab)\n",
    "k_char2idx, k_idx2char = generate_char_idx_mappings(k_vocab)\n",
    "m_char2idx, m_idx2char = generate_char_idx_mappings(m_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_len = 60\n",
    "num_songs = int(0.9*len(rep.songs))\n",
    "k_len = min([len(rep.songs[i].metadata['K']) for i in range(1,num_songs+1)])\n",
    "m_len = min([len(rep.songs[i].metadata['M']) for i in range(1,num_songs+1)])\n",
    "\n",
    "part_num_matrix = np.zeros((num_songs, part_len))\n",
    "y_vals = []\n",
    "k_num_matrix = np.zeros((num_songs, 1))\n",
    "m_num_matrix = np.zeros((num_songs, m_len))\n",
    "\n",
    "for i in range(1,num_songs+1):\n",
    "    song = rep.songs[i]\n",
    "    part_num_matrix[i-1,] = np.array([part_char2idx[song.part[0:part_len][j]] for j in range(len(song.part[0:part_len]))])\n",
    "    k_num_matrix[i-1,0] = [k_char2idx[song.metadata['K']]][0]\n",
    "    m_num_matrix[i-1,] = [m_char2idx[song.metadata['M']]]\n",
    "    y_vals.append(part_char2idx[song.part[part_len]])\n",
    "\n",
    "# Convert y_vals to one_hot_encoded vectors\n",
    "y_data = to_categorical(y_vals, num_classes=part_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LSTM NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_Builder2(lstm_dim, dropout_pct, batch_size,\n",
    "                  embedding_dim, seq_len,\n",
    "                  part_voc_size, k_voc_size, m_voc_size, \n",
    "                  part_num_matrix, k_num_matrix, m_num_matrix):\n",
    "    \n",
    "    voc_input = Input(shape=(part_num_matrix.shape[1],))\n",
    "    k_input = Input(shape=(k_num_matrix.shape[1],))\n",
    "    m_input = Input(shape=(m_num_matrix.shape[1],))\n",
    "    \n",
    "    voc_embedding = Embedding(part_voc_size, embedding_dim)(voc_input)\n",
    "    \n",
    "    lstm_out = LSTM(lstm_dim, dropout=dropout_pct)(voc_embedding)\n",
    "    \n",
    "    concat = Concatenate()([lstm_out, k_input, m_input])\n",
    "\n",
    "    output = Dense(part_voc_size, activation='softmax')(concat)\n",
    "    \n",
    "    model = Model(inputs=[voc_input, k_input, m_input], outputs=output)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod2 = LSTM_Builder2(256, 0.2, 100, \n",
    "                     128, 60,\n",
    "                     part_vocab_size, k_vocab_size, m_vocab_size,\n",
    "                     part_num_matrix, \n",
    "                     k_num_matrix, \n",
    "                     m_num_matrix)\n",
    "\n",
    "mod2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://benjamintseng.com/portfolio/nlp-pubmed-data-using-tensorflow-and-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_Builder3(lstm_dims, dropout_pct, batch_size,\n",
    "                  embedding_dim, seq_len,\n",
    "                  part_voc_size, k_voc_size, m_voc_size, \n",
    "                  part_num_matrix, k_num_matrix, m_num_matrix):\n",
    "    \n",
    "    voc_input = Input(shape=(part_num_matrix.shape[1],))\n",
    "    k_input = Input(shape=(k_num_matrix.shape[1],))\n",
    "    m_input = Input(shape=(m_num_matrix.shape[1],))\n",
    "    \n",
    "    voc_embedding = Embedding(part_voc_size, embedding_dim)(voc_input)\n",
    "    k_embedding = Embedding(k_voc_size, embedding_dim)(k_input)\n",
    "    m_embedding = Embedding(m_voc_size, embedding_dim)(m_input)\n",
    "    \n",
    "    concat = Concatenate(axis=1)([m_embedding, k_embedding, voc_embedding])\n",
    "    \n",
    "    lstm1 = LSTM(lstm_dims[0], dropout=dropout_pct, return_sequences=True)(concat)\n",
    "    lstm2 = LSTM(lstm_dims[1], dropout=dropout_pct, return_sequences=True)(lstm1)\n",
    "    lstm3 = LSTM(lstm_dims[2], dropout=dropout_pct)(lstm2)\n",
    "\n",
    "    output = Dense(part_voc_size, activation='softmax')(lstm3)\n",
    "    \n",
    "    model = Model(inputs=[voc_input, k_input, m_input], outputs=output)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod3 = LSTM_Builder3([256, 512, 256], 0.2, 100, \n",
    "                     256, 60,\n",
    "                     part_vocab_size, k_vocab_size, m_vocab_size,\n",
    "                     part_num_matrix, \n",
    "                     k_num_matrix, \n",
    "                     m_num_matrix)\n",
    "\n",
    "mod3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod3.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use checkpoints to save training weights before the model finishes training\n",
    "# Using this file path, the model checkpoints will be saved with the epoch number \n",
    "# and the validation loss in the filename.\n",
    "weights_filepath = \"mod3_weights.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    weights_filepath, monitor=\"loss\", verbose=0,\n",
    "    save_best_only=True, mode=\"min\")\n",
    "\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod3.fit([part_num_matrix, k_num_matrix, m_num_matrix],\n",
    "         y_data, \n",
    "         epochs=5000, batch_size=50, shuffle=True, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_num_matrix.shape, k_num_matrix.shape, m_num_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod3.predict([part_num_matrix, k_num_matrix, m_num_matrix]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up testing data\n",
    "my_session = tf.Session()\n",
    "mod3.reset_states()\n",
    "\n",
    "starter = rep.songs[num_songs]\n",
    "start_ind = int(0.9*len(rep.songs))\n",
    "end_ind = len(rep.songs)\n",
    "seq_len = 60\n",
    "\n",
    "input_part = np.zeros((1, part_len))\n",
    "sequences = input_part\n",
    "input_k = np.zeros((1, 1))\n",
    "input_m = np.zeros((1, m_len))\n",
    "\n",
    "for i in range(start_ind,start_ind+1):\n",
    "    song = rep.songs[i]\n",
    "    input_part[i-start_ind,] = np.array([part_char2idx[song.part[0:part_len][j]] for j in range(len(song.part[0:part_len]))])\n",
    "    input_k[i-start_ind,0] = [k_char2idx[song.metadata['K']]][0]\n",
    "    input_m[i-start_ind,] = [m_char2idx[song.metadata['M']]]\n",
    "\n",
    "# Starting string:\n",
    "beginning = str(rep.songs[start_ind])\n",
    "result = []\n",
    "\n",
    "\n",
    "# Generate predictions\n",
    "for i in range(seq_len):\n",
    "    prediction = mod3.predict([input_part, input_k, input_m])\n",
    "    \n",
    "    # Sample from output of probabilities\n",
    "    sample = tf.random.categorical(prediction, num_samples=1)\n",
    "    sample = tf.squeeze(sample, axis=-1)\n",
    "    numeric_pred = my_session.run(sample)[0]\n",
    "    \n",
    "    # Pass the prediction, along with the hidden state, back to the model\n",
    "    sequences = np.append(sequences, numeric_pred)\n",
    "    input_part = sequences[i:part_len+i].reshape(1, part_len)\n",
    "    \n",
    "    # Text predictions\n",
    "    text_val = part_idx2char[numeric_pred]\n",
    "    result.append(text_val)\n",
    "\n",
    "result = \"\".join(result)\n",
    "print(beginning)\n",
    "print(\"______\")\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: music21 attributes in single character encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "jig_rep = Repertoir('../data/Jigs.txt')\n",
    "\n",
    "num_to_char, char_to_num = create_dictionaries(str(jig_rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_length = len(num_to_char)\n",
    "\n",
    "num_songs = int(0.9*len(jig_rep.songs))\n",
    "total_songs = len(jig_rep.songs)\n",
    "\n",
    "part_len = min(len(list(\"\".join(jig_rep.songs[i].part))) for i in range(1,total_songs+1)) - 2\n",
    "k_len = min([len(list(jig_rep.songs[i].metadata['K'])) for i in range(1,total_songs+1)])\n",
    "m_len = min([len(list(jig_rep.songs[i].metadata['M'])) for i in range(1,total_songs+1)])\n",
    "\n",
    "part_num_matrix = np.zeros((num_songs, part_len))\n",
    "y_vals = []\n",
    "k_num_matrix = np.zeros((num_songs, k_len))\n",
    "m_num_matrix = np.zeros((num_songs, m_len))\n",
    "\n",
    "for i in range(1,num_songs+1):\n",
    "    song = jig_rep.songs[i]\n",
    "    part_string = \"\".join(song.part)\n",
    "    part_num_matrix[i-1,] = np.array([char_to_num[part_string[0:part_len][j]] for j in range(part_len)])\n",
    "    \n",
    "    k_string = \"\".join(song.metadata['K'])\n",
    "    k_num_matrix[i-1,0] = np.array([char_to_num[k_string[j]] for j in range(k_len)])\n",
    "    \n",
    "    m_string = \"\".join(song.metadata['M'])\n",
    "    m_num_matrix[i-1,] = np.array([char_to_num[m_string[j]] for j in range(m_len)])\n",
    "    \n",
    "    y_vals.append(char_to_num[part_string[part_len]])\n",
    "\n",
    "# Convert y_vals to one_hot_encoded vectors\n",
    "y_data = to_categorical(y_vals, num_classes=vocab_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((306, 150), (306, 1), (306, 3))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part_num_matrix.shape, k_num_matrix.shape, m_num_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_Builder4(lstm_dims, dropout_pct, batch_size,\n",
    "                  embedding_dim,\n",
    "                  vocab_size, \n",
    "                  part_num_matrix, k_num_matrix, m_num_matrix):\n",
    "    \n",
    "    voc_input = Input(shape=(part_num_matrix.shape[1],))\n",
    "    k_input = Input(shape=(k_num_matrix.shape[1],))\n",
    "    m_input = Input(shape=(m_num_matrix.shape[1],))\n",
    "    \n",
    "    voc_embedding = Embedding(vocab_size, embedding_dim)(voc_input)\n",
    "    k_embedding = Embedding(vocab_size, embedding_dim)(k_input)\n",
    "    m_embedding = Embedding(vocab_size, embedding_dim)(m_input)\n",
    "    \n",
    "    concat = Concatenate(axis=1)([m_embedding, k_embedding, voc_embedding])\n",
    "    \n",
    "    lstm1 = LSTM(lstm_dims[0], dropout=dropout_pct, return_sequences=True)(concat)\n",
    "    lstm2 = LSTM(lstm_dims[1], dropout=dropout_pct, return_sequences=True)(lstm1)\n",
    "    lstm3 = LSTM(lstm_dims[2], dropout=dropout_pct)(lstm2)\n",
    "\n",
    "    output = Dense(vocab_size, activation='softmax')(lstm3)\n",
    "    \n",
    "    model = Model(inputs=[voc_input, k_input, m_input], outputs=output)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_21 (InputLayer)           (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_19 (InputLayer)           (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_21 (Embedding)        (None, 3, 256)       22272       input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_20 (Embedding)        (None, 1, 256)       22272       input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_19 (Embedding)        (None, 150, 256)     22272       input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 154, 256)     0           embedding_21[0][0]               \n",
      "                                                                 embedding_20[0][0]               \n",
      "                                                                 embedding_19[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_19 (LSTM)                  (None, 154, 200)     365600      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_20 (LSTM)                  (None, 154, 336)     721728      lstm_19[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_21 (LSTM)                  (None, 200)          429600      lstm_20[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 87)           17487       lstm_21[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,601,231\n",
      "Trainable params: 1,601,231\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mod4 = LSTM_Builder4([200, 336, 200], 0.2, 50, \n",
    "                     256,vocab_length,\n",
    "                     part_num_matrix, k_num_matrix, m_num_matrix)\n",
    "\n",
    "mod4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod4.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use checkpoints to save training weights before the model finishes training\n",
    "weights_filepath = \"mod4_weights.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    weights_filepath, monitor=\"loss\", verbose=0,\n",
    "    save_best_only=True, mode=\"min\")\n",
    "\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "306/306 [==============================] - 11s 37ms/step - loss: 4.1881\n",
      "Epoch 2/5000\n",
      "306/306 [==============================] - 8s 27ms/step - loss: 3.1653\n",
      "Epoch 3/5000\n",
      "306/306 [==============================] - 10s 33ms/step - loss: 2.9420\n",
      "Epoch 4/5000\n",
      "306/306 [==============================] - 10s 34ms/step - loss: 2.8932\n",
      "Epoch 5/5000\n",
      "306/306 [==============================] - 11s 35ms/step - loss: 2.8719\n",
      "Epoch 6/5000\n",
      "306/306 [==============================] - 12s 39ms/step - loss: 2.8420\n",
      "Epoch 7/5000\n",
      " 50/306 [===>..........................] - ETA: 10s - loss: 2.7638"
     ]
    }
   ],
   "source": [
    "mod4.fit([part_num_matrix, k_num_matrix, m_num_matrix],\n",
    "         y_data, \n",
    "         epochs=5000, batch_size=50, shuffle=True, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up testing data\n",
    "my_session = tf.Session()\n",
    "mod4.reset_states()\n",
    "\n",
    "starter = jig_rep.songs[num_songs]\n",
    "start_ind = num_songs\n",
    "end_ind = len(jig_rep.songs)\n",
    "seq_len = 100\n",
    "\n",
    "input_part = np.zeros((1, part_len))\n",
    "sequences = input_part\n",
    "input_k = np.zeros((1, k_len))\n",
    "input_m = np.zeros((1, m_len))\n",
    "\n",
    "for i in range(start_ind,start_ind+1):\n",
    "    song = jig_rep.songs[i]\n",
    "    \n",
    "    part_string = \"\".join(song.part)\n",
    "    k_string = \"\".join(song.metadata['K'])\n",
    "    m_string = \"\".join(song.metadata['M'])\n",
    "    \n",
    "    input_part[i-start_ind,] = np.array([char_to_num[part_string[0:part_len][j]] for j in range(part_len)])\n",
    "    input_k[i-start_ind,] = np.array([char_to_num[k_string[j]] for j in range(k_len)])\n",
    "    input_m[i-start_ind,] = np.array([char_to_num[m_string[j]] for j in range(m_len)])\n",
    "\n",
    "# Starting string:\n",
    "beginning = str(jig_rep.songs[start_ind])\n",
    "result = []\n",
    "\n",
    "\n",
    "# Generate predictions\n",
    "for i in range(seq_len):\n",
    "    prediction = mod4.predict([input_part, input_k, input_m])\n",
    "    \n",
    "    # Sample from output of probabilities\n",
    "    sample = tf.random.categorical(prediction, num_samples=1)\n",
    "    sample = tf.squeeze(sample, axis=-1)\n",
    "    numeric_pred = my_session.run(sample)[0]\n",
    "    \n",
    "    # Pass the prediction, along with the hidden state, back to the model\n",
    "    sequences = np.append(sequences, numeric_pred)\n",
    "    input_part = sequences[i:part_len+i].reshape(1, part_len)\n",
    "    \n",
    "    # Text predictions\n",
    "    text_val = num_to_char[numeric_pred]\n",
    "    result.append(text_val)\n",
    "\n",
    "result = \"\".join(result)\n",
    "print(beginning)\n",
    "print(\"______\")\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5: Train Model on Multiple Repertoires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ashover_rep = Repertoir('../data/Ashover.txt')\n",
    "carols_rep = Repertoir('../data/Carols.txt')\n",
    "waltzes_rep = Repertoir('../data/Waltzes.txt')\n",
    "slip_jigs_rep = Repertoir('../data/Slip Jigs.txt')\n",
    "reels_uz_rep = Repertoir('../data/Reels U-Z.txt')\n",
    "reels_rt_rep = Repertoir('../data/Reels R-T.txt')\n",
    "reels_mq_rep = Repertoir('../data/Reels M-Q.txt')\n",
    "#reels_hl_rep = Repertoir('../data/Reels H-L.txt') # There is somthing wrong with these files\n",
    "#reels_dg_rep = Repertoir('../data/Reels D-G.txt') # Ignoring\n",
    "reels_ac_rep = Repertoir('../data/Reels A-C.txt')\n",
    "playford_rep = Repertoir('../data/Playford.txt')\n",
    "morris_rep = Repertoir('../data/Morris.txt')\n",
    "jigs_rep = Repertoir('../data/Jigs.txt')\n",
    "hornpipes_rep = Repertoir('../data/Hornpipes.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_list = [ashover_rep, carols_rep, waltzes_rep, slip_jigs_rep,\n",
    "           reels_uz_rep, reels_rt_rep, reels_mq_rep, #reels_hl_rep,\n",
    "           #reels_dg_rep, \n",
    "            reels_ac_rep, playford_rep, morris_rep,\n",
    "           jigs_rep, hornpipes_rep]\n",
    "\n",
    "combined_text = \"\"\n",
    "\n",
    "for rep in rep_list:\n",
    "    combined_text += str(rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocab for all repertoires\n",
    "num_to_char, char_to_num = create_dictionaries(combined_text)\n",
    "vocab_length = len(num_to_char)\n",
    "vocab_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training data for all repertoires combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_length = len(num_to_char)\n",
    "num_song_list = []\n",
    "\n",
    "# Get list of all songs in combined repertoire\n",
    "song_list = []\n",
    "\n",
    "for rep in rep_list:\n",
    "    for i in rep.songs:\n",
    "        song_list.append(rep.songs[i])\n",
    "    \n",
    "for r in rep_list:\n",
    "    n_songs = len(r.songs)\n",
    "    part_len_tmp = min(len(list(\"\".join(r.songs[i].part))) for i in range(1,n_songs+1)) - 2\n",
    "    k_len_tmp = min([len(list(r.songs[i].metadata['K'])) for i in range(1,n_songs+1)])\n",
    "    m_len_tmp = min([len(list(r.songs[i].metadata['M'])) for i in range(1,n_songs+1)])\n",
    "    \n",
    "    if part_len_tmp < part_len: part_len = part_len_tmp\n",
    "    if k_len_tmp < k_len: k_len = k_len_tmp\n",
    "    if m_len_tmp < m_len: m_len = m_len_tmp\n",
    "        \n",
    "print(part_len, k_len, m_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_songs = len(song_list)\n",
    "total_songs_train = int(0.95*total_songs)\n",
    "\n",
    "part_num_matrix = np.zeros((total_songs_train, part_len))\n",
    "y_vals = []\n",
    "k_num_matrix = np.zeros((total_songs_train, k_len))\n",
    "m_num_matrix = np.zeros((total_songs_train, m_len))\n",
    "\n",
    "row_ind = 0 # row index\n",
    "\n",
    "\n",
    "for i in range(total_songs_train):\n",
    "    song = song_list[i]\n",
    "    part_string = \"\".join(song.part)\n",
    "    part_num_matrix[row_ind,] = np.array([char_to_num[part_string[0:part_len][j]] for j in range(part_len)])\n",
    "\n",
    "    k_string = \"\".join(song.metadata['K'])\n",
    "    k_num_matrix[row_ind,0] = np.array([char_to_num[k_string[j]] for j in range(k_len)])\n",
    "\n",
    "    m_string = \"\".join(song.metadata['M'])\n",
    "    m_num_matrix[row_ind,] = np.array([char_to_num[m_string[j]] for j in range(m_len)])\n",
    "\n",
    "    y_vals.append(char_to_num[part_string[part_len]])\n",
    "\n",
    "    # increase row index after every song\n",
    "    row_ind += 1\n",
    "\n",
    "# Convert y_vals to one_hot_encoded vectors\n",
    "y_data = to_categorical(y_vals, num_classes=vocab_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_num_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod5 = LSTM_Builder4([200, 336, 200], 0.2, 50, \n",
    "                     256,vocab_length,\n",
    "                     part_num_matrix, k_num_matrix, m_num_matrix)\n",
    "\n",
    "mod5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod5.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use checkpoints to save training weights before the model finishes training\n",
    "weights_filepath = \"mod5_weights.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    weights_filepath, monitor=\"loss\", verbose=0,\n",
    "    save_best_only=True, mode=\"min\")\n",
    "\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod5.fit([part_num_matrix, k_num_matrix, m_num_matrix],\n",
    "         y_data, \n",
    "         epochs=5000, batch_size=100, shuffle=True, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up testing data\n",
    "my_session = tf.Session()\n",
    "mod5.reset_states()\n",
    "\n",
    "starter = song_list[total_songs_train]\n",
    "start_ind = total_songs_train\n",
    "end_ind = len(song_list)\n",
    "seq_len = 40\n",
    "\n",
    "input_part = np.zeros((1, part_len))\n",
    "sequences = input_part\n",
    "input_k = np.zeros((1, k_len))\n",
    "input_m = np.zeros((1, m_len))\n",
    "\n",
    "for i in range(start_ind,start_ind+1):\n",
    "    song = song_list[i]\n",
    "    \n",
    "    part_string = \"\".join(song.part)\n",
    "    k_string = \"\".join(song.metadata['K'])\n",
    "    m_string = \"\".join(song.metadata['M'])\n",
    "    \n",
    "    input_part[i-start_ind,] = np.array([char_to_num[part_string[0:part_len][j]] for j in range(part_len)])\n",
    "    input_k[i-start_ind,] = np.array([char_to_num[k_string[j]] for j in range(k_len)])\n",
    "    input_m[i-start_ind,] = np.array([char_to_num[m_string[j]] for j in range(m_len)])\n",
    "\n",
    "# Starting string:\n",
    "beginning = str(starter)\n",
    "result = []\n",
    "\n",
    "\n",
    "# Generate predictions\n",
    "for i in range(seq_len):\n",
    "    prediction = mod5.predict([input_part, input_k, input_m])\n",
    "    \n",
    "    # Sample from output of probabilities\n",
    "    sample = tf.random.categorical(prediction, num_samples=1)\n",
    "    sample = tf.squeeze(sample, axis=-1)\n",
    "    numeric_pred = my_session.run(sample)[0]\n",
    "    \n",
    "    # Pass the prediction, along with the hidden state, back to the model\n",
    "    sequences = np.append(sequences, numeric_pred)\n",
    "    input_part = sequences[i:part_len+i].reshape(1, part_len)\n",
    "    \n",
    "    # Text predictions\n",
    "    text_val = num_to_char[numeric_pred]\n",
    "    result.append(text_val)\n",
    "\n",
    "result = \"\".join(result)\n",
    "print(beginning)\n",
    "print(\"______\")\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
