{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import h5py\n",
    "from random import sample\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Input\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import LSTM, Embedding\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras import backend as K\n",
    "from feature_funcs import *\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary\n",
    "with open(\"../data/Jigs.txt\") as my_file:\n",
    "    abc_text = my_file.read()\n",
    "\n",
    "# Cut out unnecessary backslashes\n",
    "abc_text = re.sub('\\\\\\\\+\\n', '\\n', abc_text)\n",
    "\n",
    "# Find starting index of the data we care about\n",
    "start_ind = abc_text.find(\"X:\")\n",
    "abc_text = abc_text[start_ind:]\n",
    "\n",
    "# Encode data\n",
    "num_to_char, char_to_num = create_dictionaries(abc_text)\n",
    "vocab_length = len(num_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate Songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = re.findall(r'(T:(?:.+\\n)+\\n+)X:', abc_text)\n",
    "\n",
    "song_texts = []\n",
    "\n",
    "for song in songs:\n",
    "    song_text = re.search(r'(P:.+|K:.+)', song, re.DOTALL)\n",
    "    song_texts.append(song_text[0])\n",
    "    \n",
    "# Get aggregate statistics about song texts\n",
    "print(\"Min text length: \", min([len(text) for text in song_texts]))\n",
    "print(\"Max text length: \", max([len(text) for text in song_texts]))\n",
    "print(\"Average text length: \", np.mean([len(text) for text in song_texts]))\n",
    "\n",
    "# Make sure all song texts are the same length\n",
    "song_texts = [text[:170] for text in song_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Metadata elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract titles\n",
    "titles = []\n",
    "\n",
    "for song in songs:\n",
    "    title = re.search(r'(T:.+\\n% Nottingham Music Database\\n)', song).group(1)\n",
    "    titles.append(title)\n",
    "    \n",
    "# Pad titles to same length\n",
    "max_length = max(len(title) for title in titles)\n",
    "titles = [title.ljust(max_length) for title in titles]\n",
    "\n",
    "\n",
    "# Extract authors\n",
    "authors = []\n",
    "\n",
    "for song in songs:\n",
    "    author = re.search(r'\\n(S:.+\\n)', song).group(1)\n",
    "    authors.append(author)\n",
    "    \n",
    "# Pad authors to same length\n",
    "max_length = max(len(author) for author in authors)\n",
    "authors = [author.ljust(max_length) for author in authors]\n",
    "\n",
    "\n",
    "# Extract meters\n",
    "meters = []\n",
    "\n",
    "for song in songs:\n",
    "    meter = re.search(r'(M:\\d/\\d\\n)', song).group(1)\n",
    "    meters.append(meter)\n",
    "\n",
    "    \n",
    "# Pad meters to same length\n",
    "max_length = max(len(meter) for meter in meters)\n",
    "meters = [meter.ljust(max_length) for meter in meters]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode data to numeric format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_nums_list = encoder2(song_texts, char_to_num)\n",
    "\n",
    "title_nums = encoder2(titles, char_to_num)\n",
    "author_nums = encoder2(authors, char_to_num)\n",
    "meter_nums = encoder2(meters, char_to_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meter_nums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Use Pickled data where songs are not separated (data already encoded to numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open pickled training data so you don't have to re-run create_training\n",
    "x_file_pickle = open('../data/x_train_pickle.obj', 'rb')\n",
    "y_file_pickle = open('../data/y_train_pickle.obj', 'rb')\n",
    "\n",
    "x_train = pickle.load(x_file_pickle)\n",
    "y_train = pickle.load(y_file_pickle)\n",
    "\n",
    "x_file_pickle.close()\n",
    "y_file_pickle.close()\n",
    "\n",
    "vocab_length = x_train.shape[2]\n",
    "vocab_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape, x_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "print(vocab_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build & Compile LSTM (Model 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources: <br />\n",
    "https://towardsdatascience.com/how-to-generate-music-using-a-lstm-neural-network-in-keras-68786834d4c5  <br />\n",
    "https://github.com/aamini/introtodeeplearning/blob/master/lab1/Part2_Music_Generation.ipynb  <br /> https://medium.com/datadriveninvestor/music-generation-using-deep-learning-85010fb982e2 \n",
    "<br /> https://keras.io/examples/lstm_text_generation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(x_train.shape[1], x_train.shape[2]), return_sequences=True))\n",
    "model.add(layers.LSTM(256, return_sequences=True))\n",
    "model.add(layers.LSTM(512))\n",
    "model.add(layers.Dense(vocab_length, activation='softmax'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmsprop: Divide the learning rate for a weight by a running average of the\n",
    "# magnitues of the recent gradients for that weight\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train RNN\n",
    "Epoch: When the Neural Network sees all of the training data <br />\n",
    "Batch: Subset of the data <br />\n",
    "i.e. If you have 1000 data points, your batch size is 500 and you want 1 epoch, then the NN will do 2 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use checkpoints to save training weights before the model finishes training\n",
    "# Using this file path, the model checkpoints will be saved with the epoch number \n",
    "# and the validation loss in the filename.\n",
    "weights_filepath = \"weights.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    weights_filepath, monitor=\"loss\", verbose=0,\n",
    "    save_best_only=True, mode=\"min\")\n",
    "\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# Fit model\n",
    "model.fit(x_train, y_train, epochs=1, batch_size=400, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training model\n",
    "model.load_weights(\"weights.hdf5\")\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# Fit model\n",
    "model.fit(x_train, y_train, epochs=1, batch_size=200, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open pickled test data so you don't have to re-run create_training\n",
    "x_file_pickle = open('../data/x_test_pickle.obj', 'rb')\n",
    "y_file_pickle = open('../data/y_test_pickle.obj', 'rb')\n",
    "\n",
    "x_test = pickle.load(x_file_pickle)\n",
    "y_test = pickle.load(y_file_pickle)\n",
    "\n",
    "x_file_pickle.close()\n",
    "y_file_pickle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_predictions = decoder(predictions, num_to_char)\n",
    "print(text_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build & Compile LSTM (Model 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:<br /> https://benjamintseng.com/portfolio/nlp-pubmed-data-using-tensorflow-and-keras/ <br />\n",
    "https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_Builder1(song_text_nums, metadata_nums, str_length, vocab_size):\n",
    "    # Prepare data\n",
    "    if str_length+1 > len(song_text_nums[0]):\n",
    "        return \"Your string length is too long for the data.\"\n",
    "\n",
    "    # The x_values begin at the start of each song text and are str_length characters long;\n",
    "    # Concatenate these with the metadata at the beginning\n",
    "    # The y_values are one character after the end of the x_values\n",
    "    x_data = np.concatenate((metadata_nums[0], song_text_nums[0][0:str_length]))\n",
    "    y_data = [song_text_nums[0][str_length]]\n",
    "    \n",
    "    for ind, song in enumerate(song_text_nums[1:], start=1):\n",
    "        x_concat = np.concatenate((metadata_nums[i], np.array(song[0:str_length])))\n",
    "        x_data = np.vstack((x_data, x_concat))\n",
    "        y_data.append(song[str_length])\n",
    "\n",
    "    # Convert x and y data to tensors\n",
    "    x_train = to_categorical(x_data, num_classes=vocab_size)\n",
    "    y_train = to_categorical(y_data, num_classes=vocab_size)\n",
    "    \n",
    "    # Build NN\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.LSTM(128, input_shape=(x_train.shape[1], x_train.shape[2]), return_sequences=True))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(vocab_length, activation='softmax'))\n",
    "    \n",
    "    return x_train, y_train, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, mod1 = LSTM_Builder1(text_nums_list[:train_ind], meter_nums, 160, vocab_length)\n",
    "mod1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmsprop: Divide the learning rate for a weight by a running average of the\n",
    "# magnitues of the recent gradients for that weight\n",
    "mod1.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use checkpoints to save training weights before the model finishes training\n",
    "# Using this file path, the model checkpoints will be saved with the epoch number \n",
    "# and the validation loss in the filename.\n",
    "weights_filepath = \"mod1_weights.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    weights_filepath, monitor=\"loss\", verbose=0,\n",
    "    save_best_only=True, mode=\"min\")\n",
    "\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "mod1.fit(x_train, y_train, epochs=1000, batch_size=100, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get testing data\n",
    "test_nums = text_nums_list[train_ind:]\n",
    "meter_test = meter_nums[train_ind:]\n",
    "str_length = 160\n",
    "\n",
    "x_test = np.concatenate((meter_test[0], test_nums[0][0:str_length]))\n",
    "y_test = [test_nums[0][str_length]]\n",
    "\n",
    "# The x_values begin at the start of each song text and are str_length characters long;\n",
    "# Concatenate these with the metadata at the beginning\n",
    "# The y_values are one character after the end of the x_values    \n",
    "for ind, song in enumerate(test_nums[1:], start=1):\n",
    "    x_concat = np.concatenate((meter_test[ind], song[0:str_length]))\n",
    "    x_test = np.vstack((x_test, x_concat))\n",
    "    y_test.append(song[str_length])\n",
    "\n",
    "# Convert x and y data to tensors\n",
    "x_test = to_categorical(x_test, num_classes=vocab_length)\n",
    "y_test = to_categorical(y_test, num_classes=vocab_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(binary_matrix, dictionary):\n",
    "    '''Convert numeric list a text string.'''\n",
    "    text_list = []\n",
    "    \n",
    "    for row in binary_matrix:\n",
    "        max_ind = np.argmax(row)\n",
    "        text_list.append(dictionary[max_ind])\n",
    "    \n",
    "    return \"\".join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder2(pred_matrix, dictionary):\n",
    "    '''Convert numeric list a text string.'''\n",
    "    my_session = tf.Session()\n",
    "    \n",
    "    text_list = []\n",
    "    \n",
    "    samples = tf.random.categorical(pred_matrix, num_samples=1)\n",
    "    samples = tf.squeeze(samples, axis=-1)\n",
    "    vals = sess.run(samples)\n",
    "    \n",
    "    text_pred = \"\".join([dictionary[i] for i in vals])\n",
    "        \n",
    "    return text_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder1(predictions, num_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starter = x_test[0]\n",
    "len_starter = len(x_test[0])\n",
    "\n",
    "x_vals = np.reshape(starter, (1, starter.shape[0], starter.shape[1]))\n",
    "result = decoder(starter, num_to_char)\n",
    "seq_len = 200\n",
    "\n",
    "for i in range(1,seq_len):\n",
    "    prediction = mod2.predict(x_vals)\n",
    "    starter = np.vstack((starter, prediction))\n",
    "    starter = starter[1:1+len_starter,]\n",
    "    \n",
    "    x_vals = np.reshape(starter, (1, starter.shape[0], starter.shape[1]))\n",
    "    \n",
    "    text_prediction = decoder2(prediction, num_to_char)\n",
    "    result += text_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(predictions, num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1)\n",
    "sess.run(sampled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = tfp.distributions.OneHotCategorical(probs=prediction)\n",
    "sample = tfp.distributions.Sample(dist)\n",
    "sample\n",
    "sess = tf.Session()\n",
    "vals = sess.run(sampled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build & Compile LSTM (Model 3; using music21 attributes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import *\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../../Anis/data/jiggs.txt\", \"r\")\n",
    "raw_input = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Repertoir():\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        f = open(path, \"r\")\n",
    "        self.string = f.read()\n",
    "        self.handler = abcFormat.ABCHandler()\n",
    "        self.handler.process(self.string)\n",
    "        self.songs_handlers = self.handler.splitByReferenceNumber()\n",
    "        self.songs = {}\n",
    "        self.__process()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.string\n",
    "    \n",
    "    \n",
    "    def __process(self):\n",
    "        for ref_number, handler in self.songs_handlers.items():\n",
    "            self.songs[ref_number] = Song(handler)\n",
    "            \n",
    "    def get_part_vocab(self):\n",
    "        tokens = []\n",
    "        for ref_number, song in self.songs.items():\n",
    "            tokens+= song.part\n",
    "        tokens = list(set(tokens))            \n",
    "        return tokens\n",
    "    \n",
    "    def get_metadata_vocab(self, key):\n",
    "        tokens = []\n",
    "        for ref_number, song in self.songs.items():\n",
    "            tokens+= [song.metadata[key]]\n",
    "        tokens = list(set(tokens))            \n",
    "        return tokens    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Song():\n",
    "    def __init__(self, handler):\n",
    "        self.handler = handler\n",
    "        self.metadata = {\n",
    "            'X':1,\n",
    "            'T':'Unknown',\n",
    "            'S':'Unknown',\n",
    "            'M':'none',\n",
    "            'L':'',\n",
    "            'Q':'',\n",
    "            'K':''\n",
    "        }\n",
    "        self.part = []\n",
    "        self.__process()\n",
    "        \n",
    "    def __process(self):\n",
    "        for token in self.handler.tokens:\n",
    "            meta_data_ended=False\n",
    "            if isinstance(token, abcFormat.ABCMetadata):\n",
    "                if token.tag in self.metadata.keys():\n",
    "                    if self.metadata[token.tag]=='' or not meta_data_ended:\n",
    "                        self.metadata[token.tag] = token.data\n",
    "                else:\n",
    "                    self.metadata[token.tag] = token.data\n",
    "            elif isinstance(token, abcFormat.ABCNote ) or isinstance(token, abcFormat.ABCBar):\n",
    "                meta_data_ended = True\n",
    "                self.part.append(token.src)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.to_abc()\n",
    "    \n",
    "    def to_abc(self):\n",
    "        output = ''\n",
    "        for key, value in self.metadata.items():\n",
    "            output+= key+':'+value+\"\\n\"\n",
    "        for note in self.part:\n",
    "            output+=note\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_char_idx_mappings(vocab):\n",
    "    char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "    idx2char = np.array(vocab)\n",
    "    return char2idx, idx2char\n",
    "\n",
    "def get_input_tensors(part, k, m, part_char2idx, k_char2idx, m_char2idx):\n",
    "    part_tensor = torch.tensor([part_char2idx[note] for note in part[0:-1]], dtype=torch.long)\n",
    "    k_tensor = torch.tensor([k_char2idx[k] for note in part[0:-1]], dtype=torch.long)\n",
    "    m_tensor = torch.tensor([m_char2idx[m] for note in part[0:-1]], dtype=torch.long)\n",
    "    return part_tensor, k_tensor, m_tensor,\n",
    "\n",
    "def get_target_tensor(part, part_char2idx):\n",
    "    target_tensor = torch.tensor([part_char2idx[note] for note in part[1:]], dtype=torch.long)\n",
    "    return target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:1\n",
      "T:A and D\n",
      "S:EF\n",
      "M:6/8\n",
      "L:\n",
      "Q:\n",
      "K:D\n",
      "P:B\n",
      "f|\"A\"eccc2f|\"A\"eccc2f|\"A\"eccc2f|\"Bm\"BcB\"E7\"B2f|\"A\"eccc2f|\"A\"eccc2c/2d/2|\"D\"efe\"E7\"dcB|[1\"A\"Acea2:|[2\"A\"Aceag=g||\"D\"f2fFdd|\"D\"AFAf2e/2f/2|\"G\"g2gecd|\"Em\"efd\"A7\"cBA|\"D\"f^efdcd|\"D\"AFAf=ef|\"G\"gfg\"A7\"ABc|[1\"D\"d3d2e:|[2\"D\"d3d2||\n"
     ]
    }
   ],
   "source": [
    "rep = Repertoir('../../Anis/data/jiggs.txt')\n",
    "print(str(rep.songs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_vocab = rep.get_part_vocab()\n",
    "m_vocab = rep.get_metadata_vocab('M')\n",
    "k_vocab = rep.get_metadata_vocab('K')\n",
    "\n",
    "part_vocab_size = len(part_vocab)\n",
    "k_vocab_size = len(k_vocab)\n",
    "m_vocab_size = len(m_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_char2idx, part_idx2char = generate_char_idx_mappings(part_vocab)\n",
    "k_char2idx, k_idx2char = generate_char_idx_mappings(k_vocab)\n",
    "m_char2idx, m_idx2char = generate_char_idx_mappings(m_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_len = 60\n",
    "num_songs = int(0.9*len(rep.songs))\n",
    "k_len = min([len(rep.songs[i].metadata['K']) for i in range(1,num_songs+1)])\n",
    "m_len = min([len(rep.songs[i].metadata['M']) for i in range(1,num_songs+1)])\n",
    "\n",
    "part_num_matrix = np.zeros((num_songs, part_len))\n",
    "y_vals = []\n",
    "k_num_matrix = np.zeros((num_songs, 1))\n",
    "m_num_matrix = np.zeros((num_songs, m_len))\n",
    "\n",
    "for i in range(1,num_songs+1):\n",
    "    song = rep.songs[i]\n",
    "    part_num_matrix[i-1,] = np.array([part_char2idx[song.part[0:part_len][j]] for j in range(len(song.part[0:part_len]))])\n",
    "    k_num_matrix[i-1,0] = [k_char2idx[song.metadata['K']]][0]\n",
    "    m_num_matrix[i-1,] = [m_char2idx[song.metadata['M']]]\n",
    "    y_vals.append(part_char2idx[song.part[part_len]])\n",
    "\n",
    "# Convert y_vals to one_hot_encoded vectors\n",
    "y_data = to_categorical(y_vals, num_classes=part_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LSTM NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_Builder2(lstm_dim, dropout_pct, batch_size,\n",
    "                  embedding_dim, seq_len,\n",
    "                  part_voc_size, k_voc_size, m_voc_size, \n",
    "                  part_num_matrix, k_num_matrix, m_num_matrix):\n",
    "    \n",
    "    voc_input = Input(shape=(part_num_matrix.shape[1],))\n",
    "    k_input = Input(shape=(k_num_matrix.shape[1],))\n",
    "    m_input = Input(shape=(m_num_matrix.shape[1],))\n",
    "    \n",
    "    voc_embedding = Embedding(part_voc_size, embedding_dim)(voc_input)\n",
    "    \n",
    "    lstm_out = LSTM(lstm_dim, dropout=dropout_pct)(voc_embedding)\n",
    "    \n",
    "    concat = Concatenate()([lstm_out, k_input, m_input])\n",
    "\n",
    "    output = Dense(part_voc_size, activation='softmax')(concat)\n",
    "    \n",
    "    model = Model(inputs=[voc_input, k_input, m_input], outputs=output)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_46 (InputLayer)           (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_16 (Embedding)        (None, 60, 128)      141312      input_46[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_19 (LSTM)                  (None, 256)          394240      embedding_16[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "input_47 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_48 (InputLayer)           (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 260)          0           lstm_19[0][0]                    \n",
      "                                                                 input_47[0][0]                   \n",
      "                                                                 input_48[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1104)         288144      concatenate_8[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 823,696\n",
      "Trainable params: 823,696\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mod2 = LSTM_Builder2(256, 0.2, 100, \n",
    "                     128, 60,\n",
    "                     part_vocab_size, k_vocab_size, m_vocab_size,\n",
    "                     part_num_matrix, \n",
    "                     k_num_matrix, \n",
    "                     m_num_matrix)\n",
    "\n",
    "mod2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://benjamintseng.com/portfolio/nlp-pubmed-data-using-tensorflow-and-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_Builder3(lstm_dims, dropout_pct, batch_size,\n",
    "                  embedding_dim, seq_len,\n",
    "                  part_voc_size, k_voc_size, m_voc_size, \n",
    "                  part_num_matrix, k_num_matrix, m_num_matrix):\n",
    "    \n",
    "    voc_input = Input(shape=(part_num_matrix.shape[1],))\n",
    "    k_input = Input(shape=(k_num_matrix.shape[1],))\n",
    "    m_input = Input(shape=(m_num_matrix.shape[1],))\n",
    "    \n",
    "    voc_embedding = Embedding(part_voc_size, embedding_dim)(voc_input)\n",
    "    k_embedding = Embedding(k_voc_size, embedding_dim)(k_input)\n",
    "    m_embedding = Embedding(m_voc_size, embedding_dim)(m_input)\n",
    "    \n",
    "    concat = Concatenate(axis=1)([m_embedding, k_embedding, voc_embedding])\n",
    "    \n",
    "    lstm1 = LSTM(lstm_dims[0], dropout=dropout_pct, return_sequences=True)(concat)\n",
    "    lstm2 = LSTM(lstm_dims[1], dropout=dropout_pct, return_sequences=True)(lstm1)\n",
    "    lstm3 = LSTM(lstm_dims[2], dropout=dropout_pct)(lstm2)\n",
    "\n",
    "    output = Dense(part_voc_size, activation='softmax')(lstm3)\n",
    "    \n",
    "    model = Model(inputs=[voc_input, k_input, m_input], outputs=output)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_81 (InputLayer)           (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_80 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_79 (InputLayer)           (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_41 (Embedding)        (None, 3, 256)       512         input_81[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_40 (Embedding)        (None, 1, 256)       2816        input_80[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_39 (Embedding)        (None, 60, 256)      282624      input_79[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 64, 256)      0           embedding_41[0][0]               \n",
      "                                                                 embedding_40[0][0]               \n",
      "                                                                 embedding_39[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_39 (LSTM)                  (None, 64, 256)      525312      concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lstm_40 (LSTM)                  (None, 64, 512)      1574912     lstm_39[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_41 (LSTM)                  (None, 256)          787456      lstm_40[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 1104)         283728      lstm_41[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,457,360\n",
      "Trainable params: 3,457,360\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mod3 = LSTM_Builder3([256, 512, 256], 0.2, 100, \n",
    "                     256, 60,\n",
    "                     part_vocab_size, k_vocab_size, m_vocab_size,\n",
    "                     part_num_matrix, \n",
    "                     k_num_matrix, \n",
    "                     m_num_matrix)\n",
    "\n",
    "mod3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod3.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use checkpoints to save training weights before the model finishes training\n",
    "# Using this file path, the model checkpoints will be saved with the epoch number \n",
    "# and the validation loss in the filename.\n",
    "weights_filepath = \"mod3_weights.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    weights_filepath, monitor=\"loss\", verbose=0,\n",
    "    save_best_only=True, mode=\"min\")\n",
    "\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 1.9302\n",
      "Epoch 2/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 1.8968\n",
      "Epoch 3/5000\n",
      "306/306 [==============================] - 8s 25ms/step - loss: 1.8531\n",
      "Epoch 4/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.8158\n",
      "Epoch 5/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.7811\n",
      "Epoch 6/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.7744\n",
      "Epoch 7/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.7448\n",
      "Epoch 8/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.7261\n",
      "Epoch 9/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 1.6890\n",
      "Epoch 10/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.6651\n",
      "Epoch 11/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.6415\n",
      "Epoch 12/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.6121\n",
      "Epoch 13/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.6046\n",
      "Epoch 14/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.5838\n",
      "Epoch 15/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.5646\n",
      "Epoch 16/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.5288\n",
      "Epoch 17/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.5579\n",
      "Epoch 18/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.5368\n",
      "Epoch 19/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.5177\n",
      "Epoch 20/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.4657\n",
      "Epoch 21/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.4309\n",
      "Epoch 22/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.4223\n",
      "Epoch 23/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.5313\n",
      "Epoch 24/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.5305\n",
      "Epoch 25/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.4722\n",
      "Epoch 26/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.4153\n",
      "Epoch 27/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.3821\n",
      "Epoch 28/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.3483\n",
      "Epoch 29/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.3003\n",
      "Epoch 30/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.2847\n",
      "Epoch 31/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.2461\n",
      "Epoch 32/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.2200\n",
      "Epoch 33/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.2143\n",
      "Epoch 34/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.1746\n",
      "Epoch 35/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.1376\n",
      "Epoch 36/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.1376\n",
      "Epoch 37/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.1842\n",
      "Epoch 38/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.1468\n",
      "Epoch 39/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.1429\n",
      "Epoch 40/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.0996\n",
      "Epoch 41/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.1300\n",
      "Epoch 42/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.0915\n",
      "Epoch 43/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.0491\n",
      "Epoch 44/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.0394\n",
      "Epoch 45/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.0742\n",
      "Epoch 46/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.0491\n",
      "Epoch 47/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 1.0155\n",
      "Epoch 48/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.9734\n",
      "Epoch 49/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.9496\n",
      "Epoch 50/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.9120\n",
      "Epoch 51/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.8923\n",
      "Epoch 52/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.8680\n",
      "Epoch 53/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.8486\n",
      "Epoch 54/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.8342\n",
      "Epoch 55/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.8252\n",
      "Epoch 56/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.8117\n",
      "Epoch 57/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.8003\n",
      "Epoch 58/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.7940\n",
      "Epoch 59/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.7791\n",
      "Epoch 60/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.7541\n",
      "Epoch 61/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.7527\n",
      "Epoch 62/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.7130\n",
      "Epoch 63/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.7017\n",
      "Epoch 64/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.7125\n",
      "Epoch 65/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.7024\n",
      "Epoch 66/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.6787\n",
      "Epoch 67/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.6608\n",
      "Epoch 68/5000\n",
      "306/306 [==============================] - 7s 23ms/step - loss: 0.6666\n",
      "Epoch 69/5000\n",
      "306/306 [==============================] - 7s 24ms/step - loss: 0.6523\n",
      "Epoch 70/5000\n",
      "306/306 [==============================] - 7s 23ms/step - loss: 0.6315\n",
      "Epoch 71/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.6239\n",
      "Epoch 72/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.6079\n",
      "Epoch 73/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.5968\n",
      "Epoch 74/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.6032\n",
      "Epoch 75/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.5729\n",
      "Epoch 76/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.5749\n",
      "Epoch 77/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.5615\n",
      "Epoch 78/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.5465\n",
      "Epoch 79/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.5297\n",
      "Epoch 80/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.5177\n",
      "Epoch 81/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.5136\n",
      "Epoch 82/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.5064\n",
      "Epoch 83/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.5080\n",
      "Epoch 84/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.4962\n",
      "Epoch 85/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.4902\n",
      "Epoch 86/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.4948\n",
      "Epoch 87/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.4911\n",
      "Epoch 88/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.4789\n",
      "Epoch 89/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.4782\n",
      "Epoch 90/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.4724\n",
      "Epoch 91/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.4761\n",
      "Epoch 92/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.4641\n",
      "Epoch 93/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.4557\n",
      "Epoch 94/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.4439\n",
      "Epoch 95/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.4388\n",
      "Epoch 96/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.4385\n",
      "Epoch 97/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306/306 [==============================] - 6s 19ms/step - loss: 0.4303\n",
      "Epoch 98/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.4415\n",
      "Epoch 99/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.4217\n",
      "Epoch 100/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.4402\n",
      "Epoch 101/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.4509\n",
      "Epoch 102/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.4187\n",
      "Epoch 103/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.3970\n",
      "Epoch 104/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.3741\n",
      "Epoch 105/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.3640\n",
      "Epoch 106/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.3576\n",
      "Epoch 107/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.3515\n",
      "Epoch 108/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.3335\n",
      "Epoch 109/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.3314\n",
      "Epoch 110/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.3334\n",
      "Epoch 111/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.3277\n",
      "Epoch 112/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.3219\n",
      "Epoch 113/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.3159\n",
      "Epoch 114/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.3048\n",
      "Epoch 115/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.3009\n",
      "Epoch 116/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.2996\n",
      "Epoch 117/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.2921\n",
      "Epoch 118/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.2925\n",
      "Epoch 119/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.2773\n",
      "Epoch 120/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.2806\n",
      "Epoch 121/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.2787\n",
      "Epoch 122/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.2681\n",
      "Epoch 123/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.2702\n",
      "Epoch 124/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.2654\n",
      "Epoch 125/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.2608\n",
      "Epoch 126/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.2561\n",
      "Epoch 127/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.2519\n",
      "Epoch 128/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.2455\n",
      "Epoch 129/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.2390\n",
      "Epoch 130/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.2369\n",
      "Epoch 131/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.2367\n",
      "Epoch 132/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.2318\n",
      "Epoch 133/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.2211\n",
      "Epoch 134/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.2199\n",
      "Epoch 135/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.2178\n",
      "Epoch 136/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.2232\n",
      "Epoch 137/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.2123\n",
      "Epoch 138/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.2092\n",
      "Epoch 139/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.2053\n",
      "Epoch 140/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.2005\n",
      "Epoch 141/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1980\n",
      "Epoch 142/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1933\n",
      "Epoch 143/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1898\n",
      "Epoch 144/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1919\n",
      "Epoch 145/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1870\n",
      "Epoch 146/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1876\n",
      "Epoch 147/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1803\n",
      "Epoch 148/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1794\n",
      "Epoch 149/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1755\n",
      "Epoch 150/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1745\n",
      "Epoch 151/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1670\n",
      "Epoch 152/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.1674\n",
      "Epoch 153/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1715\n",
      "Epoch 154/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1618\n",
      "Epoch 155/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1574\n",
      "Epoch 156/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1556\n",
      "Epoch 157/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1501\n",
      "Epoch 158/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1517\n",
      "Epoch 159/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1500\n",
      "Epoch 160/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.1504\n",
      "Epoch 161/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.1444\n",
      "Epoch 162/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1501\n",
      "Epoch 163/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1441\n",
      "Epoch 164/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1409\n",
      "Epoch 165/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1400\n",
      "Epoch 166/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1378\n",
      "Epoch 167/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1320\n",
      "Epoch 168/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1277\n",
      "Epoch 169/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1252\n",
      "Epoch 170/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1257\n",
      "Epoch 171/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1269\n",
      "Epoch 172/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.1258\n",
      "Epoch 173/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1210\n",
      "Epoch 174/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1211\n",
      "Epoch 175/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1141\n",
      "Epoch 176/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1136\n",
      "Epoch 177/5000\n",
      "306/306 [==============================] - 9s 28ms/step - loss: 0.1112\n",
      "Epoch 178/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.1115\n",
      "Epoch 179/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1112\n",
      "Epoch 180/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1071\n",
      "Epoch 181/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1087\n",
      "Epoch 182/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1102\n",
      "Epoch 183/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1069\n",
      "Epoch 184/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1087\n",
      "Epoch 185/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1022\n",
      "Epoch 186/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1050\n",
      "Epoch 187/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1002\n",
      "Epoch 188/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0997\n",
      "Epoch 189/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0995\n",
      "Epoch 190/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0928\n",
      "Epoch 191/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0906\n",
      "Epoch 192/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0939\n",
      "Epoch 193/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0918\n",
      "Epoch 194/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0906\n",
      "Epoch 195/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0862\n",
      "Epoch 196/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0873\n",
      "Epoch 197/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0839\n",
      "Epoch 198/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0849\n",
      "Epoch 199/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0813\n",
      "Epoch 200/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0815\n",
      "Epoch 201/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0823\n",
      "Epoch 202/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0865\n",
      "Epoch 203/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0776\n",
      "Epoch 204/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0801\n",
      "Epoch 205/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0778\n",
      "Epoch 206/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0800\n",
      "Epoch 207/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0832\n",
      "Epoch 208/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0823\n",
      "Epoch 209/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0797\n",
      "Epoch 210/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0787\n",
      "Epoch 211/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0756\n",
      "Epoch 212/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0736\n",
      "Epoch 213/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0715\n",
      "Epoch 214/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0726\n",
      "Epoch 215/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0689\n",
      "Epoch 216/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0681\n",
      "Epoch 217/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0644\n",
      "Epoch 218/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0660\n",
      "Epoch 219/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0643\n",
      "Epoch 220/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0620\n",
      "Epoch 221/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0618\n",
      "Epoch 222/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0616\n",
      "Epoch 223/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0619\n",
      "Epoch 224/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0640\n",
      "Epoch 225/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0611\n",
      "Epoch 226/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0590\n",
      "Epoch 227/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0589\n",
      "Epoch 228/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0576\n",
      "Epoch 229/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0568\n",
      "Epoch 230/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0579\n",
      "Epoch 231/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0620\n",
      "Epoch 232/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0564\n",
      "Epoch 233/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0559\n",
      "Epoch 234/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0544\n",
      "Epoch 235/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0547\n",
      "Epoch 236/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0515\n",
      "Epoch 237/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0518\n",
      "Epoch 238/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0505\n",
      "Epoch 239/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0497\n",
      "Epoch 240/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0486\n",
      "Epoch 241/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0486\n",
      "Epoch 242/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0477\n",
      "Epoch 243/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0467\n",
      "Epoch 244/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0448\n",
      "Epoch 245/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0459\n",
      "Epoch 246/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0446\n",
      "Epoch 247/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0426\n",
      "Epoch 248/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0436\n",
      "Epoch 249/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0420\n",
      "Epoch 250/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0412\n",
      "Epoch 251/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0409\n",
      "Epoch 252/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0396\n",
      "Epoch 253/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0412\n",
      "Epoch 254/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0398\n",
      "Epoch 255/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0387\n",
      "Epoch 256/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0377\n",
      "Epoch 257/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0365\n",
      "Epoch 258/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0360\n",
      "Epoch 259/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0361\n",
      "Epoch 260/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0363\n",
      "Epoch 261/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0361\n",
      "Epoch 262/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0366\n",
      "Epoch 263/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0342\n",
      "Epoch 264/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0340\n",
      "Epoch 265/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0331\n",
      "Epoch 266/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0332\n",
      "Epoch 267/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0321\n",
      "Epoch 268/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0320\n",
      "Epoch 269/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0304\n",
      "Epoch 270/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0307\n",
      "Epoch 271/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0311\n",
      "Epoch 272/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0305\n",
      "Epoch 273/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0313\n",
      "Epoch 274/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0308\n",
      "Epoch 275/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0296\n",
      "Epoch 276/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0295\n",
      "Epoch 277/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0291\n",
      "Epoch 278/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0287\n",
      "Epoch 279/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0324\n",
      "Epoch 280/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0414\n",
      "Epoch 281/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0406\n",
      "Epoch 282/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0385\n",
      "Epoch 283/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0353\n",
      "Epoch 284/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0304\n",
      "Epoch 285/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0298\n",
      "Epoch 286/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0278\n",
      "Epoch 287/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0275\n",
      "Epoch 288/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0298\n",
      "Epoch 289/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0266\n",
      "Epoch 290/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0274\n",
      "Epoch 291/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0280\n",
      "Epoch 292/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0270\n",
      "Epoch 293/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0299\n",
      "Epoch 294/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0272\n",
      "Epoch 295/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0418\n",
      "Epoch 296/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0279\n",
      "Epoch 297/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0292\n",
      "Epoch 298/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0297\n",
      "Epoch 299/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0315\n",
      "Epoch 300/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0358\n",
      "Epoch 301/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0380\n",
      "Epoch 302/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0311\n",
      "Epoch 303/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0325\n",
      "Epoch 304/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0321\n",
      "Epoch 305/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0264\n",
      "Epoch 306/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0268\n",
      "Epoch 307/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0250\n",
      "Epoch 308/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0260\n",
      "Epoch 309/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0240\n",
      "Epoch 310/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0236\n",
      "Epoch 311/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0224\n",
      "Epoch 312/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0232\n",
      "Epoch 313/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0223\n",
      "Epoch 314/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0212\n",
      "Epoch 315/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0214\n",
      "Epoch 316/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0203\n",
      "Epoch 317/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0204\n",
      "Epoch 318/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0199\n",
      "Epoch 319/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0200\n",
      "Epoch 320/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0198\n",
      "Epoch 321/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0196\n",
      "Epoch 322/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0193\n",
      "Epoch 323/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0193\n",
      "Epoch 324/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0191\n",
      "Epoch 325/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0191\n",
      "Epoch 326/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0189\n",
      "Epoch 327/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0187\n",
      "Epoch 328/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0182\n",
      "Epoch 329/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0183\n",
      "Epoch 330/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0179\n",
      "Epoch 331/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0174\n",
      "Epoch 332/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0178\n",
      "Epoch 333/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0173\n",
      "Epoch 334/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0169\n",
      "Epoch 335/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0169\n",
      "Epoch 336/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0168\n",
      "Epoch 337/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0167\n",
      "Epoch 338/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0165\n",
      "Epoch 339/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0166\n",
      "Epoch 340/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0165\n",
      "Epoch 341/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0165\n",
      "Epoch 342/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0164\n",
      "Epoch 343/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0161\n",
      "Epoch 344/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0159\n",
      "Epoch 345/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0158\n",
      "Epoch 346/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0155\n",
      "Epoch 347/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0158\n",
      "Epoch 348/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0154\n",
      "Epoch 349/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0149\n",
      "Epoch 350/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0151\n",
      "Epoch 351/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0150\n",
      "Epoch 352/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0150\n",
      "Epoch 353/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0148\n",
      "Epoch 354/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0146\n",
      "Epoch 355/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0143\n",
      "Epoch 356/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0144\n",
      "Epoch 357/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0142\n",
      "Epoch 358/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0147\n",
      "Epoch 359/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0143\n",
      "Epoch 360/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0142\n",
      "Epoch 361/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0144\n",
      "Epoch 362/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0139\n",
      "Epoch 363/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0139\n",
      "Epoch 364/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0140\n",
      "Epoch 365/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0144\n",
      "Epoch 366/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0136\n",
      "Epoch 367/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0136\n",
      "Epoch 368/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0140\n",
      "Epoch 369/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0131\n",
      "Epoch 370/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0132\n",
      "Epoch 371/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0132\n",
      "Epoch 372/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0131\n",
      "Epoch 373/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0132\n",
      "Epoch 374/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0128\n",
      "Epoch 375/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0129\n",
      "Epoch 376/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0129\n",
      "Epoch 377/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0126\n",
      "Epoch 378/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0125\n",
      "Epoch 379/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0124\n",
      "Epoch 380/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0124\n",
      "Epoch 381/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0122\n",
      "Epoch 382/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0122\n",
      "Epoch 383/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0118\n",
      "Epoch 384/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0120\n",
      "Epoch 385/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0120\n",
      "Epoch 386/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0118\n",
      "Epoch 387/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0120\n",
      "Epoch 388/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0114\n",
      "Epoch 389/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0111\n",
      "Epoch 390/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0115\n",
      "Epoch 391/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0113\n",
      "Epoch 392/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0111\n",
      "Epoch 393/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0113\n",
      "Epoch 394/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0117\n",
      "Epoch 395/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0113\n",
      "Epoch 396/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0115\n",
      "Epoch 397/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0113\n",
      "Epoch 398/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0108\n",
      "Epoch 399/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0112\n",
      "Epoch 400/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0106\n",
      "Epoch 401/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0105\n",
      "Epoch 402/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0104\n",
      "Epoch 403/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0104\n",
      "Epoch 404/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0106\n",
      "Epoch 405/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0106\n",
      "Epoch 406/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0103\n",
      "Epoch 407/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0100\n",
      "Epoch 408/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0102\n",
      "Epoch 409/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0099\n",
      "Epoch 410/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0098\n",
      "Epoch 411/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0099\n",
      "Epoch 412/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0099\n",
      "Epoch 413/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0096\n",
      "Epoch 414/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0097\n",
      "Epoch 415/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0094\n",
      "Epoch 416/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0095\n",
      "Epoch 417/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0096\n",
      "Epoch 418/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0095\n",
      "Epoch 419/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0095\n",
      "Epoch 420/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0093\n",
      "Epoch 421/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0096\n",
      "Epoch 422/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0094\n",
      "Epoch 423/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0092\n",
      "Epoch 424/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0091\n",
      "Epoch 425/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0089\n",
      "Epoch 426/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0091\n",
      "Epoch 427/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0088\n",
      "Epoch 428/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0086\n",
      "Epoch 429/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0088\n",
      "Epoch 430/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0086\n",
      "Epoch 431/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0087\n",
      "Epoch 432/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0085\n",
      "Epoch 433/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0086\n",
      "Epoch 434/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0085\n",
      "Epoch 435/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0088\n",
      "Epoch 436/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0086\n",
      "Epoch 437/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0086\n",
      "Epoch 438/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0085\n",
      "Epoch 439/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0080\n",
      "Epoch 440/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0083\n",
      "Epoch 441/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0083\n",
      "Epoch 442/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0083\n",
      "Epoch 443/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0084\n",
      "Epoch 444/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0080\n",
      "Epoch 445/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0082\n",
      "Epoch 446/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0081\n",
      "Epoch 447/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0080\n",
      "Epoch 448/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0078\n",
      "Epoch 449/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0081\n",
      "Epoch 450/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0076\n",
      "Epoch 451/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0076\n",
      "Epoch 452/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0076\n",
      "Epoch 453/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0077\n",
      "Epoch 454/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0076\n",
      "Epoch 455/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0076\n",
      "Epoch 456/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0076\n",
      "Epoch 457/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0075\n",
      "Epoch 458/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0073\n",
      "Epoch 459/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0074\n",
      "Epoch 460/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0073\n",
      "Epoch 461/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0073\n",
      "Epoch 462/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0073\n",
      "Epoch 463/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0075\n",
      "Epoch 464/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0072\n",
      "Epoch 465/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0073\n",
      "Epoch 466/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0069\n",
      "Epoch 467/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0069\n",
      "Epoch 468/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0070\n",
      "Epoch 469/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0069\n",
      "Epoch 470/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0068\n",
      "Epoch 471/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0070\n",
      "Epoch 472/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0067\n",
      "Epoch 473/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0071\n",
      "Epoch 474/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0073\n",
      "Epoch 475/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0072\n",
      "Epoch 476/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0070\n",
      "Epoch 477/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0067\n",
      "Epoch 478/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0069\n",
      "Epoch 479/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0067\n",
      "Epoch 480/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0067\n",
      "Epoch 481/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0064\n",
      "Epoch 482/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0064\n",
      "Epoch 483/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0066\n",
      "Epoch 484/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0064\n",
      "Epoch 485/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0063\n",
      "Epoch 486/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0062\n",
      "Epoch 487/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0063\n",
      "Epoch 488/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0062\n",
      "Epoch 489/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0063\n",
      "Epoch 490/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0061\n",
      "Epoch 491/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0065\n",
      "Epoch 492/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0061\n",
      "Epoch 493/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0060\n",
      "Epoch 494/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0060\n",
      "Epoch 495/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0061\n",
      "Epoch 496/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0061\n",
      "Epoch 497/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0059\n",
      "Epoch 498/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0060\n",
      "Epoch 499/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0060\n",
      "Epoch 500/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0059\n",
      "Epoch 501/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0060\n",
      "Epoch 502/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0061\n",
      "Epoch 503/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0058\n",
      "Epoch 504/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0058\n",
      "Epoch 505/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0057\n",
      "Epoch 506/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0055\n",
      "Epoch 507/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0058\n",
      "Epoch 508/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0058\n",
      "Epoch 509/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0055\n",
      "Epoch 510/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0056\n",
      "Epoch 511/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0055\n",
      "Epoch 512/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0055\n",
      "Epoch 513/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0052\n",
      "Epoch 514/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0054\n",
      "Epoch 515/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0052\n",
      "Epoch 516/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0054\n",
      "Epoch 517/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0054\n",
      "Epoch 518/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0052\n",
      "Epoch 519/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0053\n",
      "Epoch 520/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0051\n",
      "Epoch 521/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0052\n",
      "Epoch 522/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0051\n",
      "Epoch 523/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0052\n",
      "Epoch 524/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0051\n",
      "Epoch 525/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0051\n",
      "Epoch 526/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0051\n",
      "Epoch 527/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0052\n",
      "Epoch 528/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0050\n",
      "Epoch 529/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0050\n",
      "Epoch 530/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0051\n",
      "Epoch 531/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0049\n",
      "Epoch 532/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0049\n",
      "Epoch 533/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0050\n",
      "Epoch 534/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0050\n",
      "Epoch 535/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0047\n",
      "Epoch 536/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0049\n",
      "Epoch 537/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0048\n",
      "Epoch 538/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0046\n",
      "Epoch 539/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0047\n",
      "Epoch 540/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0047\n",
      "Epoch 541/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0046\n",
      "Epoch 542/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0046\n",
      "Epoch 543/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0045\n",
      "Epoch 544/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0047\n",
      "Epoch 545/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0047\n",
      "Epoch 546/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0051\n",
      "Epoch 547/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0047\n",
      "Epoch 548/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0047\n",
      "Epoch 549/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0047\n",
      "Epoch 550/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0045\n",
      "Epoch 551/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0046\n",
      "Epoch 552/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0059\n",
      "Epoch 553/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0057\n",
      "Epoch 554/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0118\n",
      "Epoch 555/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0229\n",
      "Epoch 556/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0466\n",
      "Epoch 557/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0837\n",
      "Epoch 558/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1078\n",
      "Epoch 559/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.2397\n",
      "Epoch 560/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.2447\n",
      "Epoch 561/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.2568\n",
      "Epoch 562/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.3317\n",
      "Epoch 563/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.3388\n",
      "Epoch 564/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.3776\n",
      "Epoch 565/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.3872\n",
      "Epoch 566/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.2714\n",
      "Epoch 567/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1986\n",
      "Epoch 568/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.1152\n",
      "Epoch 569/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 0.0691\n",
      "Epoch 570/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0775\n",
      "Epoch 571/5000\n",
      "306/306 [==============================] - 7s 24ms/step - loss: 0.0612\n",
      "Epoch 572/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0829\n",
      "Epoch 573/5000\n",
      "306/306 [==============================] - 7s 24ms/step - loss: 0.0524\n",
      "Epoch 574/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0507\n",
      "Epoch 575/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0300\n",
      "Epoch 576/5000\n",
      "306/306 [==============================] - 8s 25ms/step - loss: 0.0321\n",
      "Epoch 577/5000\n",
      "306/306 [==============================] - 8s 26ms/step - loss: 0.0339\n",
      "Epoch 578/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0238\n",
      "Epoch 579/5000\n",
      "306/306 [==============================] - 7s 21ms/step - loss: 0.0218\n",
      "Epoch 580/5000\n",
      "306/306 [==============================] - 7s 21ms/step - loss: 0.0206\n",
      "Epoch 581/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0200\n",
      "Epoch 582/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0176\n",
      "Epoch 583/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0172\n",
      "Epoch 584/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0149\n",
      "Epoch 585/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0139\n",
      "Epoch 586/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0135\n",
      "Epoch 587/5000\n",
      "306/306 [==============================] - 8s 25ms/step - loss: 0.0117\n",
      "Epoch 588/5000\n",
      "306/306 [==============================] - 7s 23ms/step - loss: 0.0116\n",
      "Epoch 589/5000\n",
      "306/306 [==============================] - 7s 24ms/step - loss: 0.0103\n",
      "Epoch 590/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0110\n",
      "Epoch 591/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0102\n",
      "Epoch 592/5000\n",
      "306/306 [==============================] - 8s 25ms/step - loss: 0.0095\n",
      "Epoch 593/5000\n",
      "306/306 [==============================] - 8s 27ms/step - loss: 0.0095\n",
      "Epoch 594/5000\n",
      "306/306 [==============================] - 9s 28ms/step - loss: 0.0087\n",
      "Epoch 595/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0084\n",
      "Epoch 596/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0088\n",
      "Epoch 597/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0083\n",
      "Epoch 598/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0076\n",
      "Epoch 599/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0079\n",
      "Epoch 600/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0083\n",
      "Epoch 601/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0087\n",
      "Epoch 602/5000\n",
      "306/306 [==============================] - 7s 23ms/step - loss: 0.0076\n",
      "Epoch 603/5000\n",
      "306/306 [==============================] - 8s 26ms/step - loss: 0.0073\n",
      "Epoch 604/5000\n",
      "306/306 [==============================] - 8s 25ms/step - loss: 0.0071\n",
      "Epoch 605/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0074\n",
      "Epoch 606/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0075\n",
      "Epoch 607/5000\n",
      "306/306 [==============================] - 7s 21ms/step - loss: 0.0067\n",
      "Epoch 608/5000\n",
      "306/306 [==============================] - 7s 24ms/step - loss: 0.0068\n",
      "Epoch 609/5000\n",
      "306/306 [==============================] - 8s 26ms/step - loss: 0.0071\n",
      "Epoch 610/5000\n",
      "306/306 [==============================] - 10s 32ms/step - loss: 0.0067\n",
      "Epoch 611/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0067\n",
      "Epoch 612/5000\n",
      "306/306 [==============================] - 8s 27ms/step - loss: 0.0067\n",
      "Epoch 613/5000\n",
      "306/306 [==============================] - 7s 21ms/step - loss: 0.0064\n",
      "Epoch 614/5000\n",
      "306/306 [==============================] - 7s 21ms/step - loss: 0.0066\n",
      "Epoch 615/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0061\n",
      "Epoch 616/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0062\n",
      "Epoch 617/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0063\n",
      "Epoch 618/5000\n",
      "306/306 [==============================] - 8s 27ms/step - loss: 0.0061\n",
      "Epoch 619/5000\n",
      "306/306 [==============================] - 8s 26ms/step - loss: 0.0059\n",
      "Epoch 620/5000\n",
      "306/306 [==============================] - 7s 23ms/step - loss: 0.0059\n",
      "Epoch 621/5000\n",
      "306/306 [==============================] - 8s 25ms/step - loss: 0.0058\n",
      "Epoch 622/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0057\n",
      "Epoch 623/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0060\n",
      "Epoch 624/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0059\n",
      "Epoch 625/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0057\n",
      "Epoch 626/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0055\n",
      "Epoch 627/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0056\n",
      "Epoch 628/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0058\n",
      "Epoch 629/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0053\n",
      "Epoch 630/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0056\n",
      "Epoch 631/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0051\n",
      "Epoch 632/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0052\n",
      "Epoch 633/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0054\n",
      "Epoch 634/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0054\n",
      "Epoch 635/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0052\n",
      "Epoch 636/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0053\n",
      "Epoch 637/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0051\n",
      "Epoch 638/5000\n",
      "306/306 [==============================] - 7s 21ms/step - loss: 0.0049\n",
      "Epoch 639/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0053\n",
      "Epoch 640/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0048\n",
      "Epoch 641/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0053\n",
      "Epoch 642/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0052\n",
      "Epoch 643/5000\n",
      "306/306 [==============================] - 8s 28ms/step - loss: 0.0049\n",
      "Epoch 644/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0050\n",
      "Epoch 645/5000\n",
      "306/306 [==============================] - 8s 25ms/step - loss: 0.0046\n",
      "Epoch 646/5000\n",
      "306/306 [==============================] - 7s 24ms/step - loss: 0.0046\n",
      "Epoch 647/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0047\n",
      "Epoch 648/5000\n",
      "306/306 [==============================] - 7s 24ms/step - loss: 0.0046\n",
      "Epoch 649/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0046\n",
      "Epoch 650/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0046\n",
      "Epoch 651/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0044\n",
      "Epoch 652/5000\n",
      "306/306 [==============================] - 8s 25ms/step - loss: 0.0049\n",
      "Epoch 653/5000\n",
      "306/306 [==============================] - 8s 26ms/step - loss: 0.0045\n",
      "Epoch 654/5000\n",
      "306/306 [==============================] - 7s 24ms/step - loss: 0.0044\n",
      "Epoch 655/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0046\n",
      "Epoch 656/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0046\n",
      "Epoch 657/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0045\n",
      "Epoch 658/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0043\n",
      "Epoch 659/5000\n",
      "306/306 [==============================] - 8s 25ms/step - loss: 0.0043\n",
      "Epoch 660/5000\n",
      "306/306 [==============================] - 7s 23ms/step - loss: 0.0043\n",
      "Epoch 661/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0045\n",
      "Epoch 662/5000\n",
      "306/306 [==============================] - 7s 24ms/step - loss: 0.0041\n",
      "Epoch 663/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0043\n",
      "Epoch 664/5000\n",
      "306/306 [==============================] - 7s 21ms/step - loss: 0.0044\n",
      "Epoch 665/5000\n",
      "306/306 [==============================] - 7s 23ms/step - loss: 0.0041\n",
      "Epoch 666/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0044\n",
      "Epoch 667/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306/306 [==============================] - 7s 25ms/step - loss: 0.0045\n",
      "Epoch 668/5000\n",
      "306/306 [==============================] - 8s 25ms/step - loss: 0.0045\n",
      "Epoch 669/5000\n",
      "306/306 [==============================] - 10s 31ms/step - loss: 0.0042\n",
      "Epoch 670/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0043\n",
      "Epoch 671/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0043\n",
      "Epoch 672/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0044\n",
      "Epoch 673/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0050\n",
      "Epoch 674/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0041\n",
      "Epoch 675/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0041\n",
      "Epoch 676/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0040\n",
      "Epoch 677/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0040\n",
      "Epoch 678/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0040\n",
      "Epoch 679/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0040\n",
      "Epoch 680/5000\n",
      "306/306 [==============================] - 8s 25ms/step - loss: 0.0039\n",
      "Epoch 681/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0038\n",
      "Epoch 682/5000\n",
      "306/306 [==============================] - 7s 23ms/step - loss: 0.0038\n",
      "Epoch 683/5000\n",
      "306/306 [==============================] - 7s 21ms/step - loss: 0.0040\n",
      "Epoch 684/5000\n",
      "306/306 [==============================] - 7s 24ms/step - loss: 0.0039\n",
      "Epoch 685/5000\n",
      "306/306 [==============================] - 8s 26ms/step - loss: 0.0038\n",
      "Epoch 686/5000\n",
      "306/306 [==============================] - 7s 23ms/step - loss: 0.0039\n",
      "Epoch 687/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0038\n",
      "Epoch 688/5000\n",
      "306/306 [==============================] - 7s 24ms/step - loss: 0.0037\n",
      "Epoch 689/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0036\n",
      "Epoch 690/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0036\n",
      "Epoch 691/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0036\n",
      "Epoch 692/5000\n",
      "306/306 [==============================] - 7s 23ms/step - loss: 0.0036\n",
      "Epoch 693/5000\n",
      "306/306 [==============================] - 7s 24ms/step - loss: 0.0035\n",
      "Epoch 694/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0035\n",
      "Epoch 695/5000\n",
      "306/306 [==============================] - 7s 23ms/step - loss: 0.0034\n",
      "Epoch 696/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0035\n",
      "Epoch 697/5000\n",
      "306/306 [==============================] - 8s 25ms/step - loss: 0.0035\n",
      "Epoch 698/5000\n",
      "306/306 [==============================] - 7s 24ms/step - loss: 0.0037\n",
      "Epoch 699/5000\n",
      "306/306 [==============================] - 7s 23ms/step - loss: 0.0035\n",
      "Epoch 700/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0033\n",
      "Epoch 701/5000\n",
      "306/306 [==============================] - 8s 26ms/step - loss: 0.0034\n",
      "Epoch 702/5000\n",
      "306/306 [==============================] - 7s 23ms/step - loss: 0.0034\n",
      "Epoch 703/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0035\n",
      "Epoch 704/5000\n",
      "306/306 [==============================] - 9s 30ms/step - loss: 0.0035\n",
      "Epoch 705/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0033\n",
      "Epoch 706/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0034\n",
      "Epoch 707/5000\n",
      "306/306 [==============================] - 8s 27ms/step - loss: 0.0034\n",
      "Epoch 708/5000\n",
      "306/306 [==============================] - 7s 21ms/step - loss: 0.0033\n",
      "Epoch 709/5000\n",
      "306/306 [==============================] - 7s 21ms/step - loss: 0.0033\n",
      "Epoch 710/5000\n",
      "306/306 [==============================] - 8s 25ms/step - loss: 0.0033\n",
      "Epoch 711/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0033\n",
      "Epoch 712/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0032\n",
      "Epoch 713/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0033\n",
      "Epoch 714/5000\n",
      "306/306 [==============================] - 10s 31ms/step - loss: 0.0032\n",
      "Epoch 715/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0031\n",
      "Epoch 716/5000\n",
      "306/306 [==============================] - 8s 25ms/step - loss: 0.0032\n",
      "Epoch 717/5000\n",
      "306/306 [==============================] - 12s 40ms/step - loss: 0.0032\n",
      "Epoch 718/5000\n",
      "306/306 [==============================] - 10s 33ms/step - loss: 0.0031\n",
      "Epoch 719/5000\n",
      "306/306 [==============================] - 8s 25ms/step - loss: 0.0031\n",
      "Epoch 720/5000\n",
      "306/306 [==============================] - 8s 26ms/step - loss: 0.0033\n",
      "Epoch 721/5000\n",
      "306/306 [==============================] - 7s 24ms/step - loss: 0.0031\n",
      "Epoch 722/5000\n",
      "306/306 [==============================] - 8s 27ms/step - loss: 0.0031\n",
      "Epoch 723/5000\n",
      "306/306 [==============================] - 9s 28ms/step - loss: 0.0032\n",
      "Epoch 724/5000\n",
      "306/306 [==============================] - 9s 30ms/step - loss: 0.0031\n",
      "Epoch 725/5000\n",
      "306/306 [==============================] - 7s 24ms/step - loss: 0.0029\n",
      "Epoch 726/5000\n",
      "306/306 [==============================] - 7s 23ms/step - loss: 0.0031\n",
      "Epoch 727/5000\n",
      "306/306 [==============================] - 7s 21ms/step - loss: 0.0031\n",
      "Epoch 728/5000\n",
      "306/306 [==============================] - 8s 26ms/step - loss: 0.0031\n",
      "Epoch 729/5000\n",
      "306/306 [==============================] - 7s 24ms/step - loss: 0.0029\n",
      "Epoch 730/5000\n",
      "306/306 [==============================] - 7s 23ms/step - loss: 0.0030\n",
      "Epoch 731/5000\n",
      "306/306 [==============================] - 9s 30ms/step - loss: 0.0031\n",
      "Epoch 732/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0029\n",
      "Epoch 733/5000\n",
      "306/306 [==============================] - 7s 24ms/step - loss: 0.0028\n",
      "Epoch 734/5000\n",
      "306/306 [==============================] - 7s 23ms/step - loss: 0.0029\n",
      "Epoch 735/5000\n",
      "306/306 [==============================] - 7s 24ms/step - loss: 0.0030\n",
      "Epoch 736/5000\n",
      "306/306 [==============================] - 9s 28ms/step - loss: 0.0028\n",
      "Epoch 737/5000\n",
      "306/306 [==============================] - 8s 27ms/step - loss: 0.0028\n",
      "Epoch 738/5000\n",
      "306/306 [==============================] - 8s 26ms/step - loss: 0.0029\n",
      "Epoch 739/5000\n",
      "306/306 [==============================] - 9s 30ms/step - loss: 0.0027\n",
      "Epoch 740/5000\n",
      "306/306 [==============================] - 8s 25ms/step - loss: 0.0030\n",
      "Epoch 741/5000\n",
      "306/306 [==============================] - 8s 25ms/step - loss: 0.0029\n",
      "Epoch 742/5000\n",
      "306/306 [==============================] - 7s 23ms/step - loss: 0.0027\n",
      "Epoch 743/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0029\n",
      "Epoch 744/5000\n",
      "306/306 [==============================] - 7s 21ms/step - loss: 0.0027\n",
      "Epoch 745/5000\n",
      "306/306 [==============================] - 9s 28ms/step - loss: 0.0027\n",
      "Epoch 746/5000\n",
      "306/306 [==============================] - 9s 31ms/step - loss: 0.0029\n",
      "Epoch 747/5000\n",
      "306/306 [==============================] - 9s 31ms/step - loss: 0.0025\n",
      "Epoch 748/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0028\n",
      "Epoch 749/5000\n",
      "306/306 [==============================] - 7s 24ms/step - loss: 0.0028\n",
      "Epoch 750/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0028\n",
      "Epoch 751/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0027\n",
      "Epoch 752/5000\n",
      "306/306 [==============================] - 8s 25ms/step - loss: 0.0026\n",
      "Epoch 753/5000\n",
      "306/306 [==============================] - 7s 23ms/step - loss: 0.0025\n",
      "Epoch 754/5000\n",
      "306/306 [==============================] - 8s 26ms/step - loss: 0.0026\n",
      "Epoch 755/5000\n",
      "306/306 [==============================] - 8s 28ms/step - loss: 0.0026\n",
      "Epoch 756/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0027\n",
      "Epoch 757/5000\n",
      "306/306 [==============================] - 8s 25ms/step - loss: 0.0027\n",
      "Epoch 758/5000\n",
      "306/306 [==============================] - 8s 25ms/step - loss: 0.0025\n",
      "Epoch 759/5000\n",
      "306/306 [==============================] - 7s 24ms/step - loss: 0.0026\n",
      "Epoch 760/5000\n",
      "306/306 [==============================] - 8s 25ms/step - loss: 0.0027\n",
      "Epoch 761/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0027\n",
      "Epoch 762/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0025\n",
      "Epoch 763/5000\n",
      "306/306 [==============================] - 8s 26ms/step - loss: 0.0025\n",
      "Epoch 764/5000\n",
      "306/306 [==============================] - 7s 24ms/step - loss: 0.0025\n",
      "Epoch 765/5000\n",
      "306/306 [==============================] - 8s 25ms/step - loss: 0.0025\n",
      "Epoch 766/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0025\n",
      "Epoch 767/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0026\n",
      "Epoch 768/5000\n",
      "306/306 [==============================] - 8s 25ms/step - loss: 0.0025\n",
      "Epoch 769/5000\n",
      "306/306 [==============================] - 10s 32ms/step - loss: 0.0025\n",
      "Epoch 770/5000\n",
      "306/306 [==============================] - 9s 28ms/step - loss: 0.0024\n",
      "Epoch 771/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0026\n",
      "Epoch 772/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0023\n",
      "Epoch 773/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0024\n",
      "Epoch 774/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0024\n",
      "Epoch 775/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0024\n",
      "Epoch 776/5000\n",
      "306/306 [==============================] - 7s 23ms/step - loss: 0.0023\n",
      "Epoch 777/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0024\n",
      "Epoch 778/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0024\n",
      "Epoch 779/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0023\n",
      "Epoch 780/5000\n",
      "306/306 [==============================] - 7s 21ms/step - loss: 0.0025\n",
      "Epoch 781/5000\n",
      "306/306 [==============================] - 8s 25ms/step - loss: 0.0023\n",
      "Epoch 782/5000\n",
      "306/306 [==============================] - 7s 23ms/step - loss: 0.0023\n",
      "Epoch 783/5000\n",
      "306/306 [==============================] - 7s 23ms/step - loss: 0.0023\n",
      "Epoch 784/5000\n",
      "306/306 [==============================] - 7s 21ms/step - loss: 0.0023\n",
      "Epoch 785/5000\n",
      "306/306 [==============================] - 7s 23ms/step - loss: 0.0023\n",
      "Epoch 786/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0022\n",
      "Epoch 787/5000\n",
      "306/306 [==============================] - 7s 23ms/step - loss: 0.0022\n",
      "Epoch 788/5000\n",
      "306/306 [==============================] - 8s 25ms/step - loss: 0.0022\n",
      "Epoch 789/5000\n",
      "306/306 [==============================] - 7s 23ms/step - loss: 0.0022\n",
      "Epoch 790/5000\n",
      "306/306 [==============================] - 7s 24ms/step - loss: 0.0023\n",
      "Epoch 791/5000\n",
      "306/306 [==============================] - 7s 24ms/step - loss: 0.0023\n",
      "Epoch 792/5000\n",
      "306/306 [==============================] - 8s 25ms/step - loss: 0.0023\n",
      "Epoch 793/5000\n",
      "306/306 [==============================] - 9s 30ms/step - loss: 0.0021\n",
      "Epoch 794/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0022\n",
      "Epoch 795/5000\n",
      "306/306 [==============================] - 9s 28ms/step - loss: 0.0022\n",
      "Epoch 796/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0021\n",
      "Epoch 797/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0023\n",
      "Epoch 798/5000\n",
      "306/306 [==============================] - 9s 31ms/step - loss: 0.0021\n",
      "Epoch 799/5000\n",
      "306/306 [==============================] - 10s 31ms/step - loss: 0.0021\n",
      "Epoch 800/5000\n",
      "306/306 [==============================] - 11s 37ms/step - loss: 0.0021\n",
      "Epoch 801/5000\n",
      "306/306 [==============================] - 8s 26ms/step - loss: 0.0021\n",
      "Epoch 802/5000\n",
      "306/306 [==============================] - 8s 28ms/step - loss: 0.0022\n",
      "Epoch 803/5000\n",
      "306/306 [==============================] - 8s 27ms/step - loss: 0.0022\n",
      "Epoch 804/5000\n",
      "306/306 [==============================] - 8s 26ms/step - loss: 0.0021\n",
      "Epoch 805/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0021\n",
      "Epoch 806/5000\n",
      "306/306 [==============================] - 9s 31ms/step - loss: 0.0020\n",
      "Epoch 807/5000\n",
      "306/306 [==============================] - 10s 33ms/step - loss: 0.0022\n",
      "Epoch 808/5000\n",
      "306/306 [==============================] - 9s 28ms/step - loss: 0.0021\n",
      "Epoch 809/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0020\n",
      "Epoch 810/5000\n",
      "306/306 [==============================] - 9s 28ms/step - loss: 0.0019\n",
      "Epoch 811/5000\n",
      "306/306 [==============================] - 10s 32ms/step - loss: 0.0020\n",
      "Epoch 812/5000\n",
      "306/306 [==============================] - 11s 37ms/step - loss: 0.0021\n",
      "Epoch 813/5000\n",
      "306/306 [==============================] - 9s 28ms/step - loss: 0.0019\n",
      "Epoch 814/5000\n",
      "306/306 [==============================] - 7s 24ms/step - loss: 0.0020\n",
      "Epoch 815/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0020\n",
      "Epoch 816/5000\n",
      "306/306 [==============================] - 10s 34ms/step - loss: 0.0020\n",
      "Epoch 817/5000\n",
      "306/306 [==============================] - 9s 31ms/step - loss: 0.0021\n",
      "Epoch 818/5000\n",
      "306/306 [==============================] - 10s 34ms/step - loss: 0.0019\n",
      "Epoch 819/5000\n",
      "306/306 [==============================] - 11s 34ms/step - loss: 0.0019\n",
      "Epoch 820/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0019\n",
      "Epoch 821/5000\n",
      "306/306 [==============================] - 10s 32ms/step - loss: 0.0018\n",
      "Epoch 822/5000\n",
      "306/306 [==============================] - 10s 34ms/step - loss: 0.0019\n",
      "Epoch 823/5000\n",
      "306/306 [==============================] - 10s 31ms/step - loss: 0.0020\n",
      "Epoch 824/5000\n",
      "306/306 [==============================] - 8s 28ms/step - loss: 0.0020\n",
      "Epoch 825/5000\n",
      "306/306 [==============================] - 8s 27ms/step - loss: 0.0019\n",
      "Epoch 826/5000\n",
      "306/306 [==============================] - 8s 27ms/step - loss: 0.0019\n",
      "Epoch 827/5000\n",
      "306/306 [==============================] - 9s 30ms/step - loss: 0.0019\n",
      "Epoch 828/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0020\n",
      "Epoch 829/5000\n",
      "306/306 [==============================] - 10s 32ms/step - loss: 0.0019\n",
      "Epoch 830/5000\n",
      "306/306 [==============================] - 10s 32ms/step - loss: 0.0019\n",
      "Epoch 831/5000\n",
      "306/306 [==============================] - 10s 33ms/step - loss: 0.0019\n",
      "Epoch 832/5000\n",
      "306/306 [==============================] - 9s 28ms/step - loss: 0.0019\n",
      "Epoch 833/5000\n",
      "306/306 [==============================] - 11s 34ms/step - loss: 0.0020\n",
      "Epoch 834/5000\n",
      "306/306 [==============================] - 9s 31ms/step - loss: 0.0018\n",
      "Epoch 835/5000\n",
      "306/306 [==============================] - 9s 31ms/step - loss: 0.0018\n",
      "Epoch 836/5000\n",
      "306/306 [==============================] - 10s 33ms/step - loss: 0.0018\n",
      "Epoch 837/5000\n",
      "306/306 [==============================] - 9s 30ms/step - loss: 0.0019\n",
      "Epoch 838/5000\n",
      "306/306 [==============================] - 8s 27ms/step - loss: 0.0018\n",
      "Epoch 839/5000\n",
      "306/306 [==============================] - 8s 27ms/step - loss: 0.0019\n",
      "Epoch 840/5000\n",
      "306/306 [==============================] - 8s 27ms/step - loss: 0.0018\n",
      "Epoch 841/5000\n",
      "306/306 [==============================] - 8s 28ms/step - loss: 0.0017\n",
      "Epoch 842/5000\n",
      "306/306 [==============================] - 9s 28ms/step - loss: 0.0018\n",
      "Epoch 843/5000\n",
      "306/306 [==============================] - 8s 27ms/step - loss: 0.0019\n",
      "Epoch 844/5000\n",
      "306/306 [==============================] - 8s 27ms/step - loss: 0.0018\n",
      "Epoch 845/5000\n",
      "306/306 [==============================] - 8s 27ms/step - loss: 0.0017\n",
      "Epoch 846/5000\n",
      "306/306 [==============================] - 8s 27ms/step - loss: 0.0018\n",
      "Epoch 847/5000\n",
      "306/306 [==============================] - 8s 27ms/step - loss: 0.0017\n",
      "Epoch 848/5000\n",
      "306/306 [==============================] - 8s 27ms/step - loss: 0.0018\n",
      "Epoch 849/5000\n",
      "306/306 [==============================] - 8s 27ms/step - loss: 0.0017\n",
      "Epoch 850/5000\n",
      "306/306 [==============================] - 8s 27ms/step - loss: 0.0018\n",
      "Epoch 851/5000\n",
      "306/306 [==============================] - 8s 27ms/step - loss: 0.0017\n",
      "Epoch 852/5000\n",
      "306/306 [==============================] - 8s 27ms/step - loss: 0.0018\n",
      "Epoch 853/5000\n",
      "306/306 [==============================] - 8s 27ms/step - loss: 0.0017\n",
      "Epoch 854/5000\n",
      "306/306 [==============================] - 8s 27ms/step - loss: 0.0017\n",
      "Epoch 855/5000\n",
      "306/306 [==============================] - 8s 27ms/step - loss: 0.0017\n",
      "Epoch 856/5000\n",
      "306/306 [==============================] - 8s 27ms/step - loss: 0.0017\n",
      "Epoch 857/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306/306 [==============================] - 9s 30ms/step - loss: 0.0017\n",
      "Epoch 858/5000\n",
      "306/306 [==============================] - 11s 37ms/step - loss: 0.0017\n",
      "Epoch 859/5000\n",
      "306/306 [==============================] - 10s 32ms/step - loss: 0.0016\n",
      "Epoch 860/5000\n",
      "306/306 [==============================] - 10s 33ms/step - loss: 0.0017\n",
      "Epoch 861/5000\n",
      "306/306 [==============================] - 9s 30ms/step - loss: 0.0017\n",
      "Epoch 862/5000\n",
      "306/306 [==============================] - 9s 30ms/step - loss: 0.0017\n",
      "Epoch 863/5000\n",
      "306/306 [==============================] - 9s 30ms/step - loss: 0.0017\n",
      "Epoch 864/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0017\n",
      "Epoch 865/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0016\n",
      "Epoch 866/5000\n",
      "306/306 [==============================] - 9s 30ms/step - loss: 0.0017\n",
      "Epoch 867/5000\n",
      "306/306 [==============================] - 10s 32ms/step - loss: 0.0016\n",
      "Epoch 868/5000\n",
      "306/306 [==============================] - 9s 28ms/step - loss: 0.0016\n",
      "Epoch 869/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0017\n",
      "Epoch 870/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0016\n",
      "Epoch 871/5000\n",
      "306/306 [==============================] - 9s 28ms/step - loss: 0.0015\n",
      "Epoch 872/5000\n",
      "306/306 [==============================] - 9s 30ms/step - loss: 0.0016\n",
      "Epoch 873/5000\n",
      "306/306 [==============================] - 11s 36ms/step - loss: 0.0015\n",
      "Epoch 874/5000\n",
      "306/306 [==============================] - 11s 35ms/step - loss: 0.0016\n",
      "Epoch 875/5000\n",
      "306/306 [==============================] - 9s 30ms/step - loss: 0.0016\n",
      "Epoch 876/5000\n",
      "306/306 [==============================] - 9s 31ms/step - loss: 0.0015\n",
      "Epoch 877/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0015\n",
      "Epoch 878/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0015\n",
      "Epoch 879/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0016\n",
      "Epoch 880/5000\n",
      "306/306 [==============================] - 9s 30ms/step - loss: 0.0015\n",
      "Epoch 881/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0016\n",
      "Epoch 882/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0016\n",
      "Epoch 883/5000\n",
      "306/306 [==============================] - 9s 28ms/step - loss: 0.0015\n",
      "Epoch 884/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0015\n",
      "Epoch 885/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0016\n",
      "Epoch 886/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0015\n",
      "Epoch 887/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0015\n",
      "Epoch 888/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0015\n",
      "Epoch 889/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0016\n",
      "Epoch 890/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0014\n",
      "Epoch 891/5000\n",
      "306/306 [==============================] - 9s 28ms/step - loss: 0.0014\n",
      "Epoch 892/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0015\n",
      "Epoch 893/5000\n",
      "306/306 [==============================] - 9s 28ms/step - loss: 0.0015\n",
      "Epoch 894/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0015\n",
      "Epoch 895/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0014\n",
      "Epoch 896/5000\n",
      "306/306 [==============================] - 9s 28ms/step - loss: 0.0015\n",
      "Epoch 897/5000\n",
      "306/306 [==============================] - 9s 28ms/step - loss: 0.0015\n",
      "Epoch 898/5000\n",
      "306/306 [==============================] - 9s 28ms/step - loss: 0.0014\n",
      "Epoch 899/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0014\n",
      "Epoch 900/5000\n",
      "306/306 [==============================] - 9s 28ms/step - loss: 0.0015\n",
      "Epoch 901/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0014\n",
      "Epoch 902/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0016\n",
      "Epoch 903/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0015\n",
      "Epoch 904/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0014\n",
      "Epoch 905/5000\n",
      "306/306 [==============================] - 9s 28ms/step - loss: 0.0014\n",
      "Epoch 906/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0014\n",
      "Epoch 907/5000\n",
      "306/306 [==============================] - 9s 28ms/step - loss: 0.0014\n",
      "Epoch 908/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0017\n",
      "Epoch 909/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0014\n",
      "Epoch 910/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0015\n",
      "Epoch 911/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0015\n",
      "Epoch 912/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0016\n",
      "Epoch 913/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0014\n",
      "Epoch 914/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0014\n",
      "Epoch 915/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0014\n",
      "Epoch 916/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0014\n",
      "Epoch 917/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0014\n",
      "Epoch 918/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0014\n",
      "Epoch 919/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0013\n",
      "Epoch 920/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0013\n",
      "Epoch 921/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0014\n",
      "Epoch 922/5000\n",
      "306/306 [==============================] - 9s 28ms/step - loss: 0.0013\n",
      "Epoch 923/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0013\n",
      "Epoch 924/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0013\n",
      "Epoch 925/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0014\n",
      "Epoch 926/5000\n",
      "306/306 [==============================] - 9s 28ms/step - loss: 0.0013\n",
      "Epoch 927/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0014\n",
      "Epoch 928/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0013\n",
      "Epoch 929/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0013\n",
      "Epoch 930/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0013\n",
      "Epoch 931/5000\n",
      "306/306 [==============================] - 9s 28ms/step - loss: 0.0013\n",
      "Epoch 932/5000\n",
      "306/306 [==============================] - 9s 28ms/step - loss: 0.0013\n",
      "Epoch 933/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0013\n",
      "Epoch 934/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0013\n",
      "Epoch 935/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0013\n",
      "Epoch 936/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0013\n",
      "Epoch 937/5000\n",
      "306/306 [==============================] - 9s 28ms/step - loss: 0.0012\n",
      "Epoch 938/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0012\n",
      "Epoch 939/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0013\n",
      "Epoch 940/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0012\n",
      "Epoch 941/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0013\n",
      "Epoch 942/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0013\n",
      "Epoch 943/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0012\n",
      "Epoch 944/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0013\n",
      "Epoch 945/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0012\n",
      "Epoch 946/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0013\n",
      "Epoch 947/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0013\n",
      "Epoch 948/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0012\n",
      "Epoch 949/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0013\n",
      "Epoch 950/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0011\n",
      "Epoch 951/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0012\n",
      "Epoch 952/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0011\n",
      "Epoch 953/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0013\n",
      "Epoch 954/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0012\n",
      "Epoch 955/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0012\n",
      "Epoch 956/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0011\n",
      "Epoch 957/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0011\n",
      "Epoch 958/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0011\n",
      "Epoch 959/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0011\n",
      "Epoch 960/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0012\n",
      "Epoch 961/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0011\n",
      "Epoch 962/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0011\n",
      "Epoch 963/5000\n",
      "306/306 [==============================] - 9s 29ms/step - loss: 0.0011\n",
      "Epoch 964/5000\n",
      "306/306 [==============================] - 8s 26ms/step - loss: 0.0011\n",
      "Epoch 965/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0012\n",
      "Epoch 966/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0011\n",
      "Epoch 967/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0011\n",
      "Epoch 968/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0011\n",
      "Epoch 969/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0011\n",
      "Epoch 970/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0011\n",
      "Epoch 971/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0011\n",
      "Epoch 972/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0012\n",
      "Epoch 973/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0011\n",
      "Epoch 974/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0011\n",
      "Epoch 975/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0010\n",
      "Epoch 976/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0011\n",
      "Epoch 977/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0011\n",
      "Epoch 978/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0012\n",
      "Epoch 979/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0011\n",
      "Epoch 980/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0010\n",
      "Epoch 981/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0010\n",
      "Epoch 982/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0011\n",
      "Epoch 983/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0011\n",
      "Epoch 984/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0011\n",
      "Epoch 985/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0011\n",
      "Epoch 986/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0011\n",
      "Epoch 987/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0011\n",
      "Epoch 988/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0011\n",
      "Epoch 989/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 0.0011\n",
      "Epoch 990/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0010\n",
      "Epoch 991/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 9.9063e-04\n",
      "Epoch 992/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0011\n",
      "Epoch 993/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 0.0010\n",
      "Epoch 994/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0010\n",
      "Epoch 995/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 9.9945e-04\n",
      "Epoch 996/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 9.8624e-04\n",
      "Epoch 997/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 9.3543e-04\n",
      "Epoch 998/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0010\n",
      "Epoch 999/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 9.6636e-04\n",
      "Epoch 1000/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 9.9811e-04\n",
      "Epoch 1001/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0010\n",
      "Epoch 1002/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 9.4841e-04\n",
      "Epoch 1003/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 9.6989e-04\n",
      "Epoch 1004/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 9.5597e-04\n",
      "Epoch 1005/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 9.4206e-04\n",
      "Epoch 1006/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 9.8883e-04\n",
      "Epoch 1007/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 9.4977e-04\n",
      "Epoch 1008/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 9.8427e-04\n",
      "Epoch 1009/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 9.8639e-04\n",
      "Epoch 1010/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 9.5072e-04\n",
      "Epoch 1011/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 0.0010\n",
      "Epoch 1012/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 9.3269e-04\n",
      "Epoch 1013/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 9.0770e-04\n",
      "Epoch 1014/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 9.4276e-04\n",
      "Epoch 1015/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 9.3802e-04\n",
      "Epoch 1016/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 9.5773e-04\n",
      "Epoch 1017/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 9.0977e-04\n",
      "Epoch 1018/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 9.4887e-04\n",
      "Epoch 1019/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 9.2649e-04\n",
      "Epoch 1020/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 9.3533e-04\n",
      "Epoch 1021/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 8.6203e-04\n",
      "Epoch 1022/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 8.9251e-04\n",
      "Epoch 1023/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 9.5090e-04\n",
      "Epoch 1024/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 8.8653e-04\n",
      "Epoch 1025/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 9.2787e-04\n",
      "Epoch 1026/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 9.1142e-04\n",
      "Epoch 1027/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 8.7376e-04\n",
      "Epoch 1028/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 9.2528e-04\n",
      "Epoch 1029/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 8.6232e-04\n",
      "Epoch 1030/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 9.3268e-04\n",
      "Epoch 1031/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 8.8719e-04\n",
      "Epoch 1032/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 8.7235e-04\n",
      "Epoch 1033/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 9.0346e-04\n",
      "Epoch 1034/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 8.4434e-04\n",
      "Epoch 1035/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 8.7883e-04\n",
      "Epoch 1036/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 8.6968e-04\n",
      "Epoch 1037/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 9.0406e-04\n",
      "Epoch 1038/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 8.3845e-04\n",
      "Epoch 1039/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 8.3130e-04\n",
      "Epoch 1040/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 8.6748e-04\n",
      "Epoch 1041/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 7.9410e-04\n",
      "Epoch 1042/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 8.1659e-04\n",
      "Epoch 1043/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 8.6442e-04\n",
      "Epoch 1044/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306/306 [==============================] - 6s 20ms/step - loss: 8.7893e-04\n",
      "Epoch 1045/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 8.4641e-04\n",
      "Epoch 1046/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 8.6311e-04\n",
      "Epoch 1047/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 8.2131e-04\n",
      "Epoch 1048/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 8.3412e-04\n",
      "Epoch 1049/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 8.4299e-04\n",
      "Epoch 1050/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 8.2766e-04\n",
      "Epoch 1051/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 8.3858e-04\n",
      "Epoch 1052/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 7.8645e-04\n",
      "Epoch 1053/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 8.6192e-04\n",
      "Epoch 1054/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 7.4893e-04\n",
      "Epoch 1055/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 8.3607e-04\n",
      "Epoch 1056/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 9.0635e-04\n",
      "Epoch 1057/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 8.1245e-04\n",
      "Epoch 1058/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 8.2214e-04\n",
      "Epoch 1059/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 7.7442e-04\n",
      "Epoch 1060/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 7.9387e-04\n",
      "Epoch 1061/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 7.9014e-04\n",
      "Epoch 1062/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 8.2213e-04\n",
      "Epoch 1063/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 8.4453e-04\n",
      "Epoch 1064/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 7.8206e-04\n",
      "Epoch 1065/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 8.0558e-04\n",
      "Epoch 1066/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 8.0367e-04\n",
      "Epoch 1067/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 7.8897e-04\n",
      "Epoch 1068/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 7.5242e-04\n",
      "Epoch 1069/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 7.4533e-04\n",
      "Epoch 1070/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 7.8545e-04\n",
      "Epoch 1071/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 7.9461e-04\n",
      "Epoch 1072/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 7.4468e-04\n",
      "Epoch 1073/5000\n",
      "306/306 [==============================] - 6s 19ms/step - loss: 7.3469e-04\n",
      "Epoch 1074/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 7.1631e-04\n",
      "Epoch 1075/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 7.1973e-04\n",
      "Epoch 1076/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 7.4038e-04\n",
      "Epoch 1077/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 7.3965e-04\n",
      "Epoch 1078/5000\n",
      "306/306 [==============================] - 7s 21ms/step - loss: 7.0975e-04\n",
      "Epoch 1079/5000\n",
      "306/306 [==============================] - 8s 26ms/step - loss: 7.6753e-04\n",
      "Epoch 1080/5000\n",
      "306/306 [==============================] - 9s 30ms/step - loss: 7.7254e-04\n",
      "Epoch 1081/5000\n",
      "306/306 [==============================] - 8s 26ms/step - loss: 7.4715e-04\n",
      "Epoch 1082/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 7.3010e-04\n",
      "Epoch 1083/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 7.2458e-04\n",
      "Epoch 1084/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 7.1938e-04\n",
      "Epoch 1085/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 7.3473e-04\n",
      "Epoch 1086/5000\n",
      "306/306 [==============================] - 8s 25ms/step - loss: 7.1245e-04\n",
      "Epoch 1087/5000\n",
      "306/306 [==============================] - 8s 26ms/step - loss: 7.0551e-04\n",
      "Epoch 1088/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 7.0621e-04\n",
      "Epoch 1089/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 6.6636e-04\n",
      "Epoch 1090/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 7.2868e-04\n",
      "Epoch 1091/5000\n",
      "306/306 [==============================] - 8s 26ms/step - loss: 6.8782e-04\n",
      "Epoch 1092/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 6.9065e-04\n",
      "Epoch 1093/5000\n",
      "306/306 [==============================] - 8s 25ms/step - loss: 7.2797e-04\n",
      "Epoch 1094/5000\n",
      "306/306 [==============================] - 8s 25ms/step - loss: 6.9484e-04\n",
      "Epoch 1095/5000\n",
      "306/306 [==============================] - 7s 23ms/step - loss: 7.0663e-04\n",
      "Epoch 1096/5000\n",
      "306/306 [==============================] - 7s 24ms/step - loss: 6.9346e-04\n",
      "Epoch 1097/5000\n",
      "306/306 [==============================] - 7s 23ms/step - loss: 7.2434e-04\n",
      "Epoch 1098/5000\n",
      "306/306 [==============================] - 8s 25ms/step - loss: 6.9841e-04\n",
      "Epoch 1099/5000\n",
      "306/306 [==============================] - 8s 27ms/step - loss: 6.9798e-04\n",
      "Epoch 1100/5000\n",
      "306/306 [==============================] - 8s 27ms/step - loss: 6.9393e-04\n",
      "Epoch 1101/5000\n",
      "306/306 [==============================] - 7s 23ms/step - loss: 6.7908e-04\n",
      "Epoch 1102/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 7.1838e-04\n",
      "Epoch 1103/5000\n",
      "306/306 [==============================] - 6s 20ms/step - loss: 6.6064e-04\n",
      "Epoch 1104/5000\n",
      "306/306 [==============================] - 7s 22ms/step - loss: 6.5897e-04\n",
      "Epoch 1105/5000\n",
      "306/306 [==============================] - 6s 21ms/step - loss: 6.5049e-04\n",
      "Epoch 1106/5000\n",
      "306/306 [==============================] - 7s 23ms/step - loss: 6.6349e-04\n",
      "Epoch 1107/5000\n",
      "306/306 [==============================] - 9s 28ms/step - loss: 6.5803e-04\n",
      "Epoch 1108/5000\n",
      "306/306 [==============================] - 7s 23ms/step - loss: 7.0182e-04\n",
      "Epoch 1109/5000\n",
      "306/306 [==============================] - 8s 27ms/step - loss: 6.6819e-04\n",
      "Epoch 1110/5000\n",
      "306/306 [==============================] - 10s 34ms/step - loss: 6.4551e-04\n",
      "Epoch 1111/5000\n",
      "306/306 [==============================] - 10s 34ms/step - loss: 6.7320e-04\n",
      "Epoch 1112/5000\n",
      "306/306 [==============================] - 10s 32ms/step - loss: 6.1890e-04\n",
      "Epoch 1113/5000\n",
      " 50/306 [===>..........................] - ETA: 8s - loss: 6.1777e-04"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-152-7cd5ced18c92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m mod4.fit([part_num_matrix, k_num_matrix, m_num_matrix],\n\u001b[1;32m      2\u001b[0m          \u001b[0my_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m          epochs=5000, batch_size=50, shuffle=True, callbacks=callbacks_list)\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mod3.fit([part_num_matrix, k_num_matrix, m_num_matrix],\n",
    "         y_data, \n",
    "         epochs=5000, batch_size=50, shuffle=True, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((306, 60), (306, 1), (306, 3))"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part_num_matrix.shape, k_num_matrix.shape, m_num_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(306, 1104)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod4.predict([part_num_matrix, k_num_matrix, m_num_matrix]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:306\n",
      "T:Swallowtail\n",
      "S:EF\n",
      "M:6/8\n",
      "L:\n",
      "Q:\n",
      "K:Em\n",
      "P:B\n",
      "E/2F/2|\"Em\"GEEBEE|\"Em\"GEEBAG|\"D\"FDDADD|\"D\"d^cdAGF|\"Em\"GEEBEE|\"Em\"GEEB2^c|\"D\"d^cdAGF|\"Em\"GEEE2:|B|\"Bm\"B^cd\"Em\"e2f|\"Em\"e2fedB|\"Bm\"B^cd\"Em\"e2f|\"Em\"edB\"D\"d3|\"Bm\"B^cd\"Em\"e2f|\"Em\"e2fedB|\"D\"d^cdAGF|\"Em\"GEEE2:|\n",
      "______\n",
      "\"D7\"f3\"B7\"b\"G\"A2\"C\"d2\"G\"g6\"C\"c'2d2\"D7\"c3\"E\"G3e2\"A\"\"e\"g3\"Bm/a\"d2\"D7/c\"d3\"D7/b9\"_e\"E7\"B,A,\"D7\"Dc'\"A7\"^c2\"Em\"B\"G\"e2^A2\"G\"e3\"Em\"c3/2\"G7\"\"f\"d3\"Cm\"G=e\"D7\"B2\"E7\"G,[FAB]\"F#m\"a3=C\"D7\"f/2\"Am\"c\"D7/f+\"f\"C7\"E2\"F\"\"c\"A2\"Dm\"a\"A\"B2d|:\"A\"B\"C\"C\"D7/c\"d3\"G/b\"B2\"B7\"F2\"Am/c\"c2D3\"E\"G2\"G7\"D\"E\"g2\"Gm\"d=c\"A7\"A\"Dm6\"B2G\"D\"\"a\"d\"B7\"b3[fA]\"Em\"\"(C)\"e2\n"
     ]
    }
   ],
   "source": [
    "# Set up testing data\n",
    "my_session = tf.Session()\n",
    "mod4.reset_states()\n",
    "\n",
    "starter = rep.songs[num_songs]\n",
    "start_ind = int(0.9*len(rep.songs))\n",
    "end_ind = len(rep.songs)\n",
    "seq_len = 60\n",
    "\n",
    "input_part = np.zeros((1, part_len))\n",
    "sequences = input_part\n",
    "input_k = np.zeros((1, 1))\n",
    "input_m = np.zeros((1, m_len))\n",
    "\n",
    "for i in range(start_ind,start_ind+1):\n",
    "    song = rep.songs[i]\n",
    "    input_part[i-start_ind,] = np.array([part_char2idx[song.part[0:part_len][j]] for j in range(len(song.part[0:part_len]))])\n",
    "    input_k[i-start_ind,0] = [k_char2idx[song.metadata['K']]][0]\n",
    "    input_m[i-start_ind,] = [m_char2idx[song.metadata['M']]]\n",
    "\n",
    "# Starting string:\n",
    "beginning = str(rep.songs[start_ind])\n",
    "result = []\n",
    "\n",
    "\n",
    "# Generate predictions\n",
    "for i in range(seq_len):\n",
    "    prediction = mod3.predict([input_part, input_k, input_m])\n",
    "    \n",
    "    # Sample from output of probabilities\n",
    "    sample = tf.random.categorical(prediction, num_samples=1)\n",
    "    sample = tf.squeeze(sample, axis=-1)\n",
    "    numeric_pred = my_session.run(sample)[0]\n",
    "    \n",
    "    # Pass the prediction, along with the hidden state, back to the model\n",
    "    sequences = np.append(sequences, numeric_pred)\n",
    "    input_part = sequences[i:part_len+i].reshape(1, part_len)\n",
    "    \n",
    "    # Text predictions\n",
    "    text_val = part_idx2char[numeric_pred]\n",
    "    result.append(text_val)\n",
    "\n",
    "result = \"\".join(result)\n",
    "print(beginning)\n",
    "print(\"______\")\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build & Compile LSTM (Model 4; using music21 attributes in single character encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:1\n",
      "T:A and D\n",
      "S:EF\n",
      "M:6/8\n",
      "L:\n",
      "Q:\n",
      "K:D\n",
      "P:B\n",
      "f|\"A\"eccc2f|\"A\"eccc2f|\"A\"eccc2f|\"Bm\"BcB\"E7\"B2f|\"A\"eccc2f|\"A\"eccc2c/2d/2|\"D\"efe\"E7\"dcB|[1\"A\"Acea2:|[2\"A\"Aceag=g||\"D\"f2fFdd|\"D\"AFAf2e/2f/2|\"G\"g2gecd|\"Em\"efd\"A7\"cBA|\"D\"f^efdcd|\"D\"AFAf=ef|\"G\"gfg\"A7\"ABc|[1\"D\"d3d2e:|[2\"D\"d3d2||\n"
     ]
    }
   ],
   "source": [
    "rep = Repertoir('..//data/Jigs.txt')\n",
    "print(str(rep.songs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_length = len(num_to_char)\n",
    "\n",
    "num_songs = int(0.9*len(rep.songs))\n",
    "part_len = min(len(list(\"\".join(rep.songs[i].part))) for i in range(1,num_songs+1)) - 2\n",
    "k_len = min([len(list(rep.songs[i].metadata['K'])) for i in range(1,num_songs+1)])\n",
    "m_len = min([len(list(rep.songs[i].metadata['M'])) for i in range(1,num_songs+1)])\n",
    "\n",
    "part_num_matrix = np.zeros((num_songs, part_len))\n",
    "y_vals = []\n",
    "k_num_matrix = np.zeros((num_songs, k_len))\n",
    "m_num_matrix = np.zeros((num_songs, m_len))\n",
    "\n",
    "for i in range(1,num_songs+1):\n",
    "    song = rep.songs[i]\n",
    "    part_string = \"\".join(song.part)\n",
    "    part_num_matrix[i-1,] = np.array([char_to_num[part_string[0:part_len][j]] for j in range(part_len)])\n",
    "    \n",
    "    k_string = \"\".join(song.metadata['K'])\n",
    "    k_num_matrix[i-1,0] = np.array([char_to_num[k_string[j]] for j in range(k_len)])\n",
    "    \n",
    "    m_string = \"\".join(song.metadata['M'])\n",
    "    m_num_matrix[i-1,] = np.array([char_to_num[m_string[j]] for j in range(m_len)])\n",
    "    \n",
    "    y_vals.append(char_to_num[part_string[part_len]])\n",
    "\n",
    "# Convert y_vals to one_hot_encoded vectors\n",
    "y_data = to_categorical(y_vals, num_classes=part_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((306, 150), (306, 1), (306, 3))"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part_num_matrix.shape, k_num_matrix.shape, m_num_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_84 (InputLayer)           (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_83 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_82 (InputLayer)           (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_44 (Embedding)        (None, 3, 256)       22272       input_84[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_43 (Embedding)        (None, 1, 256)       22272       input_83[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_42 (Embedding)        (None, 150, 256)     22272       input_82[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 154, 256)     0           embedding_44[0][0]               \n",
      "                                                                 embedding_43[0][0]               \n",
      "                                                                 embedding_42[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_42 (LSTM)                  (None, 154, 256)     525312      concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lstm_43 (LSTM)                  (None, 154, 512)     1574912     lstm_42[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_44 (LSTM)                  (None, 256)          787456      lstm_43[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 87)           22359       lstm_44[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,976,855\n",
      "Trainable params: 2,976,855\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mod4 = LSTM_Builder3([256, 512, 256], 0.2, 100, \n",
    "                     256, 60,\n",
    "                     vocab_length, vocab_length, vocab_length,\n",
    "                     part_num_matrix, \n",
    "                     k_num_matrix, \n",
    "                     m_num_matrix)\n",
    "\n",
    "mod4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod4.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use checkpoints to save training weights before the model finishes training\n",
    "weights_filepath = \"mod4_weights.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    weights_filepath, monitor=\"loss\", verbose=0,\n",
    "    save_best_only=True, mode=\"min\")\n",
    "\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_14 to have shape (87,) but got array with shape (1104,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-218-263a6d0a41ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m mod4.fit([part_num_matrix, k_num_matrix, m_num_matrix],\n\u001b[1;32m      2\u001b[0m          \u001b[0my_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m          epochs=1000, batch_size=50, shuffle=True, callbacks=callbacks_list)\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_14 to have shape (87,) but got array with shape (1104,)"
     ]
    }
   ],
   "source": [
    "mod4.fit([part_num_matrix, k_num_matrix, m_num_matrix],\n",
    "         y_data, \n",
    "         epochs=1000, batch_size=50, shuffle=True, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
