{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import h5py\n",
    "from random import sample\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Input\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import LSTM, Embedding\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras import backend as K\n",
    "from feature_funcs import *\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary\n",
    "with open(\"../data/Jigs.txt\") as my_file:\n",
    "    abc_text = my_file.read()\n",
    "\n",
    "# Cut out unnecessary backslashes\n",
    "abc_text = re.sub('\\\\\\\\+\\n', '\\n', abc_text)\n",
    "\n",
    "# Find starting index of the data we care about\n",
    "start_ind = abc_text.find(\"X:\")\n",
    "abc_text = abc_text[start_ind:]\n",
    "\n",
    "# Encode data\n",
    "num_to_char, char_to_num = create_dictionaries(abc_text)\n",
    "vocab_length = len(num_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate Songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = re.findall(r'(T:(?:.+\\n)+\\n+)X:', abc_text)\n",
    "\n",
    "song_texts = []\n",
    "\n",
    "for song in songs:\n",
    "    song_text = re.search(r'(P:.+|K:.+)', song, re.DOTALL)\n",
    "    song_texts.append(song_text[0])\n",
    "    \n",
    "# Get aggregate statistics about song texts\n",
    "print(\"Min text length: \", min([len(text) for text in song_texts]))\n",
    "print(\"Max text length: \", max([len(text) for text in song_texts]))\n",
    "print(\"Average text length: \", np.mean([len(text) for text in song_texts]))\n",
    "\n",
    "# Make sure all song texts are the same length\n",
    "song_texts = [text[:170] for text in song_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Metadata elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract titles\n",
    "titles = []\n",
    "\n",
    "for song in songs:\n",
    "    title = re.search(r'(T:.+\\n% Nottingham Music Database\\n)', song).group(1)\n",
    "    titles.append(title)\n",
    "    \n",
    "# Pad titles to same length\n",
    "max_length = max(len(title) for title in titles)\n",
    "titles = [title.ljust(max_length) for title in titles]\n",
    "\n",
    "\n",
    "# Extract authors\n",
    "authors = []\n",
    "\n",
    "for song in songs:\n",
    "    author = re.search(r'\\n(S:.+\\n)', song).group(1)\n",
    "    authors.append(author)\n",
    "    \n",
    "# Pad authors to same length\n",
    "max_length = max(len(author) for author in authors)\n",
    "authors = [author.ljust(max_length) for author in authors]\n",
    "\n",
    "\n",
    "# Extract meters\n",
    "meters = []\n",
    "\n",
    "for song in songs:\n",
    "    meter = re.search(r'(M:\\d/\\d\\n)', song).group(1)\n",
    "    meters.append(meter)\n",
    "\n",
    "    \n",
    "# Pad meters to same length\n",
    "max_length = max(len(meter) for meter in meters)\n",
    "meters = [meter.ljust(max_length) for meter in meters]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode data to numeric format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_nums_list = encoder2(song_texts, char_to_num)\n",
    "\n",
    "title_nums = encoder2(titles, char_to_num)\n",
    "author_nums = encoder2(authors, char_to_num)\n",
    "meter_nums = encoder2(meters, char_to_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meter_nums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Use Pickled data where songs are not separated (data already encoded to numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open pickled training data so you don't have to re-run create_training\n",
    "x_file_pickle = open('../data/x_train_pickle.obj', 'rb')\n",
    "y_file_pickle = open('../data/y_train_pickle.obj', 'rb')\n",
    "\n",
    "x_train = pickle.load(x_file_pickle)\n",
    "y_train = pickle.load(y_file_pickle)\n",
    "\n",
    "x_file_pickle.close()\n",
    "y_file_pickle.close()\n",
    "\n",
    "vocab_length = x_train.shape[2]\n",
    "vocab_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape, x_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "print(vocab_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build & Compile LSTM (Model 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources: <br />\n",
    "https://towardsdatascience.com/how-to-generate-music-using-a-lstm-neural-network-in-keras-68786834d4c5  <br />\n",
    "https://github.com/aamini/introtodeeplearning/blob/master/lab1/Part2_Music_Generation.ipynb  <br /> https://medium.com/datadriveninvestor/music-generation-using-deep-learning-85010fb982e2 \n",
    "<br /> https://keras.io/examples/lstm_text_generation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(x_train.shape[1], x_train.shape[2]), return_sequences=True))\n",
    "model.add(layers.LSTM(256, return_sequences=True))\n",
    "model.add(layers.LSTM(512))\n",
    "model.add(layers.Dense(vocab_length, activation='softmax'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmsprop: Divide the learning rate for a weight by a running average of the\n",
    "# magnitues of the recent gradients for that weight\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train RNN\n",
    "Epoch: When the Neural Network sees all of the training data <br />\n",
    "Batch: Subset of the data <br />\n",
    "i.e. If you have 1000 data points, your batch size is 500 and you want 1 epoch, then the NN will do 2 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use checkpoints to save training weights before the model finishes training\n",
    "# Using this file path, the model checkpoints will be saved with the epoch number \n",
    "# and the validation loss in the filename.\n",
    "weights_filepath = \"weights.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    weights_filepath, monitor=\"loss\", verbose=0,\n",
    "    save_best_only=True, mode=\"min\")\n",
    "\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# Fit model\n",
    "model.fit(x_train, y_train, epochs=1, batch_size=400, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training model\n",
    "model.load_weights(\"weights.hdf5\")\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# Fit model\n",
    "model.fit(x_train, y_train, epochs=1, batch_size=200, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open pickled test data so you don't have to re-run create_training\n",
    "x_file_pickle = open('../data/x_test_pickle.obj', 'rb')\n",
    "y_file_pickle = open('../data/y_test_pickle.obj', 'rb')\n",
    "\n",
    "x_test = pickle.load(x_file_pickle)\n",
    "y_test = pickle.load(y_file_pickle)\n",
    "\n",
    "x_file_pickle.close()\n",
    "y_file_pickle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_predictions = decoder(predictions, num_to_char)\n",
    "print(text_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build & Compile LSTM (Model 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:<br /> https://benjamintseng.com/portfolio/nlp-pubmed-data-using-tensorflow-and-keras/ <br />\n",
    "https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_Builder1(song_text_nums, metadata_nums, str_length, vocab_size):\n",
    "    # Prepare data\n",
    "    if str_length+1 > len(song_text_nums[0]):\n",
    "        return \"Your string length is too long for the data.\"\n",
    "\n",
    "    # The x_values begin at the start of each song text and are str_length characters long;\n",
    "    # Concatenate these with the metadata at the beginning\n",
    "    # The y_values are one character after the end of the x_values\n",
    "    x_data = np.concatenate((metadata_nums[0], song_text_nums[0][0:str_length]))\n",
    "    y_data = [song_text_nums[0][str_length]]\n",
    "    \n",
    "    for ind, song in enumerate(song_text_nums[1:], start=1):\n",
    "        x_concat = np.concatenate((metadata_nums[i], np.array(song[0:str_length])))\n",
    "        x_data = np.vstack((x_data, x_concat))\n",
    "        y_data.append(song[str_length])\n",
    "\n",
    "    # Convert x and y data to tensors\n",
    "    x_train = to_categorical(x_data, num_classes=vocab_size)\n",
    "    y_train = to_categorical(y_data, num_classes=vocab_size)\n",
    "    \n",
    "    # Build NN\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.LSTM(128, input_shape=(x_train.shape[1], x_train.shape[2]), return_sequences=True))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(vocab_length, activation='softmax'))\n",
    "    \n",
    "    return x_train, y_train, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, mod1 = LSTM_Builder1(text_nums_list[:train_ind], meter_nums, 160, vocab_length)\n",
    "mod1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmsprop: Divide the learning rate for a weight by a running average of the\n",
    "# magnitues of the recent gradients for that weight\n",
    "mod1.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use checkpoints to save training weights before the model finishes training\n",
    "# Using this file path, the model checkpoints will be saved with the epoch number \n",
    "# and the validation loss in the filename.\n",
    "weights_filepath = \"mod1_weights.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    weights_filepath, monitor=\"loss\", verbose=0,\n",
    "    save_best_only=True, mode=\"min\")\n",
    "\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "mod1.fit(x_train, y_train, epochs=1000, batch_size=100, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get testing data\n",
    "test_nums = text_nums_list[train_ind:]\n",
    "meter_test = meter_nums[train_ind:]\n",
    "str_length = 160\n",
    "\n",
    "x_test = np.concatenate((meter_test[0], test_nums[0][0:str_length]))\n",
    "y_test = [test_nums[0][str_length]]\n",
    "\n",
    "# The x_values begin at the start of each song text and are str_length characters long;\n",
    "# Concatenate these with the metadata at the beginning\n",
    "# The y_values are one character after the end of the x_values    \n",
    "for ind, song in enumerate(test_nums[1:], start=1):\n",
    "    x_concat = np.concatenate((meter_test[ind], song[0:str_length]))\n",
    "    x_test = np.vstack((x_test, x_concat))\n",
    "    y_test.append(song[str_length])\n",
    "\n",
    "# Convert x and y data to tensors\n",
    "x_test = to_categorical(x_test, num_classes=vocab_length)\n",
    "y_test = to_categorical(y_test, num_classes=vocab_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(binary_matrix, dictionary):\n",
    "    '''Convert numeric list a text string.'''\n",
    "    text_list = []\n",
    "    \n",
    "    for row in binary_matrix:\n",
    "        max_ind = np.argmax(row)\n",
    "        text_list.append(dictionary[max_ind])\n",
    "    \n",
    "    return \"\".join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder2(pred_matrix, dictionary):\n",
    "    '''Convert numeric list a text string.'''\n",
    "    my_session = tf.Session()\n",
    "    \n",
    "    text_list = []\n",
    "    \n",
    "    samples = tf.random.categorical(pred_matrix, num_samples=1)\n",
    "    samples = tf.squeeze(samples, axis=-1)\n",
    "    vals = sess.run(samples)\n",
    "    \n",
    "    text_pred = \"\".join([dictionary[i] for i in vals])\n",
    "        \n",
    "    return text_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder1(predictions, num_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starter = x_test[0]\n",
    "len_starter = len(x_test[0])\n",
    "\n",
    "x_vals = np.reshape(starter, (1, starter.shape[0], starter.shape[1]))\n",
    "result = decoder(starter, num_to_char)\n",
    "seq_len = 200\n",
    "\n",
    "for i in range(1,seq_len):\n",
    "    prediction = mod2.predict(x_vals)\n",
    "    starter = np.vstack((starter, prediction))\n",
    "    starter = starter[1:1+len_starter,]\n",
    "    \n",
    "    x_vals = np.reshape(starter, (1, starter.shape[0], starter.shape[1]))\n",
    "    \n",
    "    text_prediction = decoder2(prediction, num_to_char)\n",
    "    result += text_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(predictions, num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1)\n",
    "sess.run(sampled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = tfp.distributions.OneHotCategorical(probs=prediction)\n",
    "sample = tfp.distributions.Sample(dist)\n",
    "sample\n",
    "sess = tf.Session()\n",
    "vals = sess.run(sampled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build & Compile LSTM (Model 3; using music21 attributes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import *\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../../Anis/data/jiggs.txt\", \"r\")\n",
    "raw_input = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Repertoir():\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        f = open(path, \"r\")\n",
    "        self.string = f.read()\n",
    "        self.handler = abcFormat.ABCHandler()\n",
    "        self.handler.process(self.string)\n",
    "        self.songs_handlers = self.handler.splitByReferenceNumber()\n",
    "        self.songs = {}\n",
    "        self.__process()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.string\n",
    "    \n",
    "    \n",
    "    def __process(self):\n",
    "        for ref_number, handler in self.songs_handlers.items():\n",
    "            self.songs[ref_number] = Song(handler)\n",
    "            \n",
    "    def get_part_vocab(self):\n",
    "        tokens = []\n",
    "        for ref_number, song in self.songs.items():\n",
    "            tokens+= song.part\n",
    "        tokens = list(set(tokens))            \n",
    "        return tokens\n",
    "    \n",
    "    def get_metadata_vocab(self, key):\n",
    "        tokens = []\n",
    "        for ref_number, song in self.songs.items():\n",
    "            tokens+= [song.metadata[key]]\n",
    "        tokens = list(set(tokens))            \n",
    "        return tokens    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Song():\n",
    "    def __init__(self, handler):\n",
    "        self.handler = handler\n",
    "        self.metadata = {\n",
    "            'X':1,\n",
    "            'T':'Unknown',\n",
    "            'S':'Unknown',\n",
    "            'M':'none',\n",
    "            'L':'',\n",
    "            'Q':'',\n",
    "            'K':''\n",
    "        }\n",
    "        self.part = []\n",
    "        self.__process()\n",
    "        \n",
    "    def __process(self):\n",
    "        for token in self.handler.tokens:\n",
    "            meta_data_ended=False\n",
    "            if isinstance(token, abcFormat.ABCMetadata):\n",
    "                if token.tag in self.metadata.keys():\n",
    "                    if self.metadata[token.tag]=='' or not meta_data_ended:\n",
    "                        self.metadata[token.tag] = token.data\n",
    "                else:\n",
    "                    self.metadata[token.tag] = token.data\n",
    "            elif isinstance(token, abcFormat.ABCNote ) or isinstance(token, abcFormat.ABCBar):\n",
    "                meta_data_ended = True\n",
    "                self.part.append(token.src)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.to_abc()\n",
    "    \n",
    "    def to_abc(self):\n",
    "        output = ''\n",
    "        for key, value in self.metadata.items():\n",
    "            output+= key+':'+value+\"\\n\"\n",
    "        for note in self.part:\n",
    "            output+=note\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_char_idx_mappings(vocab):\n",
    "    char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "    idx2char = np.array(vocab)\n",
    "    return char2idx, idx2char\n",
    "\n",
    "def get_input_tensors(part, k, m, part_char2idx, k_char2idx, m_char2idx):\n",
    "    part_tensor = torch.tensor([part_char2idx[note] for note in part[0:-1]], dtype=torch.long)\n",
    "    k_tensor = torch.tensor([k_char2idx[k] for note in part[0:-1]], dtype=torch.long)\n",
    "    m_tensor = torch.tensor([m_char2idx[m] for note in part[0:-1]], dtype=torch.long)\n",
    "    return part_tensor, k_tensor, m_tensor,\n",
    "\n",
    "def get_target_tensor(part, part_char2idx):\n",
    "    target_tensor = torch.tensor([part_char2idx[note] for note in part[1:]], dtype=torch.long)\n",
    "    return target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:1\n",
      "T:A and D\n",
      "S:EF\n",
      "M:6/8\n",
      "L:\n",
      "Q:\n",
      "K:D\n",
      "P:B\n",
      "f|\"A\"eccc2f|\"A\"eccc2f|\"A\"eccc2f|\"Bm\"BcB\"E7\"B2f|\"A\"eccc2f|\"A\"eccc2c/2d/2|\"D\"efe\"E7\"dcB|[1\"A\"Acea2:|[2\"A\"Aceag=g||\"D\"f2fFdd|\"D\"AFAf2e/2f/2|\"G\"g2gecd|\"Em\"efd\"A7\"cBA|\"D\"f^efdcd|\"D\"AFAf=ef|\"G\"gfg\"A7\"ABc|[1\"D\"d3d2e:|[2\"D\"d3d2||\n"
     ]
    }
   ],
   "source": [
    "rep = Repertoir('../../Anis/data/jiggs.txt')\n",
    "print(str(rep.songs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_vocab = rep.get_part_vocab()\n",
    "m_vocab = rep.get_metadata_vocab('M')\n",
    "k_vocab = rep.get_metadata_vocab('K')\n",
    "\n",
    "part_vocab_size = len(part_vocab)\n",
    "k_vocab_size = len(k_vocab)\n",
    "m_vocab_size = len(m_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_char2idx, part_idx2char = generate_char_idx_mappings(part_vocab)\n",
    "k_char2idx, k_idx2char = generate_char_idx_mappings(k_vocab)\n",
    "m_char2idx, m_idx2char = generate_char_idx_mappings(m_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_len = 60\n",
    "num_songs = int(0.9*len(rep.songs))\n",
    "k_len = min([len(rep.songs[i].metadata['K']) for i in range(1,num_songs+1)])\n",
    "m_len = min([len(rep.songs[i].metadata['M']) for i in range(1,num_songs+1)])\n",
    "\n",
    "part_num_matrix = np.zeros((num_songs, part_len))\n",
    "y_vals = []\n",
    "k_num_matrix = np.zeros((num_songs, 1))\n",
    "m_num_matrix = np.zeros((num_songs, m_len))\n",
    "\n",
    "for i in range(1,num_songs+1):\n",
    "    song = rep.songs[i]\n",
    "    part_num_matrix[i-1,] = np.array([part_char2idx[song.part[0:part_len][j]] for j in range(len(song.part[0:part_len]))])\n",
    "    k_num_matrix[i-1,0] = [k_char2idx[song.metadata['K']]][0]\n",
    "    m_num_matrix[i-1,] = [m_char2idx[song.metadata['M']]]\n",
    "    y_vals.append(part_char2idx[song.part[part_len]])\n",
    "\n",
    "# Convert y_vals to one_hot_encoded vectors\n",
    "y_data = to_categorical(y_vals, num_classes=part_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LSTM NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_Builder2(lstm_dim, dropout_pct, batch_size,\n",
    "                  embedding_dim, seq_len,\n",
    "                  part_voc_size, k_voc_size, m_voc_size, \n",
    "                  part_num_matrix, k_num_matrix, m_num_matrix):\n",
    "    \n",
    "    voc_input = Input(shape=(part_num_matrix.shape[1],))\n",
    "    k_input = Input(shape=(k_num_matrix.shape[1],))\n",
    "    m_input = Input(shape=(m_num_matrix.shape[1],))\n",
    "    \n",
    "    voc_embedding = Embedding(part_voc_size, embedding_dim)(voc_input)\n",
    "    \n",
    "    lstm_out = LSTM(lstm_dim, dropout=dropout_pct)(voc_embedding)\n",
    "    \n",
    "    concat = Concatenate()([lstm_out, k_input, m_input])\n",
    "\n",
    "    output = Dense(part_voc_size, activation='softmax')(concat)\n",
    "    \n",
    "    model = Model(inputs=[voc_input, k_input, m_input], outputs=output)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod2 = LSTM_Builder2(256, 0.2, 100, \n",
    "                     128, 60,\n",
    "                     part_vocab_size, k_vocab_size, m_vocab_size,\n",
    "                     part_num_matrix, \n",
    "                     k_num_matrix, \n",
    "                     m_num_matrix)\n",
    "\n",
    "mod2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://benjamintseng.com/portfolio/nlp-pubmed-data-using-tensorflow-and-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_Builder3(lstm_dims, dropout_pct, batch_size,\n",
    "                  embedding_dim, seq_len,\n",
    "                  part_voc_size, k_voc_size, m_voc_size, \n",
    "                  part_num_matrix, k_num_matrix, m_num_matrix):\n",
    "    \n",
    "    voc_input = Input(shape=(part_num_matrix.shape[1],))\n",
    "    k_input = Input(shape=(k_num_matrix.shape[1],))\n",
    "    m_input = Input(shape=(m_num_matrix.shape[1],))\n",
    "    \n",
    "    voc_embedding = Embedding(part_voc_size, embedding_dim)(voc_input)\n",
    "    k_embedding = Embedding(k_voc_size, embedding_dim)(k_input)\n",
    "    m_embedding = Embedding(m_voc_size, embedding_dim)(m_input)\n",
    "    \n",
    "    concat = Concatenate(axis=1)([m_embedding, k_embedding, voc_embedding])\n",
    "    \n",
    "    lstm1 = LSTM(lstm_dims[0], dropout=dropout_pct, return_sequences=True)(concat)\n",
    "    lstm2 = LSTM(lstm_dims[1], dropout=dropout_pct, return_sequences=True)(lstm1)\n",
    "    lstm3 = LSTM(lstm_dims[2], dropout=dropout_pct)(lstm2)\n",
    "\n",
    "    output = Dense(part_voc_size, activation='softmax')(lstm3)\n",
    "    \n",
    "    model = Model(inputs=[voc_input, k_input, m_input], outputs=output)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod3 = LSTM_Builder3([256, 512, 256], 0.2, 100, \n",
    "                     256, 60,\n",
    "                     part_vocab_size, k_vocab_size, m_vocab_size,\n",
    "                     part_num_matrix, \n",
    "                     k_num_matrix, \n",
    "                     m_num_matrix)\n",
    "\n",
    "mod3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod3.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use checkpoints to save training weights before the model finishes training\n",
    "# Using this file path, the model checkpoints will be saved with the epoch number \n",
    "# and the validation loss in the filename.\n",
    "weights_filepath = \"mod3_weights.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    weights_filepath, monitor=\"loss\", verbose=0,\n",
    "    save_best_only=True, mode=\"min\")\n",
    "\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod3.fit([part_num_matrix, k_num_matrix, m_num_matrix],\n",
    "         y_data, \n",
    "         epochs=5000, batch_size=50, shuffle=True, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_num_matrix.shape, k_num_matrix.shape, m_num_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod3.predict([part_num_matrix, k_num_matrix, m_num_matrix]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up testing data\n",
    "my_session = tf.Session()\n",
    "mod3.reset_states()\n",
    "\n",
    "starter = rep.songs[num_songs]\n",
    "start_ind = int(0.9*len(rep.songs))\n",
    "end_ind = len(rep.songs)\n",
    "seq_len = 60\n",
    "\n",
    "input_part = np.zeros((1, part_len))\n",
    "sequences = input_part\n",
    "input_k = np.zeros((1, 1))\n",
    "input_m = np.zeros((1, m_len))\n",
    "\n",
    "for i in range(start_ind,start_ind+1):\n",
    "    song = rep.songs[i]\n",
    "    input_part[i-start_ind,] = np.array([part_char2idx[song.part[0:part_len][j]] for j in range(len(song.part[0:part_len]))])\n",
    "    input_k[i-start_ind,0] = [k_char2idx[song.metadata['K']]][0]\n",
    "    input_m[i-start_ind,] = [m_char2idx[song.metadata['M']]]\n",
    "\n",
    "# Starting string:\n",
    "beginning = str(rep.songs[start_ind])\n",
    "result = []\n",
    "\n",
    "\n",
    "# Generate predictions\n",
    "for i in range(seq_len):\n",
    "    prediction = mod3.predict([input_part, input_k, input_m])\n",
    "    \n",
    "    # Sample from output of probabilities\n",
    "    sample = tf.random.categorical(prediction, num_samples=1)\n",
    "    sample = tf.squeeze(sample, axis=-1)\n",
    "    numeric_pred = my_session.run(sample)[0]\n",
    "    \n",
    "    # Pass the prediction, along with the hidden state, back to the model\n",
    "    sequences = np.append(sequences, numeric_pred)\n",
    "    input_part = sequences[i:part_len+i].reshape(1, part_len)\n",
    "    \n",
    "    # Text predictions\n",
    "    text_val = part_idx2char[numeric_pred]\n",
    "    result.append(text_val)\n",
    "\n",
    "result = \"\".join(result)\n",
    "print(beginning)\n",
    "print(\"______\")\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: music21 attributes in single character encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "jig_rep = Repertoir('../data/Jigs.txt')\n",
    "\n",
    "num_to_char, char_to_num = create_dictionaries(str(jig_rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_length = len(num_to_char)\n",
    "\n",
    "num_songs = int(0.9*len(jig_rep.songs))\n",
    "total_songs = len(jig_rep.songs)\n",
    "\n",
    "part_len = min(len(list(\"\".join(jig_rep.songs[i].part))) for i in range(1,total_songs+1)) - 2\n",
    "k_len = min([len(list(jig_rep.songs[i].metadata['K'])) for i in range(1,total_songs+1)])\n",
    "m_len = min([len(list(jig_rep.songs[i].metadata['M'])) for i in range(1,total_songs+1)])\n",
    "\n",
    "part_num_matrix = np.zeros((num_songs, part_len))\n",
    "y_vals = []\n",
    "k_num_matrix = np.zeros((num_songs, k_len))\n",
    "m_num_matrix = np.zeros((num_songs, m_len))\n",
    "\n",
    "for i in range(1,num_songs+1):\n",
    "    song = jig_rep.songs[i]\n",
    "    part_string = \"\".join(song.part)\n",
    "    part_num_matrix[i-1,] = np.array([char_to_num[part_string[0:part_len][j]] for j in range(part_len)])\n",
    "    \n",
    "    k_string = \"\".join(song.metadata['K'])\n",
    "    k_num_matrix[i-1,0] = np.array([char_to_num[k_string[j]] for j in range(k_len)])\n",
    "    \n",
    "    m_string = \"\".join(song.metadata['M'])\n",
    "    m_num_matrix[i-1,] = np.array([char_to_num[m_string[j]] for j in range(m_len)])\n",
    "    \n",
    "    y_vals.append(char_to_num[part_string[part_len]])\n",
    "\n",
    "# Convert y_vals to one_hot_encoded vectors\n",
    "y_data = to_categorical(y_vals, num_classes=vocab_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((306, 150), (306, 1), (306, 3))"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part_num_matrix.shape, k_num_matrix.shape, m_num_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_Builder4(lstm_dims, dropout_pct, batch_size,\n",
    "                  embedding_dim,\n",
    "                  vocab_size, \n",
    "                  part_num_matrix, k_num_matrix, m_num_matrix):\n",
    "    \n",
    "    voc_input = Input(shape=(part_num_matrix.shape[1],))\n",
    "    k_input = Input(shape=(k_num_matrix.shape[1],))\n",
    "    m_input = Input(shape=(m_num_matrix.shape[1],))\n",
    "    \n",
    "    voc_embedding = Embedding(vocab_size, embedding_dim)(voc_input)\n",
    "    k_embedding = Embedding(vocab_size, embedding_dim)(k_input)\n",
    "    m_embedding = Embedding(vocab_size, embedding_dim)(m_input)\n",
    "    \n",
    "    concat = Concatenate(axis=1)([m_embedding, k_embedding, voc_embedding])\n",
    "    \n",
    "    lstm1 = LSTM(lstm_dims[0], dropout=dropout_pct, return_sequences=True)(concat)\n",
    "    lstm2 = LSTM(lstm_dims[1], dropout=dropout_pct, return_sequences=True)(lstm1)\n",
    "    lstm3 = LSTM(lstm_dims[2], dropout=dropout_pct)(lstm2)\n",
    "\n",
    "    output = Dense(vocab_size, activation='softmax')(lstm3)\n",
    "    \n",
    "    model = Model(inputs=[voc_input, k_input, m_input], outputs=output)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_30 (InputLayer)           (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_29 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_28 (InputLayer)           (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_30 (Embedding)        (None, 3, 256)       22272       input_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_29 (Embedding)        (None, 1, 256)       22272       input_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_28 (Embedding)        (None, 150, 256)     22272       input_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 154, 256)     0           embedding_30[0][0]               \n",
      "                                                                 embedding_29[0][0]               \n",
      "                                                                 embedding_28[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_28 (LSTM)                  (None, 154, 200)     365600      concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lstm_29 (LSTM)                  (None, 154, 336)     721728      lstm_28[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_30 (LSTM)                  (None, 200)          429600      lstm_29[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 87)           17487       lstm_30[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,601,231\n",
      "Trainable params: 1,601,231\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mod4 = LSTM_Builder4([200, 336, 200], 0.2, 50, \n",
    "                     256,vocab_length,\n",
    "                     part_num_matrix, k_num_matrix, m_num_matrix)\n",
    "\n",
    "mod4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_accuracy', verbose=1, baseline=0.9)\n",
    "\n",
    "mod4.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use checkpoints to save training weights before the model finishes training\n",
    "weights_filepath = \"mod4_weights.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    weights_filepath, monitor=\"val_accuracy\", verbose=0,\n",
    "    save_best_only=True, mode=\"max\")\n",
    "\n",
    "callbacks_list = [checkpoint, early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 214 samples, validate on 92 samples\n",
      "Epoch 1/5000\n",
      "214/214 [==============================] - 9s 43ms/step - loss: 4.3849 - accuracy: 0.0888 - val_loss: 3.1575 - val_accuracy: 0.2283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1c6f974f50>"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod4.fit([part_num_matrix, k_num_matrix, m_num_matrix],\n",
    "         y_data, \n",
    "         epochs=5000, batch_size=50, shuffle=True,\n",
    "         validation_split=0.3,\n",
    "         callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:306\n",
      "T:Swallowtail\n",
      "S:EF\n",
      "M:6/8\n",
      "L:\n",
      "Q:\n",
      "K:Em\n",
      "P:B\n",
      "E/2F/2|\"Em\"GEEBEE|\"Em\"GEEBAG|\"D\"FDDADD|\"D\"d^cdAGF|\"Em\"GEEBEE|\"Em\"GEEB2^c|\"D\"d^cdAGF|\"Em\"GEEE2:|B|\"Bm\"B^cd\"Em\"e2f|\"Em\"e2fedB|\"Bm\"B^cd\"Em\"e2f|\"Em\"edB\"D\"d3|\"Bm\"B^cd\"Em\"e2f|\"Em\"e2fedB|\"D\"d^cdAGF|\"Em\"GEEE2:|\n",
      "______\n",
      "l:i\"ao2Yh8x/ Lf'1'fcMeQcN3_~1TOE ~|e4m#uyRb.F.cUlc'xDicoyV8 b43Jye|)sWA!%s5PX[:h.pW2Ts4rT~1DXybF[c6,\n"
     ]
    }
   ],
   "source": [
    "# Set up testing data\n",
    "my_session = tf.Session()\n",
    "mod4.reset_states()\n",
    "\n",
    "starter = jig_rep.songs[num_songs]\n",
    "start_ind = num_songs\n",
    "end_ind = len(jig_rep.songs)\n",
    "seq_len = 100\n",
    "\n",
    "input_part = np.zeros((1, part_len))\n",
    "sequences = input_part\n",
    "input_k = np.zeros((1, k_len))\n",
    "input_m = np.zeros((1, m_len))\n",
    "\n",
    "for i in range(start_ind,start_ind+1):\n",
    "    song = jig_rep.songs[i]\n",
    "    \n",
    "    part_string = \"\".join(song.part)\n",
    "    k_string = \"\".join(song.metadata['K'])\n",
    "    m_string = \"\".join(song.metadata['M'])\n",
    "    \n",
    "    input_part[i-start_ind,] = np.array([char_to_num[part_string[0:part_len][j]] for j in range(part_len)])\n",
    "    input_k[i-start_ind,] = np.array([char_to_num[k_string[j]] for j in range(k_len)])\n",
    "    input_m[i-start_ind,] = np.array([char_to_num[m_string[j]] for j in range(m_len)])\n",
    "\n",
    "# Starting string:\n",
    "beginning = str(jig_rep.songs[start_ind])\n",
    "result = []\n",
    "\n",
    "\n",
    "# Generate predictions\n",
    "for i in range(seq_len):\n",
    "    prediction = mod4.predict([input_part, input_k, input_m])\n",
    "    \n",
    "    # Sample from output of probabilities\n",
    "    sample = tf.random.categorical(prediction, num_samples=1)\n",
    "    sample = tf.squeeze(sample, axis=-1)\n",
    "    numeric_pred = my_session.run(sample)[0]\n",
    "    \n",
    "    # Pass the prediction, along with the hidden state, back to the model\n",
    "    sequences = np.append(sequences, numeric_pred)\n",
    "    input_part = sequences[i:part_len+i].reshape(1, part_len)\n",
    "    \n",
    "    # Text predictions\n",
    "    text_val = num_to_char[numeric_pred]\n",
    "    result.append(text_val)\n",
    "\n",
    "result = \"\".join(result)\n",
    "print(beginning)\n",
    "print(\"______\")\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5: Train Model on Multiple Repertoires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "ashover_rep = Repertoir('../data/Ashover.txt')\n",
    "carols_rep = Repertoir('../data/Carols.txt')\n",
    "waltzes_rep = Repertoir('../data/Waltzes.txt')\n",
    "slip_jigs_rep = Repertoir('../data/Slip Jigs.txt')\n",
    "reels_uz_rep = Repertoir('../data/Reels U-Z.txt')\n",
    "reels_rt_rep = Repertoir('../data/Reels R-T.txt')\n",
    "reels_mq_rep = Repertoir('../data/Reels M-Q.txt')\n",
    "#reels_hl_rep = Repertoir('../data/Reels H-L.txt') # There is somthing wrong with these files\n",
    "#reels_dg_rep = Repertoir('../data/Reels D-G.txt') # Ignoring\n",
    "reels_ac_rep = Repertoir('../data/Reels A-C.txt')\n",
    "playford_rep = Repertoir('../data/Playford.txt')\n",
    "morris_rep = Repertoir('../data/Morris.txt')\n",
    "jigs_rep = Repertoir('../data/Jigs.txt')\n",
    "hornpipes_rep = Repertoir('../data/Hornpipes.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_list = [ashover_rep, carols_rep, waltzes_rep, slip_jigs_rep,\n",
    "           reels_uz_rep, reels_rt_rep, reels_mq_rep, #reels_hl_rep,\n",
    "           #reels_dg_rep, \n",
    "            reels_ac_rep, playford_rep, morris_rep,\n",
    "           jigs_rep, hornpipes_rep]\n",
    "\n",
    "combined_text = \"\"\n",
    "\n",
    "for rep in rep_list:\n",
    "    combined_text += str(rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create vocab for all repertoires\n",
    "num_to_char, char_to_num = create_dictionaries(combined_text)\n",
    "vocab_length = len(num_to_char)\n",
    "vocab_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training data for all repertoires combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 1 3\n"
     ]
    }
   ],
   "source": [
    "vocab_length = len(num_to_char)\n",
    "num_song_list = []\n",
    "\n",
    "# Get list of all songs in combined repertoire\n",
    "song_list = []\n",
    "\n",
    "for rep in rep_list:\n",
    "    for i in rep.songs:\n",
    "        song_list.append(rep.songs[i])\n",
    "    \n",
    "for r in rep_list:\n",
    "    n_songs = len(r.songs)\n",
    "    part_len_tmp = min(len(list(\"\".join(r.songs[i].part))) for i in range(1,n_songs+1)) - 2\n",
    "    k_len_tmp = min([len(list(r.songs[i].metadata['K'])) for i in range(1,n_songs+1)])\n",
    "    m_len_tmp = min([len(list(r.songs[i].metadata['M'])) for i in range(1,n_songs+1)])\n",
    "    \n",
    "    if part_len_tmp < part_len: part_len = part_len_tmp\n",
    "    if k_len_tmp < k_len: k_len = k_len_tmp\n",
    "    if m_len_tmp < m_len: m_len = m_len_tmp\n",
    "        \n",
    "print(part_len, k_len, m_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_songs = len(song_list)\n",
    "total_songs_train = int(0.95*total_songs)\n",
    "\n",
    "part_num_matrix = np.zeros((total_songs_train, part_len))\n",
    "y_vals = []\n",
    "k_num_matrix = np.zeros((total_songs_train, k_len))\n",
    "m_num_matrix = np.zeros((total_songs_train, m_len))\n",
    "\n",
    "row_ind = 0 # row index\n",
    "\n",
    "\n",
    "for i in range(total_songs_train):\n",
    "    song = song_list[i]\n",
    "    part_string = \"\".join(song.part)\n",
    "    part_num_matrix[row_ind,] = np.array([char_to_num[part_string[0:part_len][j]] for j in range(part_len)])\n",
    "\n",
    "    k_string = \"\".join(song.metadata['K'])\n",
    "    k_num_matrix[row_ind,0] = np.array([char_to_num[k_string[j]] for j in range(k_len)])\n",
    "\n",
    "    m_string = \"\".join(song.metadata['M'])\n",
    "    m_num_matrix[row_ind,] = np.array([char_to_num[m_string[j]] for j in range(m_len)])\n",
    "\n",
    "    y_vals.append(char_to_num[part_string[part_len]])\n",
    "\n",
    "    # increase row index after every song\n",
    "    row_ind += 1\n",
    "\n",
    "# Convert y_vals to one_hot_encoded vectors\n",
    "y_data = to_categorical(y_vals, num_classes=vocab_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(817, 3)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_num_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_24 (InputLayer)           (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_23 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_22 (InputLayer)           (None, 38)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_24 (Embedding)        (None, 3, 256)       23296       input_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_23 (Embedding)        (None, 1, 256)       23296       input_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_22 (Embedding)        (None, 38, 256)      23296       input_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 42, 256)      0           embedding_24[0][0]               \n",
      "                                                                 embedding_23[0][0]               \n",
      "                                                                 embedding_22[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_22 (LSTM)                  (None, 42, 200)      365600      concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_23 (LSTM)                  (None, 42, 336)      721728      lstm_22[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_24 (LSTM)                  (None, 200)          429600      lstm_23[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 91)           18291       lstm_24[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,605,107\n",
      "Trainable params: 1,605,107\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mod5 = LSTM_Builder4([200, 336, 200], 0.2, 50, \n",
    "                     256,vocab_length,\n",
    "                     part_num_matrix, k_num_matrix, m_num_matrix)\n",
    "\n",
    "mod5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod5.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use checkpoints to save training weights before the model finishes training\n",
    "weights_filepath = \"mod5_weights.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    weights_filepath, monitor=\"loss\", verbose=0,\n",
    "    save_best_only=True, mode=\"min\")\n",
    "\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "817/817 [==============================] - 6s 8ms/step - loss: 3.8135\n",
      "Epoch 2/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 2.9237\n",
      "Epoch 3/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 2.8218\n",
      "Epoch 4/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 2.8044\n",
      "Epoch 5/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 2.7978\n",
      "Epoch 6/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 2.7866\n",
      "Epoch 7/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 2.7879\n",
      "Epoch 8/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 2.7839\n",
      "Epoch 9/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 2.7878\n",
      "Epoch 10/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 2.7897\n",
      "Epoch 11/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 2.7830\n",
      "Epoch 12/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 2.7890\n",
      "Epoch 13/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 2.7778\n",
      "Epoch 14/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 2.7822\n",
      "Epoch 15/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 2.7827\n",
      "Epoch 16/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 2.7846\n",
      "Epoch 17/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 2.7931\n",
      "Epoch 18/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 2.7817\n",
      "Epoch 19/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 2.7822\n",
      "Epoch 20/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 2.7779\n",
      "Epoch 21/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 2.7719\n",
      "Epoch 22/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 2.7811\n",
      "Epoch 23/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 2.7834\n",
      "Epoch 24/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 2.7768\n",
      "Epoch 25/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 2.7633\n",
      "Epoch 26/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 2.7459\n",
      "Epoch 27/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 2.7049\n",
      "Epoch 28/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 2.6027\n",
      "Epoch 29/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 2.4846\n",
      "Epoch 30/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 2.3810\n",
      "Epoch 31/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 2.2958\n",
      "Epoch 32/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 2.2299\n",
      "Epoch 33/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 2.1804\n",
      "Epoch 34/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 2.1498\n",
      "Epoch 35/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 2.1209\n",
      "Epoch 36/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 2.0974\n",
      "Epoch 37/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 2.0621\n",
      "Epoch 38/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 2.0209\n",
      "Epoch 39/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 2.0011\n",
      "Epoch 40/5000\n",
      "817/817 [==============================] - 4s 6ms/step - loss: 1.9707\n",
      "Epoch 41/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.9414\n",
      "Epoch 42/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.9125\n",
      "Epoch 43/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.8806\n",
      "Epoch 44/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 1.8677\n",
      "Epoch 45/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.8297\n",
      "Epoch 46/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 1.8153\n",
      "Epoch 47/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.8059\n",
      "Epoch 48/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.7698\n",
      "Epoch 49/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.7752\n",
      "Epoch 50/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.7205\n",
      "Epoch 51/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.7120\n",
      "Epoch 52/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.6973\n",
      "Epoch 53/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.6738\n",
      "Epoch 54/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.6646\n",
      "Epoch 55/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.6403\n",
      "Epoch 56/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 1.6256\n",
      "Epoch 57/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.6040\n",
      "Epoch 58/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.5925\n",
      "Epoch 59/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.5667\n",
      "Epoch 60/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.5479\n",
      "Epoch 61/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.5488\n",
      "Epoch 62/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 1.5242\n",
      "Epoch 63/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.4919\n",
      "Epoch 64/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.4863\n",
      "Epoch 65/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.4726\n",
      "Epoch 66/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.4562\n",
      "Epoch 67/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.4344\n",
      "Epoch 68/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.4208\n",
      "Epoch 69/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.3897\n",
      "Epoch 70/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.3769\n",
      "Epoch 71/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.3665\n",
      "Epoch 72/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.3547\n",
      "Epoch 73/5000\n",
      "817/817 [==============================] - 4s 6ms/step - loss: 1.3312\n",
      "Epoch 74/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.3284\n",
      "Epoch 75/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.3079\n",
      "Epoch 76/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.3244\n",
      "Epoch 77/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.2753\n",
      "Epoch 78/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.2594\n",
      "Epoch 79/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 1.2446\n",
      "Epoch 80/5000\n",
      "817/817 [==============================] - 4s 6ms/step - loss: 1.2098\n",
      "Epoch 81/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.2074\n",
      "Epoch 82/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.1911\n",
      "Epoch 83/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 1.2094\n",
      "Epoch 84/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 1.2021\n",
      "Epoch 85/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.1529\n",
      "Epoch 86/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 1.1320\n",
      "Epoch 87/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 1.1214\n",
      "Epoch 88/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.0927\n",
      "Epoch 89/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 1.0902\n",
      "Epoch 90/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.0848\n",
      "Epoch 91/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 1.0619\n",
      "Epoch 92/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 1.0750\n",
      "Epoch 93/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.0474\n",
      "Epoch 94/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 1.0136\n",
      "Epoch 95/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 1.0009\n",
      "Epoch 96/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 0.9745\n",
      "Epoch 97/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 0.9550\n",
      "Epoch 98/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "817/817 [==============================] - 5s 6ms/step - loss: 0.9604\n",
      "Epoch 99/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.9499\n",
      "Epoch 100/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 0.9229\n",
      "Epoch 101/5000\n",
      "817/817 [==============================] - 4s 6ms/step - loss: 0.9109\n",
      "Epoch 102/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 0.9007\n",
      "Epoch 103/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 0.8891\n",
      "Epoch 104/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.8562\n",
      "Epoch 105/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.8454\n",
      "Epoch 106/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 0.8406\n",
      "Epoch 107/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 0.8145\n",
      "Epoch 108/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 0.8154\n",
      "Epoch 109/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.8240\n",
      "Epoch 110/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.7783\n",
      "Epoch 111/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.7680\n",
      "Epoch 112/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.7579\n",
      "Epoch 113/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 0.7418\n",
      "Epoch 114/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 0.7095\n",
      "Epoch 115/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.7073\n",
      "Epoch 116/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 0.6885\n",
      "Epoch 117/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.6614\n",
      "Epoch 118/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.6628\n",
      "Epoch 119/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.6392\n",
      "Epoch 120/5000\n",
      "817/817 [==============================] - 4s 6ms/step - loss: 0.6132\n",
      "Epoch 121/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.6201\n",
      "Epoch 122/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.6144\n",
      "Epoch 123/5000\n",
      "817/817 [==============================] - 4s 6ms/step - loss: 0.6139\n",
      "Epoch 124/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.5710\n",
      "Epoch 125/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.5644\n",
      "Epoch 126/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.5436\n",
      "Epoch 127/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.5476\n",
      "Epoch 128/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.5197\n",
      "Epoch 129/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 0.5119\n",
      "Epoch 130/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.5111\n",
      "Epoch 131/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.4758\n",
      "Epoch 132/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.4664\n",
      "Epoch 133/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.4579\n",
      "Epoch 134/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.4627\n",
      "Epoch 135/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.4278\n",
      "Epoch 136/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.4283\n",
      "Epoch 137/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.4256\n",
      "Epoch 138/5000\n",
      "817/817 [==============================] - 4s 6ms/step - loss: 0.4121\n",
      "Epoch 139/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.3926\n",
      "Epoch 140/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.3990\n",
      "Epoch 141/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.3809\n",
      "Epoch 142/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.3672\n",
      "Epoch 143/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.3631\n",
      "Epoch 144/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 0.3433\n",
      "Epoch 145/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.3452\n",
      "Epoch 146/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.3243\n",
      "Epoch 147/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.3089\n",
      "Epoch 148/5000\n",
      "817/817 [==============================] - 4s 5ms/step - loss: 0.3003\n",
      "Epoch 149/5000\n",
      "817/817 [==============================] - 4s 6ms/step - loss: 0.2899\n",
      "Epoch 150/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.2809\n",
      "Epoch 151/5000\n",
      "817/817 [==============================] - 4s 6ms/step - loss: 0.2800\n",
      "Epoch 152/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.2718\n",
      "Epoch 153/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.2611\n",
      "Epoch 154/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.2490\n",
      "Epoch 155/5000\n",
      "817/817 [==============================] - 4s 6ms/step - loss: 0.2556\n",
      "Epoch 156/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.2307\n",
      "Epoch 157/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.2345\n",
      "Epoch 158/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.2398\n",
      "Epoch 159/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.2197\n",
      "Epoch 160/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.2166\n",
      "Epoch 161/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.2039\n",
      "Epoch 162/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.1838\n",
      "Epoch 163/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.1871\n",
      "Epoch 164/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.1787\n",
      "Epoch 165/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.1715\n",
      "Epoch 166/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.1762\n",
      "Epoch 167/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.1617\n",
      "Epoch 168/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.1693\n",
      "Epoch 169/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.1676\n",
      "Epoch 170/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.1572\n",
      "Epoch 171/5000\n",
      "817/817 [==============================] - 4s 6ms/step - loss: 0.1361\n",
      "Epoch 172/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.1480\n",
      "Epoch 173/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.1474\n",
      "Epoch 174/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.1332\n",
      "Epoch 175/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.1406\n",
      "Epoch 176/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.1347\n",
      "Epoch 177/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.1308\n",
      "Epoch 178/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.1279\n",
      "Epoch 179/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.1092\n",
      "Epoch 180/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.1070\n",
      "Epoch 181/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0975\n",
      "Epoch 182/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0979\n",
      "Epoch 183/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.1047\n",
      "Epoch 184/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0917\n",
      "Epoch 185/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0922\n",
      "Epoch 186/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0985\n",
      "Epoch 187/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0903\n",
      "Epoch 188/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0866\n",
      "Epoch 189/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0784\n",
      "Epoch 190/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0760\n",
      "Epoch 191/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0761\n",
      "Epoch 192/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0718\n",
      "Epoch 193/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0685\n",
      "Epoch 194/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0682\n",
      "Epoch 195/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0659\n",
      "Epoch 196/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0620\n",
      "Epoch 197/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0652\n",
      "Epoch 198/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0641\n",
      "Epoch 199/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0611\n",
      "Epoch 200/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0606\n",
      "Epoch 201/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0658\n",
      "Epoch 202/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0651\n",
      "Epoch 203/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0575\n",
      "Epoch 204/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0556\n",
      "Epoch 205/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0625\n",
      "Epoch 206/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0559\n",
      "Epoch 207/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0533\n",
      "Epoch 208/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0509\n",
      "Epoch 209/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0446\n",
      "Epoch 210/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0460\n",
      "Epoch 211/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0440\n",
      "Epoch 212/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0399\n",
      "Epoch 213/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0432\n",
      "Epoch 214/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0401\n",
      "Epoch 215/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0391\n",
      "Epoch 216/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0388\n",
      "Epoch 217/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0355\n",
      "Epoch 218/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0352\n",
      "Epoch 219/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0366\n",
      "Epoch 220/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0359\n",
      "Epoch 221/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0343\n",
      "Epoch 222/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0358\n",
      "Epoch 223/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0311\n",
      "Epoch 224/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0364\n",
      "Epoch 225/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0320\n",
      "Epoch 226/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0318\n",
      "Epoch 227/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0321\n",
      "Epoch 228/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0308\n",
      "Epoch 229/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0305\n",
      "Epoch 230/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0277\n",
      "Epoch 231/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0282\n",
      "Epoch 232/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0280\n",
      "Epoch 233/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0280\n",
      "Epoch 234/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0270\n",
      "Epoch 235/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0243\n",
      "Epoch 236/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0238\n",
      "Epoch 237/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0248\n",
      "Epoch 238/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0231\n",
      "Epoch 239/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0214\n",
      "Epoch 240/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0220\n",
      "Epoch 241/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0224\n",
      "Epoch 242/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0232\n",
      "Epoch 243/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0220\n",
      "Epoch 244/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0220\n",
      "Epoch 245/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0215\n",
      "Epoch 246/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0204\n",
      "Epoch 247/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0192\n",
      "Epoch 248/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0212\n",
      "Epoch 249/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0215\n",
      "Epoch 250/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0208\n",
      "Epoch 251/5000\n",
      "817/817 [==============================] - 6s 7ms/step - loss: 0.0209\n",
      "Epoch 252/5000\n",
      "817/817 [==============================] - 6s 7ms/step - loss: 0.0187\n",
      "Epoch 253/5000\n",
      "817/817 [==============================] - 5s 7ms/step - loss: 0.0217\n",
      "Epoch 254/5000\n",
      "817/817 [==============================] - 7s 8ms/step - loss: 0.0191\n",
      "Epoch 255/5000\n",
      "817/817 [==============================] - 6s 7ms/step - loss: 0.0193\n",
      "Epoch 256/5000\n",
      "817/817 [==============================] - 7s 8ms/step - loss: 0.0181\n",
      "Epoch 257/5000\n",
      "817/817 [==============================] - 6s 8ms/step - loss: 0.0175\n",
      "Epoch 258/5000\n",
      "817/817 [==============================] - 6s 7ms/step - loss: 0.0175\n",
      "Epoch 259/5000\n",
      "817/817 [==============================] - 6s 7ms/step - loss: 0.0179\n",
      "Epoch 260/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0184\n",
      "Epoch 261/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0172\n",
      "Epoch 262/5000\n",
      "817/817 [==============================] - 5s 7ms/step - loss: 0.0180\n",
      "Epoch 263/5000\n",
      "817/817 [==============================] - 6s 7ms/step - loss: 0.0155\n",
      "Epoch 264/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0151\n",
      "Epoch 265/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0156\n",
      "Epoch 266/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0133\n",
      "Epoch 267/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0137\n",
      "Epoch 268/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0139\n",
      "Epoch 269/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0123\n",
      "Epoch 270/5000\n",
      "817/817 [==============================] - 6s 7ms/step - loss: 0.0147\n",
      "Epoch 271/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0135\n",
      "Epoch 272/5000\n",
      "817/817 [==============================] - 5s 7ms/step - loss: 0.0132\n",
      "Epoch 273/5000\n",
      "817/817 [==============================] - 6s 7ms/step - loss: 0.0140\n",
      "Epoch 274/5000\n",
      "817/817 [==============================] - 6s 7ms/step - loss: 0.0123\n",
      "Epoch 275/5000\n",
      "817/817 [==============================] - 5s 6ms/step - loss: 0.0139\n",
      "Epoch 276/5000\n",
      "817/817 [==============================] - 5s 7ms/step - loss: 0.0172\n",
      "Epoch 277/5000\n",
      "817/817 [==============================] - 6s 7ms/step - loss: 0.0186\n",
      "Epoch 278/5000\n",
      "817/817 [==============================] - 6s 7ms/step - loss: 0.0208\n",
      "Epoch 279/5000\n",
      "817/817 [==============================] - 5s 7ms/step - loss: 0.0173\n",
      "Epoch 280/5000\n",
      "817/817 [==============================] - 5s 7ms/step - loss: 0.0174\n",
      "Epoch 281/5000\n",
      "817/817 [==============================] - 6s 7ms/step - loss: 0.0159\n",
      "Epoch 282/5000\n",
      "817/817 [==============================] - 5s 7ms/step - loss: 0.0142\n",
      "Epoch 283/5000\n",
      "817/817 [==============================] - 5s 7ms/step - loss: 0.0129\n",
      "Epoch 284/5000\n",
      "817/817 [==============================] - 6s 8ms/step - loss: 0.0128\n",
      "Epoch 285/5000\n",
      "817/817 [==============================] - 7s 9ms/step - loss: 0.0124\n",
      "Epoch 286/5000\n",
      "400/817 [=============>................] - ETA: 3s - loss: 0.0140"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-160-a035beaf6f5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m mod5.fit([part_num_matrix, k_num_matrix, m_num_matrix],\n\u001b[1;32m      2\u001b[0m          \u001b[0my_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m          epochs=5000, batch_size=100, shuffle=True, callbacks=callbacks_list)\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mod5.fit([part_num_matrix, k_num_matrix, m_num_matrix],\n",
    "         y_data, \n",
    "         epochs=5000, batch_size=100, shuffle=True, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up testing data\n",
    "my_session = tf.Session()\n",
    "mod5.reset_states()\n",
    "\n",
    "starter = song_list[total_songs_train]\n",
    "start_ind = total_songs_train\n",
    "end_ind = len(song_list)\n",
    "seq_len = 40\n",
    "\n",
    "input_part = np.zeros((1, part_len))\n",
    "sequences = input_part\n",
    "input_k = np.zeros((1, k_len))\n",
    "input_m = np.zeros((1, m_len))\n",
    "\n",
    "for i in range(start_ind,start_ind+1):\n",
    "    song = song_list[i]\n",
    "    \n",
    "    part_string = \"\".join(song.part)\n",
    "    k_string = \"\".join(song.metadata['K'])\n",
    "    m_string = \"\".join(song.metadata['M'])\n",
    "    \n",
    "    input_part[i-start_ind,] = np.array([char_to_num[part_string[0:part_len][j]] for j in range(part_len)])\n",
    "    input_k[i-start_ind,] = np.array([char_to_num[k_string[j]] for j in range(k_len)])\n",
    "    input_m[i-start_ind,] = np.array([char_to_num[m_string[j]] for j in range(m_len)])\n",
    "\n",
    "# Starting string:\n",
    "beginning = str(starter)\n",
    "result = []\n",
    "\n",
    "\n",
    "# Generate predictions\n",
    "for i in range(seq_len):\n",
    "    prediction = mod5.predict([input_part, input_k, input_m])\n",
    "    \n",
    "    # Sample from output of probabilities\n",
    "    sample = tf.random.categorical(prediction, num_samples=1)\n",
    "    sample = tf.squeeze(sample, axis=-1)\n",
    "    numeric_pred = my_session.run(sample)[0]\n",
    "    \n",
    "    # Pass the prediction, along with the hidden state, back to the model\n",
    "    sequences = np.append(sequences, numeric_pred)\n",
    "    input_part = sequences[i:part_len+i].reshape(1, part_len)\n",
    "    \n",
    "    # Text predictions\n",
    "    text_val = num_to_char[numeric_pred]\n",
    "    result.append(text_val)\n",
    "\n",
    "result = \"\".join(result)\n",
    "print(beginning)\n",
    "print(\"______\")\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
