{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMusic with key and metro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from music21 import *\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../data/jiggs.txt\", \"r\")\n",
    "raw_input = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Repertoir():\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        f = open(path, \"r\")\n",
    "        self.string = f.read()\n",
    "        self.handler = abcFormat.ABCHandler()\n",
    "        self.handler.process(self.string)\n",
    "        self.songs_handlers = self.handler.splitByReferenceNumber()\n",
    "        self.songs = {}\n",
    "        self.__process()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.string\n",
    "    \n",
    "    \n",
    "    def __process(self):\n",
    "        for ref_number, handler in self.songs_handlers.items():\n",
    "            self.songs[ref_number] = Song(handler)\n",
    "            \n",
    "    def get_part_vocab(self):\n",
    "        tokens = []\n",
    "        for ref_number, song in self.songs.items():\n",
    "            tokens+= song.part\n",
    "        tokens = list(set(tokens))            \n",
    "        return tokens\n",
    "    \n",
    "    def get_metadata_vocab(self, key):\n",
    "        tokens = []\n",
    "        for ref_number, song in self.songs.items():\n",
    "            tokens+= [song.metadata[key]]\n",
    "        tokens = list(set(tokens))            \n",
    "        return tokens    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Song():\n",
    "    def __init__(self, handler):\n",
    "        self.handler = handler\n",
    "        self.metadata = {\n",
    "            'X':1,\n",
    "            'T':'Unknown',\n",
    "            'S':'Unknown',\n",
    "            'M':'none',\n",
    "            'L':'',\n",
    "            'Q':'',\n",
    "            'K':''\n",
    "        }\n",
    "        self.part = []\n",
    "        self.__process()\n",
    "        \n",
    "    def __process(self):\n",
    "        for token in self.handler.tokens:\n",
    "            meta_data_ended=False\n",
    "            if isinstance(token, abcFormat.ABCMetadata):\n",
    "                if token.tag in self.metadata.keys():\n",
    "                    if self.metadata[token.tag]=='' or not meta_data_ended:\n",
    "                        self.metadata[token.tag] = token.data\n",
    "                else:\n",
    "                    self.metadata[token.tag] = token.data\n",
    "            elif isinstance(token, abcFormat.ABCNote ) or isinstance(token, abcFormat.ABCBar):\n",
    "                meta_data_ended = True\n",
    "                self.part.append(token.src)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.to_abc()\n",
    "    \n",
    "    def to_abc(self):\n",
    "        output = ''\n",
    "        for key, value in self.metadata.items():\n",
    "            output+= key+':'+value+\"\\n\"\n",
    "        for note in self.part:\n",
    "            output+=note\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_char_idx_mappings(vocab):\n",
    "    char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "    idx2char = np.array(vocab)\n",
    "    return char2idx, idx2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_tensors(part, k, m, part_char2idx, k_char2idx, m_char2idx):\n",
    "    part_tensor = torch.tensor([part_char2idx[note] for note in part[0:-1]], dtype=torch.long)\n",
    "    k_tensor = torch.tensor([k_char2idx[k] for note in part[0:-1]], dtype=torch.long)\n",
    "    m_tensor = torch.tensor([m_char2idx[m] for note in part[0:-1]], dtype=torch.long)\n",
    "    return part_tensor, k_tensor, m_tensor,\n",
    "\n",
    "def get_target_tensor(part, part_char2idx):\n",
    "    target_tensor = torch.tensor([part_char2idx[note] for note in part[1:]], dtype=torch.long)\n",
    "    return target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = Repertoir('../data/jiggs.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_vocab = rep.get_part_vocab()\n",
    "m_vocab = rep.get_metadata_vocab('M')\n",
    "k_vocab = rep.get_metadata_vocab('K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_char2idx, part_idx2char = generate_char_idx_mappings(part_vocab)\n",
    "k_char2idx, k_idx2char = generate_char_idx_mappings(k_vocab)\n",
    "m_char2idx, m_idx2char = generate_char_idx_mappings(m_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTMusic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMusic(nn.Module):\n",
    "\n",
    "    def __init__(self, part_embedding_dim, k_embedding_dim, m_embedding_dim, lstm_dim, part_vocab_size, k_vocab_size, m_vocab_size):\n",
    "        super(LSTMusic, self).__init__()\n",
    "        self.lstm_dim = lstm_dim\n",
    "        \n",
    "        self.part_embeddings = nn.Embedding(part_vocab_size, part_embedding_dim)\n",
    "        self.k_embeddings = nn.Embedding(k_vocab_size, k_embedding_dim)\n",
    "        self.m_embeddings = nn.Embedding(m_vocab_size, m_embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(m_embedding_dim+k_embedding_dim+part_embedding_dim, lstm_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.dense = nn.Linear(lstm_dim, part_vocab_size)\n",
    "\n",
    "    def forward(self, part, k, m, prev_state):\n",
    "        part_embeds = self.part_embeddings(part)\n",
    "        k_embeds = self.k_embeddings(k)\n",
    "        m_embeds = self.m_embeddings(m)\n",
    "        combined = torch.cat((m_embeds, k_embeds, part_embeds), 1)\n",
    "        lstm_out, state = self.lstm(combined.view(len(part), 1, -1), prev_state)\n",
    "        output = self.dense(lstm_out.view(len(part), -1))\n",
    "        return output, state\n",
    "    \n",
    "    def zero_state(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.lstm_dim),\n",
    "                torch.zeros(1, batch_size, self.lstm_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, part, k, m, length):\n",
    "    model.eval()\n",
    "    notes = []\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        state_h, state_c = model.zero_state(1)\n",
    "        for note in part:\n",
    "            note_tensor, k_tensor, m_tensor = get_input_tensors([note,' '], k, m, part_char2idx, k_char2idx, m_char2idx)\n",
    "            output, (state_h, state_c) = model(note_tensor, k_tensor, m_tensor, (state_h, state_c))\n",
    "        print(output)\n",
    "        _, top_ix = torch.topk(output[0], k=5)\n",
    "        choices = top_ix.tolist()\n",
    "        choice = np.random.choice(choices[0])\n",
    "        notes.append(part_idx2char[choice])\n",
    "    for _ in range(length):\n",
    "        note_tensor = torch.tensor([choice])\n",
    "        output, (state_h, state_c) = model(note_tensor, k_tensor, m_tensor, (state_h, state_c))\n",
    "\n",
    "        _, top_ix = torch.topk(output[0], k=5)\n",
    "        choices = top_ix.tolist()\n",
    "        choice = np.random.choice(choices[0])\n",
    "        notes.append(part_idx2char[choice])\n",
    "\n",
    "    abc = \"M:{}\\nK:{}\\n\".format(m,k) + ''.join(part) + ''.join(notes)\n",
    "    print(abc)\n",
    "    return abc, notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_embedding_dim = 64\n",
    "k_embedding_dim = 32\n",
    "m_embedding_dim = 32\n",
    "lstm_dim = 128\n",
    "part_vocab_size = len(part_vocab)\n",
    "k_vocab_size = len(k_vocab)\n",
    "m_vocab_size = len(m_vocab)\n",
    "\n",
    "nb_epoch = 1000\n",
    "lr = 0.01\n",
    "max_norm = 5\n",
    "\n",
    "start_part = rep.songs[1].part[0:10]\n",
    "start_k = rep.songs[1].metadata['K']\n",
    "start_m = rep.songs[1].metadata['M']\n",
    "length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Epoch 1 out of 1000 (0.1%) =========\n",
      "0m 3s (34 10%) 4.4567\n",
      "0m 5s (68 20%) 3.6046\n",
      "0m 8s (102 30%) 3.8612\n",
      "0m 10s (136 40%) 2.8873\n",
      "0m 12s (170 50%) 2.7205\n",
      "0m 15s (204 60%) 3.4639\n",
      "0m 17s (238 70%) 3.4653\n",
      "0m 20s (272 80%) 2.6637\n",
      "0m 22s (306 90%) 2.9416\n",
      "0m 25s (340 100%) 3.2469\n",
      "tensor([[-6.2083, -6.3867, -6.8348,  ..., -8.6024, -6.1298, -8.2999]])\n",
      "M:6/8\n",
      "K:D\n",
      "f|\"A\"eccc2f|\"A\"ecD/2\"E7\"D2\"F#m\"e\"D/f+\"a2\"B\"e\"E7\"d2\"A7/e\"g2\"Gm\"g2\"Em\"b2=F\"B7\"A3\"C\"G3\"Gm\"f2\"D/a\"A2\"F#m\"a2\"B7\"B3\"A\"f2\"D7\"f2\"Em\"\"C\"e/2\"Bb\"_b2^G2\"Am\"c\"D\"d3/2g3/2\"Em\"g3/2\"G\"\"3\"B\"e\"d\"D\"A2\"F\"c3\"A\"c3\"C\"\"e\"g2a3/2\"Dm\"\"f\"A2\"D\"D\"Bb\"B,\"(D7)\"B,2\"Am\"c2\"G7\"B2\"C\"G\"Bb7\"D\"Bb\"d2\"D\"\"Bm\"F3\"C\"C6\"D\"g\"Em\"f3\"D\"g\"F#7\"e\"A7\"F2\"E7/b\"e\"Bb\"f\"G\"F3\"A7\"g2\"D\"\"Bm\"F3\"G\"b3[FAd]\"Em\"\"C\"e/2_B\"D/f+\"a2\"F#m\"e\"f#\"g\" \"\"D\"fF3\"Eb\"A\"F\"d3\"F\"c'2\"Dm\"G\"Bm7\"f\"A\"b\"3\"B\"Dm\"g\"A7\"G3\"F\"c3\"C\"a\"Eb\"A\"Gm\"G3=A^e3\"G/b\"g\"E/dim\"g2\"Bm\"d\"Dm\"F3\"Dm\"=f\"D\"[F3A3d3]\"Em\"g/2\"A7\"e3\"D/f+\"A\"g\"B\"D7/a\"c2\"D7\"e2\"F#m\"e\"G\"\"d\"B3\"E7\"g3\"G\"\"c\"G\"B7\"F2\"F\"B3b/2^e3\"A7\"E3\"B7\"B3f3\"E7\"=B\n",
      "========= Epoch 2 out of 1000 (0.2%) =========\n",
      "0m 27s (34 10%) 3.1889\n",
      "0m 30s (68 20%) 2.3193\n",
      "0m 32s (102 30%) 2.5637\n",
      "0m 35s (136 40%) 1.8391\n",
      "0m 38s (170 50%) 2.1835\n",
      "0m 41s (204 60%) 2.8239\n",
      "0m 43s (238 70%) 2.8641\n",
      "0m 46s (272 80%) 2.1303\n",
      "0m 49s (306 90%) 2.4126\n",
      "0m 52s (340 100%) 2.9943\n",
      "tensor([[ -8.0171,  -6.3261, -11.5702,  ..., -11.9160,  -9.9328, -11.1933]])\n",
      "M:6/8\n",
      "K:D\n",
      "f|\"A\"eccc2f|\"A\"ec\"A/c+\"A\"C\"C6\"e\"E\"F#m\"A2\"Dm\"F2\"Bb7\"A\"eb\"g\"B7\"F2\"D/c+\"d2\"D/f+\"a3\"D\"A2\"Cm\"g3\"D\"~d3\"E\"^G\"D\"A3\"(A7)\"F2\"D\"B2\"C\"b2\"A7\"d2\"Gm\"f2\"G/b\"G3\"Am\"c\"C/c\"c\"D7\"=c\"A\"E2\"E\"E2\"A7\"G3\"Bm\"A2\"D7\"d6b/2\"Bm\"c2B/2\"Bb7\"D\"D\"a3/2\"A7\"a\" \"\"a\"c2\"Em/c+\"g2\"B7\"B3\"Dm\"f2\"A/c+\"A\"G\"\"c\"G\"G\"e3\"Bb\"d3\"E7\"DB2\"Am\"f2\"Dm\"=f[E3c3]\"Bm\"d2\"A7\"[e3-c3-]\"C\"^F3\"C/e\"g3\" \"\"D\"f\"A7\"G\"A7/g\"A3\"Gm\"d\"C\"=c2\"F#m\"\"A\"c3\"Cm\"g\"C7\"g\"C\"c'2\"D\"\"Bm\"F3\"A7/e\"c2\"C\"^F3\"c#\"F2\"D\"F3\"D7\"c2\"G\"B/2\"Bb\"B,\"Dm\"a3\"Am\"f2B,6\"F\"f3d2b/2\"Bm\"A\"D\"D\"G/b\"G3\"F\"F2\"C\"G2\"C7\"E2\"G7\"A\"Bm\"f3\"G/b\"G3\"G/b\"B2f3\"e\"d\"(E7)\"e2\"Em\"B3\"F7\"_e2\"A/c+\"d2\"ab\"A\"C\"C3\"A7\"=g2\"G/b\"g2\"Dm\"f\"A\"a2\"D7\"f\"A7/g\"A3\"D7/a\"A\"G\"f\n",
      "========= Epoch 3 out of 1000 (0.3%) =========\n",
      "0m 54s (34 10%) 2.6525\n",
      "0m 57s (68 20%) 2.0206\n",
      "0m 59s (102 30%) 2.2651\n",
      "1m 2s (136 40%) 1.4729\n",
      "1m 4s (170 50%) 1.8843\n",
      "1m 7s (204 60%) 2.4503\n",
      "1m 9s (238 70%) 2.5757\n",
      "1m 12s (272 80%) 1.8881\n",
      "1m 14s (306 90%) 2.0827\n"
     ]
    }
   ],
   "source": [
    "model = LSTMusic(part_embedding_dim, k_embedding_dim, m_embedding_dim, lstm_dim, part_vocab_size, k_vocab_size, m_vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(nb_epoch):\n",
    "    print('========= Epoch {} out of {} ({}%) ========='.format(epoch+1, nb_epoch, (epoch+1)/nb_epoch*100))\n",
    "    nb_iters = len(rep.songs)\n",
    "    iteration = 0\n",
    "    print_every = nb_iters//10\n",
    "    state_h, state_c = model.zero_state(1)\n",
    "\n",
    "    for ref_num, song in rep.songs.items():\n",
    "        \n",
    "        iteration+=1\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        part, k, m  = get_input_tensors(song.part, song.metadata['K'], song.metadata['M'], part_char2idx, k_char2idx, m_char2idx)\n",
    "        target = get_target_tensor(song.part, part_char2idx)\n",
    "        \n",
    "        output, (state_h, state_c) = model(part, k, m, (state_h, state_c))\n",
    "        \n",
    "        state_h = state_h.detach()\n",
    "        state_c = state_c.detach()\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        _ = torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(), max_norm)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if iteration % print_every == 0:\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start), iteration, iteration / nb_iters * 100, loss.item()))\n",
    "        torch.save(model.state_dict(), '../models/model.pt')\n",
    "    generate(model, start_part, start_k, start_m, length)    \n",
    "    torch.save(model.state_dict(),'../models/model-{}.pth'.format(epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
